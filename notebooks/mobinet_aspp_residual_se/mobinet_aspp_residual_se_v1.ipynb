{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af8de5bb",
   "metadata": {},
   "source": [
    "# MobileNetV2-Based ASPP Residual SE U-Net for Fetal Head Segmentation\n",
    "## Training Notebook (Google Colab Compatible)\n",
    "\n",
    "**Platform:** Google Colab with GPU acceleration\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "This notebook implements a **MobileNetV2-Based ASPP Residual SE U-Net** for efficient medical image segmentation with:\n",
    "\n",
    "**Core Architecture:**\n",
    "- **Encoder:** MobileNetV2 backbone (pre-trained on ImageNet, frozen for transfer learning)\n",
    "- **Bottleneck:** ASPP module for multi-scale contextual feature extraction\n",
    "- **Decoder:** 5 upsampling stages with ResidualBlockSE (trainable)\n",
    "- **Skip Connections:** SE blocks applied before concatenation\n",
    "\n",
    "**Key Innovations:**\n",
    "\n",
    "1. **MobileNetV2 Encoder**: Efficient transfer learning\n",
    "   - Pre-trained on ImageNet (frozen for fast training)\n",
    "   - Depthwise separable convolutions (lightweight)\n",
    "   - Feature extraction at 5 scales: [H/2, H/4, H/8, H/16, H/32]\n",
    "   - ~70% of parameters frozen ‚Üí reduced memory + faster training\n",
    "\n",
    "2. **ASPP Module at Bottleneck**: Multi-scale contextual feature extraction\n",
    "   - 1√ó1 convolution (point-wise features)\n",
    "   - 3√ó3 atrous convolutions with dilation rates [6, 12, 18]\n",
    "   - Global average pooling branch (image-level features)\n",
    "   - Captures features at different scales simultaneously\n",
    "\n",
    "3. **Squeeze-and-Excitation (SE) Blocks**: Channel-wise attention mechanism\n",
    "   - Applied after every ResidualBlockSE in decoder\n",
    "   - Applied to skip connections before concatenation\n",
    "   - Learns to emphasize informative channels and suppress less useful ones\n",
    "   - Reduction ratio: 16\n",
    "\n",
    "4. **Residual Decoder Blocks**: Skip connections within blocks for better gradient flow\n",
    "\n",
    "**Output:**\n",
    "- **Raw logits** (no sigmoid activation in model)\n",
    "- Sigmoid applied by loss function (BCEWithLogitsLoss) for numerical stability\n",
    "- Compatible with DiceBCEWithLogitsLoss (combined Dice + BCEWithLogits loss)\n",
    "\n",
    "**Advantages:**\n",
    "- ‚úì Efficient: MobileNetV2 uses depthwise separable convolutions\n",
    "- ‚úì Transfer Learning: Pre-trained weights from ImageNet\n",
    "- ‚úì Multi-scale Context: ASPP captures features at multiple scales\n",
    "- ‚úì Channel Attention: SE blocks enhance feature representation\n",
    "- ‚úì Numerically Stable: BCEWithLogitsLoss prevents overflow/underflow issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cf26ad",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "### Google Colab Configuration\n",
    "\n",
    "**Repository:** `https://github.com/TrinhThaiSonDHQT/Fetal-Head-Segmentation`\n",
    "\n",
    "**Directory Structure:**\n",
    "- **Project Root:** `/content/Fetal-Head-Segmentation/`\n",
    "- **Outputs:** `/content/outputs/results/`\n",
    "  - Checkpoints, logs, predictions, and visualizations\n",
    "  - Download results after training completes\n",
    "\n",
    "**Steps:**\n",
    "1. Clone repository from GitHub\n",
    "2. Install dependencies (Albumentations 1.3.1)\n",
    "3. Import required modules and verify CUDA availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a831552f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the GitHub repository\n",
    "import os\n",
    "\n",
    "# Check if already cloned\n",
    "if not os.path.exists('/content/Fetal-Head-Segmentation'):\n",
    "    print(\"Cloning repository from GitHub...\")\n",
    "    !git clone https://github.com/TrinhThaiSonDHQT/Fetal-Head-Segmentation.git\n",
    "    print(\"‚úì Repository cloned successfully\")\n",
    "else:\n",
    "    print(\"‚úì Repository already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb5a71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"[Google Colab Setup]\")\n",
    "\n",
    "# Setup paths for Google Colab\n",
    "project_root = Path('/content/Fetal-Head-Segmentation')\n",
    "output_root = Path('/content/outputs')\n",
    "cache_root = output_root / 'cache'\n",
    "\n",
    "# Verify project exists\n",
    "if not project_root.exists():\n",
    "    raise RuntimeError(\n",
    "        f\"Project not found at {project_root}\\n\"\n",
    "        f\"Please run the previous cell to clone the repository from GitHub.\"\n",
    "    )\n",
    "\n",
    "if not (project_root / 'efficient_focus').exists():\n",
    "    raise RuntimeError(\n",
    "        f\"'efficient_focus' folder not found in {project_root}\\n\"\n",
    "        f\"Please ensure the repository was cloned correctly.\"\n",
    "    )\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Output root: {output_root}\")\n",
    "\n",
    "# Add project to path\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"\\n‚úì Environment setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95c0271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for Google Colab\n",
    "print(\"Installing required packages...\")\n",
    "\n",
    "# Colab has most packages pre-installed (PyTorch, NumPy, Matplotlib, OpenCV)\n",
    "# Pin Albumentations to 1.3.1 for compatibility with both Colab and Kaggle\n",
    "!pip install -q albumentations==1.3.1\n",
    "\n",
    "print(\"\\n‚úì Packages installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7de075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from efficient_focus.src.losses import DiceBCEWithLogitsLoss\n",
    "from efficient_focus.src.models.mobinet_aspp_residual_se.mobinet_aspp_residual_se import MobileNetV2ASPPResidualSEUNet, count_parameters\n",
    "\n",
    "from shared.src.data import HC18Dataset\n",
    "from shared.src.metrics.segmentation_metrics import dice_coefficient, iou_score, pixel_accuracy\n",
    "from shared.src.utils.visualization import save_prediction_grid, visualize_sample\n",
    "from shared.src.utils.transforms import get_transforms\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93459378",
   "metadata": {},
   "source": [
    "## 2. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ccbd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration for MobileNetV2-Based ASPP Residual SE U-Net\n",
    "config_path = project_root / 'efficient_focus' / 'configs' / 'mobinet_aspp_residual_se_config.yaml'\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Adjust output paths for Google Colab environment\n",
    "print(f\"Adjusting paths for Google Colab environment...\")\n",
    "config['logging']['checkpoint_dir'] = str(output_root / 'results' / 'checkpoints')\n",
    "config['logging']['log_dir'] = str(output_root / 'results' / 'logs')\n",
    "config['logging']['prediction_dir'] = str(output_root / 'results' / 'predictions')\n",
    "config['logging']['visualization_dir'] = str(output_root / 'results' / 'visualizations')\n",
    "\n",
    "print(f\"  Outputs will be saved to: {output_root / 'results'}\")\n",
    "\n",
    "print(\"\\nConfiguration loaded:\")\n",
    "print(f\"  Model: {config['model']['name']}\")\n",
    "print(f\"  Pre-trained: {config['model']['pretrained']}\")\n",
    "print(f\"  Freeze Encoder: {config['model']['freeze_encoder']}\")\n",
    "print(f\"  SE Reduction Ratio: {config['model']['reduction_ratio']}\")\n",
    "print(f\"  ASPP Atrous Rates: {config['model']['atrous_rates']}\")\n",
    "print(f\"  ASPP Dropout: {config['model']['aspp_dropout']}\")\n",
    "print(f\"  ASPP Use GroupNorm: {config['model']['aspp_use_groupnorm']}\")\n",
    "print(f\"  Learning Rate: {config['training']['optimizer']['lr']}\")\n",
    "print(f\"  Weight Decay: {config['training']['optimizer']['weight_decay']}\")\n",
    "print(f\"  Loss Function: {config['loss']['name']}\")\n",
    "print(f\"  Batch Size: {config['training']['batch_size']}\")\n",
    "print(f\"  Epochs: {config['training']['num_epochs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3423668",
   "metadata": {},
   "source": [
    "## 3. Model Initialization\n",
    "\n",
    "### MobileNetV2-Based ASPP Residual SE U-Net Architecture Details\n",
    "\n",
    "**Encoder Path (MobileNetV2 - Frozen, Pre-trained):**\n",
    "- Pre-trained on ImageNet for transfer learning\n",
    "- Feature extraction at 5 scales: [H/2, H/4, H/8, H/16, H/32]\n",
    "- Channel progression: 16 ‚Üí 24 ‚Üí 32 ‚Üí 96 ‚Üí 1280\n",
    "- Depthwise separable convolutions (efficient)\n",
    "- **Frozen weights** ‚Üí ~70% of parameters not trained\n",
    "- Custom initial conv (trainable) for full-resolution skip connection (32 channels)\n",
    "\n",
    "**Bottleneck (ASPP Module):**\n",
    "- Multi-scale feature extraction, output: 512 channels\n",
    "- **1√ó1 convolution**: Point-wise features\n",
    "- **3√ó3 atrous convolutions**: Dilation rates [6, 12, 18] for multi-scale context\n",
    "- **Global Average Pooling**: Image-level features with GroupNorm\n",
    "- **Dropout (0.5)**: Regularization to prevent overfitting\n",
    "- **Fusion**: Concatenate all branches and project to 512 channels\n",
    "\n",
    "**Decoder Path (Trainable):**\n",
    "- 5 upsampling blocks: 512 ‚Üí 256 ‚Üí 128 ‚Üí 64 ‚Üí 32 ‚Üí 32 filters\n",
    "- Upsampling: ConvTranspose2d (2√ó2, stride=2)\n",
    "- Skip connections: SE-enhanced encoder features concatenated with decoder\n",
    "- Each block: ResidualBlockSE after concatenation\n",
    "\n",
    "**Channel Attention (SE Blocks):**\n",
    "- Applied after every ResidualBlockSE in decoder\n",
    "- Applied to skip connections before concatenation\n",
    "- Squeeze: Global average pooling\n",
    "- Excitation: FC ‚Üí ReLU ‚Üí FC ‚Üí Sigmoid\n",
    "- Reduction ratio: 16 (balances performance vs. parameters)\n",
    "\n",
    "**Output Layer:**\n",
    "- Conv2d (1√ó1) to single channel\n",
    "- **No sigmoid activation** ‚Üí outputs raw logits\n",
    "- Sigmoid applied by loss function (BCEWithLogitsLoss) for numerical stability\n",
    "\n",
    "**Key Advantages:**\n",
    "- ‚úì **Efficient**: MobileNetV2 uses depthwise separable convolutions\n",
    "- ‚úì **Transfer Learning**: Pre-trained weights from ImageNet\n",
    "- ‚úì **Fast Training**: Frozen encoder reduces trainable parameters by ~70%\n",
    "- ‚úì **Less Memory**: Can use larger batch sizes or higher resolution images\n",
    "- ‚úì **Multi-scale Context**: ASPP captures features at multiple scales\n",
    "- ‚úì **Channel Attention**: SE blocks enhance feature representation\n",
    "- ‚úì **Numerically Stable**: Raw logits prevent double sigmoid application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef60416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(config['device'] if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize MobileNetV2-Based ASPP Residual SE U-Net\n",
    "model = MobileNetV2ASPPResidualSEUNet(\n",
    "    in_channels=config['model']['in_channels'],\n",
    "    out_channels=config['model']['out_channels'],\n",
    "    pretrained=config['model']['pretrained'],\n",
    "    freeze_encoder=config['model']['freeze_encoder'],\n",
    "    reduction_ratio=config['model']['reduction_ratio'],\n",
    "    atrous_rates=config['model']['atrous_rates'],\n",
    "    aspp_dropout=config['model']['aspp_dropout'],\n",
    "    aspp_use_groupnorm=config['model']['aspp_use_groupnorm']\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params, trainable_params, frozen_params = count_parameters(model)\n",
    "\n",
    "# Model summary\n",
    "print(f\"\\nMobileNetV2-Based ASPP Residual SE U-Net Architecture:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.1f}%)\")\n",
    "print(f\"  Frozen parameters: {frozen_params:,} ({100 * frozen_params / total_params:.1f}%)\")\n",
    "print(f\"  Total model size: ~{total_params * 4 / (1024**2):.2f} MB (float32)\")\n",
    "print(f\"  Trainable size: ~{trainable_params * 4 / (1024**2):.2f} MB (float32)\")\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  Input channels: {config['model']['in_channels']}\")\n",
    "print(f\"  Output channels: {config['model']['out_channels']}\")\n",
    "print(f\"  Pre-trained encoder: {config['model']['pretrained']}\")\n",
    "print(f\"  Freeze encoder: {config['model']['freeze_encoder']}\")\n",
    "print(f\"  SE reduction ratio: {config['model']['reduction_ratio']}\")\n",
    "print(f\"  ASPP atrous rates: {config['model']['atrous_rates']}\")\n",
    "print(f\"  ASPP dropout: {config['model']['aspp_dropout']}\")\n",
    "print(f\"  ASPP use GroupNorm: {config['model']['aspp_use_groupnorm']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ed3aa7",
   "metadata": {},
   "source": [
    "## 4. Loss Function and Optimizer\n",
    "\n",
    "### Loss Function: DiceBCEWithLogitsLoss (Hybrid Loss)\n",
    "\n",
    "A weighted combination of **Dice Loss** and **BCEWithLogits Loss**.\n",
    "\n",
    "**Dice Loss (80%):** Optimizes region overlap\n",
    "  - Directly optimizes the Dice coefficient metric\n",
    "  - Effective for imbalanced segmentation (small target regions)\n",
    "  - Handles class imbalance naturally\n",
    "\n",
    "**BCEWithLogits Loss (20%):** Optimizes pixel-wise classification\n",
    "  - Numerically stable (combines sigmoid + BCE in one operation)\n",
    "  - Handles boundary refinement\n",
    "  - Auto-computes `pos_weight` from data to balance classes\n",
    "\n",
    "**Key Features:**\n",
    "- Expects **logits** (raw values before sigmoid) as input\n",
    "- Auto-computes `pos_weight` from first batch for class balance\n",
    "- Smooth parameter (1e-6) for numerical stability in Dice calculation\n",
    "\n",
    "### Optimizer: Adam\n",
    "- Adaptive learning rate per parameter\n",
    "- Learning rate: 3e-4 (higher for faster decoder training, encoder frozen)\n",
    "- Weight decay: 1e-4 (L2 regularization to prevent decoder overfitting)\n",
    "\n",
    "### Learning Rate Scheduler: ReduceLROnPlateau\n",
    "- Monitors validation Dice coefficient (mode='max')\n",
    "- Reduces LR by factor of 0.5 when validation plateaus\n",
    "- Patience: 10 epochs\n",
    "- Minimum LR: 1e-6\n",
    "- Helps fine-tune convergence and escape local minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8d5edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_config = config['loss']\n",
    "dice_weight_config = loss_config.get('dice_weight', 0.8)\n",
    "bce_weight_config = loss_config.get('bce_weight', 0.2)\n",
    "smooth_config = loss_config.get('smooth', 1.0e-6)\n",
    "pos_weight_config = loss_config.get('pos_weight', None)\n",
    "auto_weight_config = loss_config.get('auto_weight', True)\n",
    "\n",
    "# Use DiceBCEWithLogitsLoss\n",
    "criterion = DiceBCEWithLogitsLoss(\n",
    "    dice_weight=dice_weight_config,\n",
    "    bce_weight=bce_weight_config,\n",
    "    pos_weight=pos_weight_config,\n",
    "    auto_weight=auto_weight_config,\n",
    "    smooth=smooth_config\n",
    ")\n",
    "\n",
    "print(f\"Loss Function: DiceBCELoss\")\n",
    "print(f\"  Dice weight: {dice_weight_config}\")\n",
    "print(f\"  BCE weight: {bce_weight_config}\")\n",
    "print(f\"  Smooth parameter: {smooth_config}\")\n",
    "\n",
    "# Optimizer (Adam)\n",
    "optimizer_config = config['training']['optimizer']\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=optimizer_config['lr'],\n",
    "    betas=tuple(optimizer_config['betas']),\n",
    "    eps=optimizer_config['eps'],\n",
    "    weight_decay=optimizer_config['weight_decay']\n",
    ")\n",
    "print(f\"\\nOptimizer: Adam\")\n",
    "print(f\"  Learning rate: {optimizer_config['lr']}\")\n",
    "print(f\"  Weight decay: {optimizer_config['weight_decay']}\")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler_config = config['training']['scheduler']\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=scheduler_config['mode'],\n",
    "    factor=scheduler_config['factor'],\n",
    "    patience=scheduler_config['patience'],\n",
    "    min_lr=scheduler_config['min_lr']\n",
    ")\n",
    "print(f\"\\nScheduler: ReduceLROnPlateau\")\n",
    "print(f\"  Mode: {scheduler_config['mode']}\")\n",
    "print(f\"  Factor: {scheduler_config['factor']}\")\n",
    "print(f\"  Patience: {scheduler_config['patience']}\")\n",
    "print(f\"  Min LR: {scheduler_config['min_lr']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d490a309",
   "metadata": {},
   "source": [
    "## 5. Data Loading and Augmentation\n",
    "\n",
    "### Preprocessing Pipeline (All Images)\n",
    "\n",
    "Applied consistently to training, validation, and test sets:\n",
    "1. **Normalization:** Divide pixel values by 255.0 ‚Üí [0, 1] range\n",
    "2. **Resizing:** 256√ó256 pixels (maintains aspect ratio consistency)\n",
    "3. **Tensor Conversion:** NumPy array ‚Üí PyTorch tensor (C√óH√óW format)\n",
    "\n",
    "### Data Augmentation (Training Only)\n",
    "\n",
    "**On-the-fly augmentation** using Albumentations library:\n",
    "- **HorizontalFlip:** p=0.5 (mirrors left-right)\n",
    "- **VerticalFlip:** p=0.5 (mirrors top-bottom)\n",
    "- **Rotation:** ¬±20¬∞ with p=0.5 (handles probe orientation variations)\n",
    "- **ShiftScaleRotate:** p=0.5\n",
    "  - Translation: ¬±10% (handles positioning variations)\n",
    "  - Scaling: ¬±10% (handles zoom variations)\n",
    "\n",
    "**Benefits:**\n",
    "- Augmentation applied **per epoch** ‚Üí different samples each time\n",
    "- Improves model generalization and robustness\n",
    "- Prevents overfitting on small datasets\n",
    "- Image-mask transforms synchronized automatically\n",
    "\n",
    "**Validation/Test:**\n",
    "- **No augmentation** applied\n",
    "- Only preprocessing (normalize, resize, tensorize)\n",
    "- Ensures consistent evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f51a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = config['data']\n",
    "training_config = config['training']\n",
    "\n",
    "# Helper to build paths\n",
    "def get_path(config_path):\n",
    "    \"\"\"Helper to handle both absolute and relative paths\"\"\"\n",
    "    p = Path(config_path)\n",
    "    if p.is_absolute():\n",
    "        return str(p)\n",
    "    else:\n",
    "        return str(project_root / config_path)\n",
    "\n",
    "# Create augmentation transforms\n",
    "print(\"Creating augmentation transforms...\")\n",
    "train_transform = get_transforms(height=256, width=256, is_train=True)\n",
    "val_transform = get_transforms(height=256, width=256, is_train=False)\n",
    "print(\"  Train transform: WITH augmentation (HorizontalFlip, VerticalFlip, Rotation, ShiftScaleRotate)\")\n",
    "print(\"  Val transform: WITHOUT augmentation (resize + normalize only)\")\n",
    "\n",
    "# Create datasets - using HC18Dataset for on-the-fly augmentation\n",
    "print(\"\\nCreating training dataset...\")\n",
    "train_dataset = HC18Dataset(\n",
    "    image_dir=get_path(data_config['train_images']),\n",
    "    mask_dir=get_path(data_config['train_masks']),\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "print(\"Creating validation dataset...\")\n",
    "val_dataset = HC18Dataset(\n",
    "    image_dir=get_path(data_config['val_images']),\n",
    "    mask_dir=get_path(data_config['val_masks']),\n",
    "    transform=val_transform\n",
    ")\n",
    "\n",
    "print(\"Creating test dataset...\")\n",
    "test_dataset = HC18Dataset(\n",
    "    image_dir=get_path(data_config['test_images']),\n",
    "    mask_dir=get_path(data_config['test_masks']),\n",
    "    transform=val_transform\n",
    ")\n",
    "\n",
    "# Use num_workers=0 for Google Colab to avoid multiprocessing issues\n",
    "num_workers = 0\n",
    "print(f\"\\nDataLoader settings:\")\n",
    "print(f\"  num_workers: {num_workers} (disabled for Google Colab)\")\n",
    "print(f\"  Batch size: {training_config['batch_size']}\")\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=training_config['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=training_config['pin_memory']\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=training_config['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=training_config['pin_memory']\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=training_config['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=training_config['pin_memory']\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Datasets Ready:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Train samples: {len(train_dataset)}\")\n",
    "print(f\"  Validation samples: {len(val_dataset)}\")\n",
    "print(f\"  Test samples: {len(test_dataset)}\")\n",
    "print(f\"  Batch size: {training_config['batch_size']}\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")\n",
    "print(f\"  Train augmentation: ENABLED (on-the-fly)\")\n",
    "print(f\"  Val/Test augmentation: DISABLED\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7081dcb4",
   "metadata": {},
   "source": [
    "## 6. Data Quality Verification\n",
    "\n",
    "Verify data integrity before training to catch preprocessing errors early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f56d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of training data\n",
    "sample_images, sample_masks = next(iter(train_loader))\n",
    "\n",
    "print(f\"Sample batch:\")\n",
    "print(f\"  Images shape: {sample_images.shape}\")\n",
    "print(f\"  Masks shape: {sample_masks.shape}\")\n",
    "print(f\"  Image range: [{sample_images.min():.4f}, {sample_images.max():.4f}]\")\n",
    "print(f\"  Mask range: [{sample_masks.min():.4f}, {sample_masks.max():.4f}]\")\n",
    "print(f\"  Mask unique values: {torch.unique(sample_masks)}\")\n",
    "print(f\"  Mask mean (% foreground): {sample_masks.mean():.4f}\")\n",
    "\n",
    "# CRITICAL CHECK: Ensure masks are binary {0, 1}\n",
    "if not torch.all((sample_masks == 0) | (sample_masks == 1)):\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: Masks are not binary! Check preprocessing.\")\n",
    "else:\n",
    "    print(\"\\n‚úì Masks are properly binary {0, 1}\")\n",
    "\n",
    "# Check if masks have reasonable foreground ratio (2-10% typical for fetal head)\n",
    "fg_ratio = sample_masks.mean().item()\n",
    "if fg_ratio < 0.01 or fg_ratio > 0.3:\n",
    "    print(f\"‚ö†Ô∏è  WARNING: Unusual foreground ratio: {fg_ratio:.2%} (expected 2-10%)\")\n",
    "else:\n",
    "    print(f\"‚úì Foreground ratio looks reasonable: {fg_ratio:.2%}\")\n",
    "\n",
    "# Visualize first sample\n",
    "visualize_sample(sample_images[0], sample_masks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2a803c",
   "metadata": {},
   "source": [
    "## 7. Training and Validation Functions\n",
    "\n",
    "### train_one_epoch()\n",
    "- Sets model to training mode\n",
    "- Iterates through training batches\n",
    "- Forward pass ‚Üí loss calculation ‚Üí backward pass ‚Üí optimizer step\n",
    "- Returns average epoch loss\n",
    "\n",
    "### validate()\n",
    "- Sets model to evaluation mode (disables dropout, batchnorm updates)\n",
    "- Computes loss and metrics on validation set\n",
    "- Model outputs raw logits, applies sigmoid for predictions\n",
    "- Thresholds at 0.5 for binary predictions\n",
    "- Calculates: Dice coefficient, IoU, Pixel Accuracy\n",
    "- Returns average metrics across all validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9898354",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MODEL-LOSS COMPATIBILITY VERIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test with a small batch\n",
    "sample_images, sample_masks = next(iter(train_loader))\n",
    "sample_images = sample_images.to(device)\n",
    "sample_masks = sample_masks.to(device)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    model_outputs = model(sample_images)\n",
    "\n",
    "print(f\"\\n1. Model Output Analysis:\")\n",
    "print(f\"   Shape: {model_outputs.shape}\")\n",
    "print(f\"   Range: [{model_outputs.min().item():.4f}, {model_outputs.max().item():.4f}]\")\n",
    "print(f\"   Mean: {model_outputs.mean().item():.4f}\")\n",
    "print(f\"   Std: {model_outputs.std().item():.4f}\")\n",
    "\n",
    "# Check if outputs are logits (should have negative values and values > 1)\n",
    "has_negative = (model_outputs < 0).any().item()\n",
    "has_large_positive = (model_outputs > 1).any().item()\n",
    "\n",
    "if has_negative or has_large_positive:\n",
    "    print(f\"   ‚úì Outputs are LOGITS (raw values before sigmoid)\")\n",
    "    print(f\"     - Has negative values: {has_negative}\")\n",
    "    print(f\"     - Has values > 1: {has_large_positive}\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  WARNING: Outputs look like probabilities [0,1], not logits!\")\n",
    "    print(f\"     - Model may have sigmoid in output layer\")\n",
    "\n",
    "print(f\"\\n2. Loss Function Test:\")\n",
    "loss_value = criterion(model_outputs, sample_masks)\n",
    "print(f\"   Loss value: {loss_value.item():.6f}\")\n",
    "print(f\"   ‚úì Loss computed successfully\")\n",
    "\n",
    "print(f\"\\n3. Validation Metrics Test:\")\n",
    "# Apply sigmoid to get probabilities for metrics\n",
    "probs = torch.sigmoid(model_outputs)\n",
    "preds = (probs > 0.5).float()\n",
    "\n",
    "print(f\"   Probabilities after sigmoid:\")\n",
    "print(f\"     Range: [{probs.min().item():.4f}, {probs.max().item():.4f}]\")\n",
    "print(f\"     Mean: {probs.mean().item():.4f}\")\n",
    "\n",
    "dice = dice_coefficient(preds[0], sample_masks[0]).item()\n",
    "iou = iou_score(preds[0], sample_masks[0]).item()\n",
    "pa = pixel_accuracy(preds[0], sample_masks[0])\n",
    "\n",
    "print(f\"   Sample metrics (before training):\")\n",
    "print(f\"     Dice: {dice:.4f}\")\n",
    "print(f\"     IoU: {iou:.4f}\")\n",
    "print(f\"     PA: {pa:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úì ALL CHECKS PASSED - Model and loss are compatible!\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b68a8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, device, epoch):\n",
    "    \"\"\"\n",
    "    Train for one epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1} [Train]\", leave=False)\n",
    "    for batch_idx, (images, masks) in enumerate(pbar):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images)  # Model outputs raw logits\n",
    "        \n",
    "        # Calculate loss (DiceBCEWithLogitsLoss expects logits)\n",
    "        loss = criterion(logits, masks)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update statistics\n",
    "        running_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, device, epoch):\n",
    "    \"\"\"\n",
    "    Validate the model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    dice_scores = []\n",
    "    iou_scores = []\n",
    "    pa_scores = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1} [Val]\", leave=False)\n",
    "        for batch_idx, (images, masks) in enumerate(pbar):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(images)  # Model outputs raw logits\n",
    "            \n",
    "            # Calculate loss (DiceBCEWithLogitsLoss expects logits)\n",
    "            loss = criterion(logits, masks)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Calculate metrics\n",
    "            # Apply sigmoid to convert logits to probabilities [0, 1]\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs > 0.5).float()\n",
    "            \n",
    "            for i in range(images.size(0)):\n",
    "                dice = dice_coefficient(preds[i], masks[i])\n",
    "                iou = iou_score(preds[i], masks[i])\n",
    "                pa = pixel_accuracy(preds[i], masks[i])\n",
    "                \n",
    "                dice_scores.append(dice.item())\n",
    "                iou_scores.append(iou.item())\n",
    "                pa_scores.append(pa.item())\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': f\"{loss.item():.4f}\",\n",
    "                'dice': f\"{np.mean(dice_scores):.4f}\"\n",
    "            })\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    val_loss = running_loss / len(dataloader)\n",
    "    val_dice = np.mean(dice_scores)\n",
    "    val_iou = np.mean(iou_scores)\n",
    "    val_pa = np.mean(pa_scores)\n",
    "    \n",
    "    return val_loss, val_dice, val_iou, val_pa\n",
    "\n",
    "\n",
    "print(\"Training functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2c0afb",
   "metadata": {},
   "source": [
    "## 8. Training Loop\n",
    "\n",
    "### Training Configuration\n",
    "- **Epochs:** Configurable (typically 50-100)\n",
    "- **Early Stopping:** Monitors validation Dice coefficient\n",
    "  - Stops if no improvement for N consecutive epochs\n",
    "  - Prevents overfitting and saves compute time\n",
    "- **Model Checkpointing:** Saves best model based on validation Dice\n",
    "\n",
    "### Per-Epoch Workflow\n",
    "1. Train on full training set\n",
    "2. Validate on validation set\n",
    "3. Update learning rate (ReduceLROnPlateau scheduler)\n",
    "4. Log metrics: loss, Dice, IoU, pixel accuracy\n",
    "5. Save model if validation Dice improves\n",
    "6. Check early stopping criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c20750f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{'='*60}\")\n",
    "print(f\"Starting Training - ASPP-Enhanced Residual SE U-Net\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Training configuration\n",
    "num_epochs = config['training']['num_epochs']\n",
    "patience = config['training']['early_stopping_patience']\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'val_dice': [],\n",
    "    'val_iou': [],\n",
    "    'val_pa': [],\n",
    "    'lr': []\n",
    "}\n",
    "\n",
    "best_dice = 0.0\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "print(f\"Epochs: {num_epochs}\")\n",
    "print(f\"Early Stopping Patience: {patience}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Main training loop\n",
    "for epoch in range(num_epochs):    \n",
    "    # Train\n",
    "    train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device, epoch)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_dice, val_iou, val_pa = validate(model, val_loader, criterion, device, epoch)\n",
    "    \n",
    "    # Update learning rate\n",
    "    old_lr = optimizer.param_groups[0]['lr']\n",
    "    scheduler.step(val_dice)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Update history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_dice'].append(val_dice)\n",
    "    history['val_iou'].append(val_iou)\n",
    "    history['val_pa'].append(val_pa)\n",
    "    history['lr'].append(current_lr)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"{'='*100}\")\n",
    "    dice_indicator = ' üèÜ' if val_dice > best_dice else ''\n",
    "    lr_change = f' ‚¨áÔ∏è (reduced from {old_lr:.6f})' if current_lr < old_lr else ''\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Dice: {val_dice:.4f}{dice_indicator}\")\n",
    "    print(f\"Val mIoU: {val_iou:.4f} | Val mPA: {val_pa:.4f}\")\n",
    "    if lr_change: print(f\"LR: {current_lr:.6f}{lr_change}\")\n",
    "    print(f\"{'='*100}\")\n",
    "    \n",
    "    # Check for improvement\n",
    "    is_best = val_dice > best_dice\n",
    "    if is_best:\n",
    "        best_dice = val_dice\n",
    "        epochs_without_improvement = 0\n",
    "        \n",
    "        # Save best model\n",
    "        checkpoint_dir = Path(get_path(config['logging']['checkpoint_dir']))\n",
    "        checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "        best_model_path = checkpoint_dir / 'best_model_aspp_residual_se_unet.pth'\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'best_dice': best_dice,\n",
    "            'history': history,\n",
    "            'config': config\n",
    "        }, best_model_path)\n",
    "        \n",
    "        print(f\"  ‚Üí Saved best model (Dice: {best_dice:.4f})\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        print(f\"  ‚ö†Ô∏è  No improvement for {epochs_without_improvement}/{patience} epochs\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"‚õî EARLY STOPPING TRIGGERED\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"  Stopped at epoch: {epoch+1}\")\n",
    "        print(f\"  Best Dice Score:  {best_dice:.4f}\")\n",
    "        print(f\"  Patience limit:   {patience} epochs without improvement\")\n",
    "        print(f\"{'='*70}\")\n",
    "        break\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training Completed!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Best Validation Dice: {best_dice:.4f}\")\n",
    "print(f\"Best Validation IoU:  {max(history['val_iou']):.4f}\")\n",
    "print(f\"Best Validation PA:   {max(history['val_pa']):.4f}\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b29807",
   "metadata": {},
   "source": [
    "## 9. Training Visualization\n",
    "\n",
    "Generate plots to analyze training dynamics and convergence:\n",
    "- **Loss curves:** Training vs validation loss over epochs\n",
    "- **Dice coefficient:** Validation performance trend\n",
    "- **IoU score:** Intersection over Union metric progression\n",
    "- **Learning rate:** ReduceLROnPlateau schedule adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97005d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Loss curves\n",
    "axes[0, 0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0, 0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Loss (Dice + BCE)', fontsize=12)\n",
    "axes[0, 0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=11)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Dice coefficient\n",
    "axes[0, 1].plot(history['val_dice'], label='Val Dice', color='green', linewidth=2)\n",
    "axes[0, 1].axhline(y=best_dice, color='red', linestyle='--', label=f'Best: {best_dice:.4f}')\n",
    "axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Dice Coefficient', fontsize=12)\n",
    "axes[0, 1].set_title('Validation Dice Coefficient', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=11)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# IoU\n",
    "axes[1, 0].plot(history['val_iou'], label='Val IoU', color='orange', linewidth=2)\n",
    "axes[1, 0].axhline(y=max(history['val_iou']), color='red', linestyle='--', \n",
    "                   label=f\"Best: {max(history['val_iou']):.4f}\")\n",
    "axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 0].set_ylabel('IoU Score', fontsize=12)\n",
    "axes[1, 0].set_title('Validation IoU Score', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend(fontsize=11)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "axes[1, 1].plot(history['lr'], label='Learning Rate', color='red', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Learning Rate', fontsize=12)\n",
    "axes[1, 1].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].legend(fontsize=11)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save figure\n",
    "log_dir = Path(get_path(config['logging']['log_dir']))\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(log_dir / 'training_curves.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"Training curves saved to {log_dir / 'training_curves.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc6834e",
   "metadata": {},
   "source": [
    "## 10. Model Inference and Results\n",
    "\n",
    "### Evaluation on Test Set\n",
    "\n",
    "Load the best checkpoint (highest validation Dice) and visualize predictions:\n",
    "- Compare input images, ground truth masks, and model predictions\n",
    "- Calculate per-sample metrics (Dice, IoU, Pixel Accuracy)\n",
    "- Assess segmentation quality visually\n",
    "\n",
    "**Note:** Model outputs raw logits. Apply sigmoid to get probabilities [0, 1], then threshold at 0.5 for binary predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d67ce74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint_path = Path(get_path(config['logging']['checkpoint_dir'])) / 'best_model_aspp_residual_se_unet.pth'\n",
    "checkpoint = torch.load(checkpoint_path, weights_only=False)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loaded best model from epoch {checkpoint['epoch']+1}\")\n",
    "print(f\"Best Dice Score: {checkpoint['best_dice']:.4f}\")\n",
    "\n",
    "# Get test samples\n",
    "test_images, test_masks = next(iter(test_loader))\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "# Generate predictions\n",
    "with torch.no_grad():\n",
    "    # Model outputs raw logits - apply sigmoid to get probabilities\n",
    "    test_logits = model(test_images)\n",
    "    test_probs = torch.sigmoid(test_logits)  # Convert logits to [0, 1] probabilities\n",
    "    test_preds = (test_probs > 0.5).float()  # Threshold at 0.5\n",
    "\n",
    "# Visualize predictions\n",
    "num_samples = min(4, len(test_images))\n",
    "fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4 * num_samples))\n",
    "\n",
    "if num_samples == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for i in range(num_samples):\n",
    "    # Move to CPU and convert to numpy\n",
    "    img = test_images[i, 0].cpu().numpy()\n",
    "    mask = test_masks[i, 0].numpy()\n",
    "    pred = test_preds[i, 0].cpu().numpy()\n",
    "    \n",
    "    # Calculate metrics for this sample (ensure both tensors on same device)\n",
    "    pred_tensor = test_preds[i].cpu()\n",
    "    mask_tensor = test_masks[i].to(pred_tensor.device)\n",
    "    dice = dice_coefficient(pred_tensor, mask_tensor).item()\n",
    "    iou = iou_score(pred_tensor, mask_tensor).item()\n",
    "    pa = pixel_accuracy(pred_tensor, mask_tensor)\n",
    "    \n",
    "    # Input image\n",
    "    axes[i, 0].imshow(img, cmap='gray')\n",
    "    axes[i, 0].set_title('Input Image', fontsize=12, fontweight='bold')\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    # Ground truth\n",
    "    axes[i, 1].imshow(mask, cmap='gray')\n",
    "    axes[i, 1].set_title('Ground Truth', fontsize=12, fontweight='bold')\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    # Prediction\n",
    "    axes[i, 2].imshow(pred, cmap='gray')\n",
    "    axes[i, 2].set_title(f'Prediction\\nDice: {dice:.4f} | IoU: {iou:.4f} | PA: {pa:.4f}', \n",
    "                         fontsize=10, fontweight='bold')\n",
    "    axes[i, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save predictions\n",
    "pred_dir = Path(get_path(config['logging']['prediction_dir']))\n",
    "pred_dir.mkdir(parents=True, exist_ok=True)\n",
    "save_prediction_grid(test_images[:4].cpu(), test_masks[:4], test_preds[:4].cpu(), \n",
    "                    str(pred_dir / 'sample_predictions.png'), num_samples=4)\n",
    "print(f\"Sample predictions saved to {pred_dir / 'sample_predictions.png'}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
