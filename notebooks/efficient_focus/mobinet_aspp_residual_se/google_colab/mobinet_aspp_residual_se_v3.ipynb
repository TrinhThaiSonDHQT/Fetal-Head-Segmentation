{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af8de5bb",
   "metadata": {},
   "source": [
    "# MobileNetV2-Based ASPP Residual SE U-Net for Fetal Head Segmentation\n",
    "## Training Notebook (Google Colab Compatible)\n",
    "\n",
    "---\n",
    "\n",
    "**Project:** Fetal Head Segmentation in Ultrasound Images using Deep Learning  \n",
    "**Model:** Efficiency-Focused Architecture with Transfer Learning  \n",
    "**Platform:** Google Colab with GPU acceleration (T4/A100/V100)  \n",
    "**Dataset:** Large-Scale Fetal Ultrasound Dataset (3,792 annotated images)  \n",
    "**Framework:** PyTorch 2.0+  \n",
    "\n",
    "---\n",
    "\n",
    "### üìã Notebook Overview\n",
    "\n",
    "This notebook implements a **complete training pipeline** for fetal head segmentation using an efficient U-Net variant optimized for medical imaging tasks. The model combines:\n",
    "\n",
    "1. **Transfer Learning:** MobileNetV2 encoder pre-trained on ImageNet\n",
    "2. **Multi-Scale Context:** ASPP (Atrous Spatial Pyramid Pooling) module\n",
    "3. **Channel Attention:** Squeeze-and-Excitation (SE) blocks\n",
    "4. **Residual Connections:** Improved gradient flow in decoder\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Dataset Information\n",
    "\n",
    "**Source:** Large-Scale Annotation Dataset for Fetal Head Biometry  \n",
    "**Zenodo Link:** https://zenodo.org/records/8265464  \n",
    "**Citation:** van den Heuvel et al. (2018) - HC18 Challenge + Extended Annotations\n",
    "\n",
    "**Dataset Composition:**\n",
    "- **Total Images:** 3,792 annotated ultrasound frames\n",
    "- **Training Set:** 2,648 images (69.8%)\n",
    "- **Validation Set:** 568 images (15.0%)\n",
    "- **Test Set:** 576 images (15.2%)\n",
    "\n",
    "**Anatomical Planes (Multi-Plane Coverage):**\n",
    "- **Trans-thalamic:** 1,557 images (41.0%) - Standard HC measurement plane\n",
    "- **Diverse Fetal Head:** 999 images (26.3%) - HC18 Challenge dataset\n",
    "- **Trans-cerebellum:** 681 images (18.0%) - Posterior fossa view\n",
    "- **Trans-ventricular:** 555 images (14.6%) - Lateral ventricle view\n",
    "\n",
    "**Image Characteristics:**\n",
    "- **Format:** Grayscale ultrasound (PNG)\n",
    "- **Resolution:** Variable (resized to 256√ó256 for training)\n",
    "- **Pixel Size:** 0.09-0.33 mm (heterogeneous, real-world conditions)\n",
    "- **Annotations:** Binary masks for fetal head boundary\n",
    "\n",
    "**Dataset Advantages:**\n",
    "- ‚úì Large-scale: 3,792 images (3.8√ó larger than original HC18)\n",
    "- ‚úì Multi-plane: Covers 4 standard anatomical views\n",
    "- ‚úì Diverse: Multiple patients, imaging conditions, and gestational ages\n",
    "- ‚úì Clinically relevant: Includes both standard and challenging cases\n",
    "\n",
    "---\n",
    "\n",
    "### üèóÔ∏è Architecture Overview\n",
    "\n",
    "#### **Encoder Path (MobileNetV2 - Frozen, Pre-trained)**\n",
    "- **Pre-training:** ImageNet classification (1.2M images, 1000 classes)\n",
    "- **Feature Extraction:** 5 scales [H/2, H/4, H/8, H/16, H/32]\n",
    "- **Channel Progression:** 16 ‚Üí 24 ‚Üí 32 ‚Üí 96 ‚Üí 1280\n",
    "- **Efficiency:** Depthwise separable convolutions (lightweight)\n",
    "- **Transfer Learning:** Frozen weights ‚Üí ~70% parameters not trained\n",
    "- **Custom Initial Conv:** Trainable 32-channel layer for full-resolution skip connection\n",
    "\n",
    "#### **Bottleneck (ASPP Module)**\n",
    "- **Output Channels:** 512\n",
    "- **Multi-Scale Branches:**\n",
    "  - 1√ó1 convolution (point-wise features)\n",
    "  - 3√ó3 atrous convolutions with dilation rates [6, 12, 18]\n",
    "  - Global average pooling (image-level context with GroupNorm)\n",
    "- **Regularization:** Dropout (0.5) to prevent overfitting\n",
    "- **Fusion:** Concatenate all branches ‚Üí 1√ó1 conv projection to 512 channels\n",
    "\n",
    "#### **Decoder Path (Trainable)**\n",
    "- **5 Upsampling Blocks:** 512 ‚Üí 256 ‚Üí 128 ‚Üí 64 ‚Üí 32 ‚Üí 32 filters\n",
    "- **Upsampling Method:** ConvTranspose2d (2√ó2, stride=2) for learned upsampling\n",
    "- **Skip Connections:** SE-enhanced encoder features concatenated with decoder\n",
    "- **Each Block:** ResidualBlockSE (two 3√ó3 convs + skip connection + SE attention)\n",
    "\n",
    "#### **Channel Attention (SE Blocks)**\n",
    "- **Application Points:**\n",
    "  - After every ResidualBlockSE in decoder\n",
    "  - On skip connections before concatenation\n",
    "- **Mechanism:** Squeeze (global pooling) ‚Üí Excitation (FC-ReLU-FC-Sigmoid)\n",
    "- **Reduction Ratio:** 16 (balances performance vs parameters)\n",
    "\n",
    "#### **Output Layer**\n",
    "- **Final Convolution:** 1√ó1 conv ‚Üí single channel\n",
    "- **No Activation:** Outputs raw logits (not probabilities)\n",
    "- **Loss Compatibility:** BCEWithLogitsLoss applies sigmoid internally for numerical stability\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Key Advantages\n",
    "\n",
    "**Efficiency:**\n",
    "- ‚úì MobileNetV2 uses depthwise separable convolutions (fewer parameters, faster inference)\n",
    "- ‚úì Frozen encoder reduces trainable parameters by ~70%\n",
    "- ‚úì Smaller memory footprint ‚Üí can use larger batch sizes or higher resolution\n",
    "\n",
    "**Effectiveness:**\n",
    "- ‚úì Transfer learning from ImageNet ‚Üí better feature extraction\n",
    "- ‚úì ASPP captures multi-scale context ‚Üí handles various fetal head sizes\n",
    "- ‚úì SE blocks enhance informative channels ‚Üí improves segmentation quality\n",
    "- ‚úì Residual connections ‚Üí easier optimization and gradient flow\n",
    "\n",
    "**Stability:**\n",
    "- ‚úì Raw logits output prevents double sigmoid application\n",
    "- ‚úì BCEWithLogitsLoss combines sigmoid + BCE in one numerically stable operation\n",
    "- ‚úì DiceBCEWithLogitsLoss auto-computes `pos_weight` for class balance\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Training Configuration\n",
    "\n",
    "**Loss Function:** DiceBCEWithLogitsLoss (80% Dice + 20% BCE)  \n",
    "**Optimizer:** Adam (lr=3e-4, weight_decay=1e-4)  \n",
    "**Scheduler:** ReduceLROnPlateau (factor=0.5, patience=10)  \n",
    "**Epochs:** 50-100 with early stopping (patience=15)  \n",
    "**Batch Size:** 8-16 (depending on GPU memory)  \n",
    "**Augmentation:** HorizontalFlip, VerticalFlip, Rotation (¬±20¬∞), ShiftScaleRotate (¬±10%)  \n",
    "\n",
    "---\n",
    "\n",
    "### üìä Notebook Structure\n",
    "\n",
    "1. **Environment Setup** - Clone repository, install dependencies, verify GPU\n",
    "2. **Configuration Loading** - Load YAML config and adjust paths for Colab\n",
    "3. **Model Initialization** - Instantiate architecture, count parameters\n",
    "4. **Loss & Optimizer** - Configure training components\n",
    "5. **Data Loading** - Create datasets with augmentation pipeline\n",
    "6. **Data Verification** - Validate preprocessing before training\n",
    "7. **Training Functions** - Define train/validate logic\n",
    "8. **Training Loop** - Execute full training with checkpointing\n",
    "9. **Visualization** - Plot training curves and metrics\n",
    "10. **Evaluation** - Test best model and generate predictions\n",
    "11. **Download Results** - Export trained model and visualizations\n",
    "12. **Summary** - Review performance and next steps\n",
    "\n",
    "---\n",
    "\n",
    "### ‚è±Ô∏è Estimated Training Time\n",
    "\n",
    "- **Google Colab T4 GPU (Free Tier):** 3-5 hours for 100 epochs\n",
    "- **Google Colab A100 GPU (Colab Pro):** 1-2 hours for 100 epochs\n",
    "- **V100 GPU:** 1.5-3 hours for 100 epochs\n",
    "\n",
    "**Note:** Actual time depends on dataset size, batch size, and early stopping\n",
    "\n",
    "---\n",
    "\n",
    "### üìö References\n",
    "\n",
    "**Model Architecture:**\n",
    "- **MobileNetV2:** [Sandler et al., 2018](https://arxiv.org/abs/1801.04381)\n",
    "- **ASPP (DeepLabv3):** [Chen et al., 2017](https://arxiv.org/abs/1706.05587)\n",
    "- **Squeeze-and-Excitation Networks:** [Hu et al., 2018](https://arxiv.org/abs/1709.01507)\n",
    "- **U-Net:** [Ronneberger et al., 2015](https://arxiv.org/abs/1505.04597)\n",
    "\n",
    "**Dataset:**\n",
    "- **HC18 Challenge:** [van den Heuvel et al., 2018](https://hc18.grand-challenge.org/)\n",
    "- **Large-Scale Dataset:** [Zenodo Repository](https://zenodo.org/records/8265464)\n",
    "\n",
    "---\n",
    "\n",
    "**Let's begin! Run the cells sequentially from top to bottom.** ‚¨áÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cf26ad",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "### Google Colab Configuration\n",
    "\n",
    "This notebook is designed to run on **Google Colab** with GPU acceleration for efficient training.\n",
    "\n",
    "**Repository Information:**\n",
    "- **GitHub Repository:** `https://github.com/TrinhThaiSonDHQT/Fetal-Head-Segmentation`\n",
    "- **Clone Method:** HTTPS (no authentication required)\n",
    "\n",
    "**Directory Structure:**\n",
    "```\n",
    "/content/\n",
    "‚îú‚îÄ‚îÄ Fetal-Head-Segmentation/          # Cloned repository (project root)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ efficient_focus/              # Efficiency-focused models\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ shared/                       # Shared utilities and datasets\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îî‚îÄ‚îÄ outputs/                          # Training outputs (not in repo)\n",
    "    ‚îî‚îÄ‚îÄ results/\n",
    "        ‚îú‚îÄ‚îÄ checkpoints/              # Model weights (.pth files)\n",
    "        ‚îú‚îÄ‚îÄ logs/                     # Training curves and metrics\n",
    "        ‚îú‚îÄ‚îÄ predictions/              # Sample prediction visualizations\n",
    "        ‚îî‚îÄ‚îÄ visualizations/           # Additional analysis plots\n",
    "```\n",
    "\n",
    "**Setup Steps:**\n",
    "1. **Clone Repository:** Download project from GitHub to `/content/Fetal-Head-Segmentation/`\n",
    "2. **Install Dependencies:** Install Albumentations 1.3.1 (PyTorch, NumPy, OpenCV pre-installed)\n",
    "3. **Configure Paths:** Set up project root and output directories\n",
    "4. **Import Modules:** Load model, dataset, and training utilities\n",
    "5. **Verify CUDA:** Ensure GPU acceleration is available\n",
    "\n",
    "**Important Notes:**\n",
    "- Output directory (`/content/outputs/`) is stored in Colab's temporary runtime\n",
    "- Download results before runtime disconnects (12-hour limit on free tier)\n",
    "- GPU recommended: T4 (free tier) or A100/V100 (Colab Pro)\n",
    "- Expected training time: 2-4 hours on T4 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a831552f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the GitHub repository\n",
    "import os\n",
    "\n",
    "# Check if already cloned\n",
    "if not os.path.exists('/content/Fetal-Head-Segmentation'):\n",
    "    print(\"Cloning repository from GitHub...\")\n",
    "    !git clone https://github.com/TrinhThaiSonDHQT/Fetal-Head-Segmentation.git\n",
    "    print(\"‚úì Repository cloned successfully\")\n",
    "else:\n",
    "    print(\"‚úì Repository already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb5a71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"[Google Colab Setup]\")\n",
    "\n",
    "# Setup paths for Google Colab\n",
    "project_root = Path('/content/Fetal-Head-Segmentation')\n",
    "output_root = Path('/content/outputs')\n",
    "cache_root = output_root / 'cache'\n",
    "\n",
    "# Verify project exists\n",
    "if not project_root.exists():\n",
    "    raise RuntimeError(\n",
    "        f\"Project not found at {project_root}\\n\"\n",
    "        f\"Please run the previous cell to clone the repository from GitHub.\"\n",
    "    )\n",
    "\n",
    "if not (project_root / 'efficient_focus').exists():\n",
    "    raise RuntimeError(\n",
    "        f\"'efficient_focus' folder not found in {project_root}\\n\"\n",
    "        f\"Please ensure the repository was cloned correctly.\"\n",
    "    )\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Output root: {output_root}\")\n",
    "\n",
    "# Add project to path\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"\\n‚úì Environment setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95c0271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for Google Colab\n",
    "print(\"Installing required packages...\")\n",
    "\n",
    "# Colab has most packages pre-installed (PyTorch, NumPy, Matplotlib, OpenCV)\n",
    "# Pin Albumentations to 1.3.1 for compatibility with both Colab and Kaggle\n",
    "!pip install -q albumentations==1.3.1\n",
    "\n",
    "print(\"\\n‚úì Packages installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7de075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from efficient_focus.src.losses import DiceBCEWithLogitsLoss\n",
    "from efficient_focus.src.models.mobinet_aspp_residual_se.mobinet_aspp_residual_se import MobileNetV2ASPPResidualSEUNet, count_parameters\n",
    "\n",
    "from shared.src.data import LargeScaleDataset\n",
    "from shared.src.metrics.segmentation_metrics import dice_coefficient, iou_score, pixel_accuracy\n",
    "from shared.src.utils.visualization import save_prediction_grid, visualize_sample\n",
    "from shared.src.utils.transforms import get_transforms\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93459378",
   "metadata": {},
   "source": [
    "## 2. Configuration Loading\n",
    "\n",
    "### YAML Configuration File\n",
    "\n",
    "This notebook uses a centralized YAML configuration file for reproducible experiments:\n",
    "\n",
    "**Configuration File:** `efficient_focus/configs/mobinet_aspp_residual_se_config_v2.yaml`\n",
    "\n",
    "**Key Configuration Sections:**\n",
    "\n",
    "1. **Model Architecture:**\n",
    "   - Pre-trained MobileNetV2 encoder (ImageNet weights)\n",
    "   - Encoder freeze status (reduces trainable parameters)\n",
    "   - SE block reduction ratio (channel attention strength)\n",
    "   - ASPP module parameters (atrous rates, dropout, normalization)\n",
    "\n",
    "2. **Training Hyperparameters:**\n",
    "   - Optimizer settings (Adam with learning rate, weight decay)\n",
    "   - Learning rate scheduler (ReduceLROnPlateau parameters)\n",
    "   - Batch size and number of epochs\n",
    "   - Early stopping patience\n",
    "\n",
    "3. **Loss Function:**\n",
    "   - DiceBCEWithLogitsLoss weights (Dice vs BCE ratio)\n",
    "   - Auto-weighting for class imbalance handling\n",
    "   - Numerical stability parameters\n",
    "\n",
    "4. **Dataset Paths:**\n",
    "   - Training, validation, and test set directories\n",
    "   - Automatic path resolution (relative to project root)\n",
    "\n",
    "5. **Logging Configuration:**\n",
    "   - Checkpoint directory (model weights)\n",
    "   - Log directory (training curves)\n",
    "   - Prediction/visualization directories\n",
    "\n",
    "**Path Adjustment for Google Colab:**\n",
    "- Original paths in YAML are relative to project root\n",
    "- This cell adjusts output paths to `/content/outputs/results/`\n",
    "- Ensures outputs are stored in Colab's workspace (not in cloned repo)\n",
    "\n",
    "**Benefits of YAML Configuration:**\n",
    "- ‚úì Centralized hyperparameter management\n",
    "- ‚úì Easy experiment tracking and reproducibility\n",
    "- ‚úì No hardcoded values in notebook cells\n",
    "- ‚úì Version control friendly (track config changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ccbd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration for MobileNetV2-Based ASPP Residual SE U-Net\n",
    "config_path = project_root / 'efficient_focus' / 'configs' / 'mobinet_aspp_residual_se_config.yaml'\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Adjust output paths for Google Colab environment\n",
    "print(f\"Adjusting paths for Google Colab environment...\")\n",
    "config['logging']['checkpoint_dir'] = str(output_root / 'results' / 'checkpoints')\n",
    "config['logging']['log_dir'] = str(output_root / 'results' / 'logs')\n",
    "config['logging']['prediction_dir'] = str(output_root / 'results' / 'predictions')\n",
    "config['logging']['visualization_dir'] = str(output_root / 'results' / 'visualizations')\n",
    "\n",
    "print(f\"  Outputs will be saved to: {output_root / 'results'}\")\n",
    "\n",
    "print(\"\\nConfiguration loaded:\")\n",
    "print(f\"  Model: {config['model']['name']}\")\n",
    "print(f\"  Pre-trained: {config['model']['pretrained']}\")\n",
    "print(f\"  Freeze Encoder: {config['model']['freeze_encoder']}\")\n",
    "print(f\"  SE Reduction Ratio: {config['model']['reduction_ratio']}\")\n",
    "print(f\"  ASPP Atrous Rates: {config['model']['atrous_rates']}\")\n",
    "print(f\"  ASPP Dropout: {config['model']['aspp_dropout']}\")\n",
    "print(f\"  ASPP Use GroupNorm: {config['model']['aspp_use_groupnorm']}\")\n",
    "print(f\"  Learning Rate: {config['training']['optimizer']['lr']}\")\n",
    "print(f\"  Weight Decay: {config['training']['optimizer']['weight_decay']}\")\n",
    "print(f\"  Loss Function: {config['loss']['name']}\")\n",
    "print(f\"  Batch Size: {config['training']['batch_size']}\")\n",
    "print(f\"  Epochs: {config['training']['num_epochs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3423668",
   "metadata": {},
   "source": [
    "## 3. Model Initialization\n",
    "\n",
    "### MobileNetV2-Based ASPP Residual SE U-Net Architecture Details\n",
    "\n",
    "**Encoder Path (MobileNetV2 - Frozen, Pre-trained):**\n",
    "- Pre-trained on ImageNet for transfer learning\n",
    "- Feature extraction at 5 scales: [H/2, H/4, H/8, H/16, H/32]\n",
    "- Channel progression: 16 ‚Üí 24 ‚Üí 32 ‚Üí 96 ‚Üí 1280\n",
    "- Depthwise separable convolutions (efficient)\n",
    "- **Frozen weights** ‚Üí ~70% of parameters not trained\n",
    "- Custom initial conv (trainable) for full-resolution skip connection (32 channels)\n",
    "\n",
    "**Bottleneck (ASPP Module):**\n",
    "- Multi-scale feature extraction, output: 512 channels\n",
    "- **1√ó1 convolution**: Point-wise features\n",
    "- **3√ó3 atrous convolutions**: Dilation rates [6, 12, 18] for multi-scale context\n",
    "- **Global Average Pooling**: Image-level features with GroupNorm\n",
    "- **Dropout (0.5)**: Regularization to prevent overfitting\n",
    "- **Fusion**: Concatenate all branches and project to 512 channels\n",
    "\n",
    "**Decoder Path (Trainable):**\n",
    "- 5 upsampling blocks: 512 ‚Üí 256 ‚Üí 128 ‚Üí 64 ‚Üí 32 ‚Üí 32 filters\n",
    "- Upsampling: ConvTranspose2d (2√ó2, stride=2)\n",
    "- Skip connections: SE-enhanced encoder features concatenated with decoder\n",
    "- Each block: ResidualBlockSE after concatenation\n",
    "\n",
    "**Channel Attention (SE Blocks):**\n",
    "- Applied after every ResidualBlockSE in decoder\n",
    "- Applied to skip connections before concatenation\n",
    "- Squeeze: Global average pooling\n",
    "- Excitation: FC ‚Üí ReLU ‚Üí FC ‚Üí Sigmoid\n",
    "- Reduction ratio: 16 (balances performance vs. parameters)\n",
    "\n",
    "**Output Layer:**\n",
    "- Conv2d (1√ó1) to single channel\n",
    "- **No sigmoid activation** ‚Üí outputs raw logits\n",
    "- Sigmoid applied by loss function (BCEWithLogitsLoss) for numerical stability\n",
    "\n",
    "**Key Advantages:**\n",
    "- ‚úì **Efficient**: MobileNetV2 uses depthwise separable convolutions\n",
    "- ‚úì **Transfer Learning**: Pre-trained weights from ImageNet\n",
    "- ‚úì **Fast Training**: Frozen encoder reduces trainable parameters by ~70%\n",
    "- ‚úì **Less Memory**: Can use larger batch sizes or higher resolution images\n",
    "- ‚úì **Multi-scale Context**: ASPP captures features at multiple scales\n",
    "- ‚úì **Channel Attention**: SE blocks enhance feature representation\n",
    "- ‚úì **Numerically Stable**: Raw logits prevent double sigmoid application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef60416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(config['device'] if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize MobileNetV2-Based ASPP Residual SE U-Net\n",
    "model = MobileNetV2ASPPResidualSEUNet(\n",
    "    in_channels=config['model']['in_channels'],\n",
    "    out_channels=config['model']['out_channels'],\n",
    "    pretrained=config['model']['pretrained'],\n",
    "    freeze_encoder=config['model']['freeze_encoder'],\n",
    "    reduction_ratio=config['model']['reduction_ratio'],\n",
    "    atrous_rates=config['model']['atrous_rates'],\n",
    "    aspp_dropout=config['model']['aspp_dropout'],\n",
    "    aspp_use_groupnorm=config['model']['aspp_use_groupnorm']\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params, trainable_params, frozen_params = count_parameters(model)\n",
    "\n",
    "# Model summary\n",
    "print(f\"\\nMobileNetV2-Based ASPP Residual SE U-Net Architecture:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.1f}%)\")\n",
    "print(f\"  Frozen parameters: {frozen_params:,} ({100 * frozen_params / total_params:.1f}%)\")\n",
    "print(f\"  Total model size: ~{total_params * 4 / (1024**2):.2f} MB (float32)\")\n",
    "print(f\"  Trainable size: ~{trainable_params * 4 / (1024**2):.2f} MB (float32)\")\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  Input channels: {config['model']['in_channels']}\")\n",
    "print(f\"  Output channels: {config['model']['out_channels']}\")\n",
    "print(f\"  Pre-trained encoder: {config['model']['pretrained']}\")\n",
    "print(f\"  Freeze encoder: {config['model']['freeze_encoder']}\")\n",
    "print(f\"  SE reduction ratio: {config['model']['reduction_ratio']}\")\n",
    "print(f\"  ASPP atrous rates: {config['model']['atrous_rates']}\")\n",
    "print(f\"  ASPP dropout: {config['model']['aspp_dropout']}\")\n",
    "print(f\"  ASPP use GroupNorm: {config['model']['aspp_use_groupnorm']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ed3aa7",
   "metadata": {},
   "source": [
    "## 4. Loss Function and Optimizer\n",
    "\n",
    "### Loss Function: DiceBCEWithLogitsLoss (Hybrid Loss)\n",
    "\n",
    "A weighted combination of **Dice Loss** and **BCEWithLogits Loss**.\n",
    "\n",
    "**Dice Loss (80%):** Optimizes region overlap\n",
    "  - Directly optimizes the Dice coefficient metric\n",
    "  - Effective for imbalanced segmentation (small target regions)\n",
    "  - Handles class imbalance naturally\n",
    "\n",
    "**BCEWithLogits Loss (20%):** Optimizes pixel-wise classification\n",
    "  - Numerically stable (combines sigmoid + BCE in one operation)\n",
    "  - Handles boundary refinement\n",
    "  - Auto-computes `pos_weight` from data to balance classes\n",
    "\n",
    "**Key Features:**\n",
    "- Expects **logits** (raw values before sigmoid) as input\n",
    "- Auto-computes `pos_weight` from first batch for class balance\n",
    "- Smooth parameter (1e-6) for numerical stability in Dice calculation\n",
    "\n",
    "### Optimizer: Adam\n",
    "- Adaptive learning rate per parameter\n",
    "- Learning rate: 3e-4 (higher for faster decoder training, encoder frozen)\n",
    "- Weight decay: 1e-4 (L2 regularization to prevent decoder overfitting)\n",
    "\n",
    "### Learning Rate Scheduler: ReduceLROnPlateau\n",
    "- Monitors validation Dice coefficient (mode='max')\n",
    "- Reduces LR by factor of 0.5 when validation plateaus\n",
    "- Patience: 10 epochs\n",
    "- Minimum LR: 1e-6\n",
    "- Helps fine-tune convergence and escape local minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8d5edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_config = config['loss']\n",
    "dice_weight_config = loss_config.get('dice_weight', 0.8)\n",
    "bce_weight_config = loss_config.get('bce_weight', 0.2)\n",
    "smooth_config = loss_config.get('smooth', 1.0e-6)\n",
    "pos_weight_config = loss_config.get('pos_weight', None)\n",
    "auto_weight_config = loss_config.get('auto_weight', True)\n",
    "\n",
    "# Use DiceBCEWithLogitsLoss\n",
    "criterion = DiceBCEWithLogitsLoss(\n",
    "    dice_weight=dice_weight_config,\n",
    "    bce_weight=bce_weight_config,\n",
    "    pos_weight=pos_weight_config,\n",
    "    auto_weight=auto_weight_config,\n",
    "    smooth=smooth_config\n",
    ")\n",
    "\n",
    "print(f\"Loss Function: DiceBCELoss\")\n",
    "print(f\"  Dice weight: {dice_weight_config}\")\n",
    "print(f\"  BCE weight: {bce_weight_config}\")\n",
    "print(f\"  Smooth parameter: {smooth_config}\")\n",
    "\n",
    "# Optimizer (Adam)\n",
    "optimizer_config = config['training']['optimizer']\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=optimizer_config['lr'],\n",
    "    betas=tuple(optimizer_config['betas']),\n",
    "    eps=optimizer_config['eps'],\n",
    "    weight_decay=optimizer_config['weight_decay']\n",
    ")\n",
    "print(f\"\\nOptimizer: Adam\")\n",
    "print(f\"  Learning rate: {optimizer_config['lr']}\")\n",
    "print(f\"  Weight decay: {optimizer_config['weight_decay']}\")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler_config = config['training']['scheduler']\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=scheduler_config['mode'],\n",
    "    factor=scheduler_config['factor'],\n",
    "    patience=scheduler_config['patience'],\n",
    "    min_lr=scheduler_config['min_lr']\n",
    ")\n",
    "print(f\"\\nScheduler: ReduceLROnPlateau\")\n",
    "print(f\"  Mode: {scheduler_config['mode']}\")\n",
    "print(f\"  Factor: {scheduler_config['factor']}\")\n",
    "print(f\"  Patience: {scheduler_config['patience']}\")\n",
    "print(f\"  Min LR: {scheduler_config['min_lr']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d490a309",
   "metadata": {},
   "source": [
    "## 5. Data Loading and Augmentation\n",
    "\n",
    "### Preprocessing Pipeline (All Images)\n",
    "\n",
    "Applied consistently to training, validation, and test sets:\n",
    "1. **Normalization:** Divide pixel values by 255.0 ‚Üí [0, 1] range\n",
    "2. **Resizing:** 256√ó256 pixels (maintains aspect ratio consistency)\n",
    "3. **Tensor Conversion:** NumPy array ‚Üí PyTorch tensor (C√óH√óW format)\n",
    "\n",
    "### Data Augmentation (Training Only)\n",
    "\n",
    "**On-the-fly augmentation** using Albumentations library:\n",
    "- **HorizontalFlip:** p=0.5 (mirrors left-right)\n",
    "- **VerticalFlip:** p=0.5 (mirrors top-bottom)\n",
    "- **Rotation:** ¬±20¬∞ with p=0.5 (handles probe orientation variations)\n",
    "- **ShiftScaleRotate:** p=0.5\n",
    "  - Translation: ¬±10% (handles positioning variations)\n",
    "  - Scaling: ¬±10% (handles zoom variations)\n",
    "\n",
    "**Benefits:**\n",
    "- Augmentation applied **per epoch** ‚Üí different samples each time\n",
    "- Improves model generalization and robustness\n",
    "- Prevents overfitting on small datasets\n",
    "- Image-mask transforms synchronized automatically\n",
    "\n",
    "**Validation/Test:**\n",
    "- **No augmentation** applied\n",
    "- Only preprocessing (normalize, resize, tensorize)\n",
    "- Ensures consistent evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f51a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = config['data']\n",
    "training_config = config['training']\n",
    "\n",
    "# Helper to build paths\n",
    "def get_path(config_path):\n",
    "    \"\"\"Helper to handle both absolute and relative paths\"\"\"\n",
    "    p = Path(config_path)\n",
    "    if p.is_absolute():\n",
    "        return str(p)\n",
    "    else:\n",
    "        return str(project_root / config_path)\n",
    "\n",
    "# Create augmentation transforms\n",
    "print(\"Creating augmentation transforms...\")\n",
    "train_transform = get_transforms(height=256, width=256, is_train=True)\n",
    "val_transform = get_transforms(height=256, width=256, is_train=False)\n",
    "print(\"  Train transform: WITH augmentation (HorizontalFlip, VerticalFlip, Rotation, ShiftScaleRotate)\")\n",
    "print(\"  Val transform: WITHOUT augmentation (resize + normalize only)\")\n",
    "\n",
    "# Create datasets - using LargeScaleDataset for on-the-fly augmentation\n",
    "print(\"\\nCreating training dataset...\")\n",
    "train_dataset = LargeScaleDataset(\n",
    "    image_dir=get_path(data_config['train_images']),\n",
    "    mask_dir=get_path(data_config['train_masks']),\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "print(\"Creating validation dataset...\")\n",
    "val_dataset = LargeScaleDataset(\n",
    "    image_dir=get_path(data_config['val_images']),\n",
    "    mask_dir=get_path(data_config['val_masks']),\n",
    "    transform=val_transform\n",
    ")\n",
    "\n",
    "print(\"Creating test dataset...\")\n",
    "test_dataset = LargeScaleDataset(\n",
    "    image_dir=get_path(data_config['test_images']),\n",
    "    mask_dir=get_path(data_config['test_masks']),\n",
    "    transform=val_transform\n",
    ")\n",
    "\n",
    "# Use num_workers=0 for Google Colab to avoid multiprocessing issues\n",
    "num_workers = 0\n",
    "print(f\"\\nDataLoader settings:\")\n",
    "print(f\"  num_workers: {num_workers} (disabled for Google Colab)\")\n",
    "print(f\"  Batch size: {training_config['batch_size']}\")\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=training_config['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=training_config['pin_memory']\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=training_config['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=training_config['pin_memory']\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=training_config['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=training_config['pin_memory']\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Datasets Ready:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Train samples: {len(train_dataset)}\")\n",
    "print(f\"  Validation samples: {len(val_dataset)}\")\n",
    "print(f\"  Test samples: {len(test_dataset)}\")\n",
    "print(f\"  Batch size: {training_config['batch_size']}\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")\n",
    "print(f\"  Train augmentation: ENABLED (on-the-fly)\")\n",
    "print(f\"  Val/Test augmentation: DISABLED\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7081dcb4",
   "metadata": {},
   "source": [
    "## 6. Data Quality Verification\n",
    "\n",
    "### Pre-Training Data Validation\n",
    "\n",
    "**Critical checks before training to prevent silent failures:**\n",
    "\n",
    "1. **Tensor Shape Verification:**\n",
    "   - Images: `[B, 1, 256, 256]` (batch, grayscale, height, width)\n",
    "   - Masks: `[B, 1, 256, 256]` (same dimensions as images)\n",
    "\n",
    "2. **Value Range Checks:**\n",
    "   - Images: Should be in `[0, 1]` after normalization\n",
    "   - Masks: **Must be binary {0, 1}** (not {0, 255})\n",
    "\n",
    "3. **Mask Statistics:**\n",
    "   - Unique values: Should only contain `{0, 1}`\n",
    "   - Foreground ratio: Typically 2-10% for fetal head segmentation\n",
    "   - Too low (<1%): Possible mask preprocessing error\n",
    "   - Too high (>30%): Possible incorrect mask or target\n",
    "\n",
    "4. **Visual Inspection:**\n",
    "   - Display one sample from training batch\n",
    "   - Verify image-mask alignment\n",
    "   - Check for obvious augmentation artifacts\n",
    "\n",
    "**Why This Matters:**\n",
    "- ‚ö†Ô∏è Masks in `{0, 255}` instead of `{0, 1}` ‚Üí training divergence\n",
    "- ‚ö†Ô∏è Unnormalized images ‚Üí gradient explosion\n",
    "- ‚ö†Ô∏è Misaligned image-mask pairs ‚Üí model learns incorrect associations\n",
    "- ‚ö†Ô∏è Extreme foreground ratios ‚Üí potential data pipeline bugs\n",
    "\n",
    "**Expected Output:**\n",
    "- ‚úì Masks are binary {0, 1}\n",
    "- ‚úì Foreground ratio is reasonable (2-10%)\n",
    "- ‚úì Visual sample shows proper image-mask correspondence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f56d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of training data\n",
    "sample_images, sample_masks = next(iter(train_loader))\n",
    "\n",
    "print(f\"Sample batch:\")\n",
    "print(f\"  Images shape: {sample_images.shape}\")\n",
    "print(f\"  Masks shape: {sample_masks.shape}\")\n",
    "print(f\"  Image range: [{sample_images.min():.4f}, {sample_images.max():.4f}]\")\n",
    "print(f\"  Mask range: [{sample_masks.min():.4f}, {sample_masks.max():.4f}]\")\n",
    "print(f\"  Mask unique values: {torch.unique(sample_masks)}\")\n",
    "print(f\"  Mask mean (% foreground): {sample_masks.mean():.4f}\")\n",
    "\n",
    "# CRITICAL CHECK: Ensure masks are binary {0, 1}\n",
    "if not torch.all((sample_masks == 0) | (sample_masks == 1)):\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: Masks are not binary! Check preprocessing.\")\n",
    "else:\n",
    "    print(\"\\n‚úì Masks are properly binary {0, 1}\")\n",
    "\n",
    "# Check if masks have reasonable foreground ratio (2-10% typical for fetal head)\n",
    "fg_ratio = sample_masks.mean().item()\n",
    "if fg_ratio < 0.01 or fg_ratio > 0.3:\n",
    "    print(f\"‚ö†Ô∏è  WARNING: Unusual foreground ratio: {fg_ratio:.2%} (expected 2-10%)\")\n",
    "else:\n",
    "    print(f\"‚úì Foreground ratio looks reasonable: {fg_ratio:.2%}\")\n",
    "\n",
    "# Visualize first sample\n",
    "visualize_sample(sample_images[0], sample_masks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2a803c",
   "metadata": {},
   "source": [
    "## 7. Training and Validation Functions\n",
    "\n",
    "### Core Training Components\n",
    "\n",
    "#### `train_one_epoch()`\n",
    "**Purpose:** Execute one complete pass through the training dataset\n",
    "\n",
    "**Workflow:**\n",
    "1. Set model to training mode (`model.train()`)\n",
    "   - Enables dropout layers\n",
    "   - Updates BatchNorm statistics\n",
    "2. Iterate through training batches\n",
    "3. For each batch:\n",
    "   - Move data to GPU (`images.to(device)`)\n",
    "   - Forward pass: `logits = model(images)`\n",
    "   - Compute loss: `loss = criterion(logits, masks)`\n",
    "   - Backward pass: `loss.backward()`\n",
    "   - Update weights: `optimizer.step()`\n",
    "4. Return average epoch loss\n",
    "\n",
    "**Key Details:**\n",
    "- Model outputs **raw logits** (no sigmoid)\n",
    "- Loss function expects logits (BCEWithLogitsLoss)\n",
    "- Progress bar shows real-time loss\n",
    "\n",
    "---\n",
    "\n",
    "#### `validate()`\n",
    "**Purpose:** Evaluate model on validation set without updating weights\n",
    "\n",
    "**Workflow:**\n",
    "1. Set model to evaluation mode (`model.eval()`)\n",
    "   - Disables dropout layers\n",
    "   - Freezes BatchNorm statistics\n",
    "2. Disable gradient computation (`with torch.no_grad()`)\n",
    "3. For each validation batch:\n",
    "   - Forward pass: `logits = model(images)`\n",
    "   - Compute loss: `loss = criterion(logits, masks)`\n",
    "   - Convert logits to predictions:\n",
    "     - Apply sigmoid: `probs = torch.sigmoid(logits)`\n",
    "     - Threshold: `preds = (probs > 0.5).float()`\n",
    "   - Calculate metrics per sample:\n",
    "     - **Dice Coefficient:** Region overlap metric\n",
    "     - **IoU (Intersection over Union):** Segmentation accuracy\n",
    "     - **Pixel Accuracy:** Correct pixel classification rate\n",
    "4. Return average metrics across all validation samples\n",
    "\n",
    "**Important Notes:**\n",
    "- Validation uses same loss function as training (for monitoring)\n",
    "- Metrics computed on **binary predictions** (after sigmoid + threshold)\n",
    "- No weight updates during validation (read-only mode)\n",
    "- Used for model selection (save best based on Dice coefficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9898354",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MODEL-LOSS COMPATIBILITY VERIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test with a small batch\n",
    "sample_images, sample_masks = next(iter(train_loader))\n",
    "sample_images = sample_images.to(device)\n",
    "sample_masks = sample_masks.to(device)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    model_outputs = model(sample_images)\n",
    "\n",
    "print(f\"\\n1. Model Output Analysis:\")\n",
    "print(f\"   Shape: {model_outputs.shape}\")\n",
    "print(f\"   Range: [{model_outputs.min().item():.4f}, {model_outputs.max().item():.4f}]\")\n",
    "print(f\"   Mean: {model_outputs.mean().item():.4f}\")\n",
    "print(f\"   Std: {model_outputs.std().item():.4f}\")\n",
    "\n",
    "# Check if outputs are logits (should have negative values and values > 1)\n",
    "has_negative = (model_outputs < 0).any().item()\n",
    "has_large_positive = (model_outputs > 1).any().item()\n",
    "\n",
    "if has_negative or has_large_positive:\n",
    "    print(f\"   ‚úì Outputs are LOGITS (raw values before sigmoid)\")\n",
    "    print(f\"     - Has negative values: {has_negative}\")\n",
    "    print(f\"     - Has values > 1: {has_large_positive}\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  WARNING: Outputs look like probabilities [0,1], not logits!\")\n",
    "    print(f\"     - Model may have sigmoid in output layer\")\n",
    "\n",
    "print(f\"\\n2. Loss Function Test:\")\n",
    "loss_value = criterion(model_outputs, sample_masks)\n",
    "print(f\"   Loss value: {loss_value.item():.6f}\")\n",
    "print(f\"   ‚úì Loss computed successfully\")\n",
    "\n",
    "print(f\"\\n3. Validation Metrics Test:\")\n",
    "# Apply sigmoid to get probabilities for metrics\n",
    "probs = torch.sigmoid(model_outputs)\n",
    "preds = (probs > 0.5).float()\n",
    "\n",
    "print(f\"   Probabilities after sigmoid:\")\n",
    "print(f\"     Range: [{probs.min().item():.4f}, {probs.max().item():.4f}]\")\n",
    "print(f\"     Mean: {probs.mean().item():.4f}\")\n",
    "\n",
    "dice = dice_coefficient(preds[0], sample_masks[0]).item()\n",
    "iou = iou_score(preds[0], sample_masks[0]).item()\n",
    "pa = pixel_accuracy(preds[0], sample_masks[0])\n",
    "\n",
    "print(f\"   Sample metrics (before training):\")\n",
    "print(f\"     Dice: {dice:.4f}\")\n",
    "print(f\"     IoU: {iou:.4f}\")\n",
    "print(f\"     PA: {pa:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úì ALL CHECKS PASSED - Model and loss are compatible!\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b68a8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, device, epoch):\n",
    "    \"\"\"\n",
    "    Train for one epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1} [Train]\", leave=False)\n",
    "    for batch_idx, (images, masks) in enumerate(pbar):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images)  # Model outputs raw logits\n",
    "        \n",
    "        # Calculate loss (DiceBCEWithLogitsLoss expects logits)\n",
    "        loss = criterion(logits, masks)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update statistics\n",
    "        running_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, device, epoch):\n",
    "    \"\"\"\n",
    "    Validate the model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    dice_scores = []\n",
    "    iou_scores = []\n",
    "    pa_scores = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1} [Val]\", leave=False)\n",
    "        for batch_idx, (images, masks) in enumerate(pbar):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(images)  # Model outputs raw logits\n",
    "            \n",
    "            # Calculate loss (DiceBCEWithLogitsLoss expects logits)\n",
    "            loss = criterion(logits, masks)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Calculate metrics\n",
    "            # Apply sigmoid to convert logits to probabilities [0, 1]\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs > 0.5).float()\n",
    "            \n",
    "            for i in range(images.size(0)):\n",
    "                dice = dice_coefficient(preds[i], masks[i])\n",
    "                iou = iou_score(preds[i], masks[i])\n",
    "                pa = pixel_accuracy(preds[i], masks[i])\n",
    "                \n",
    "                dice_scores.append(dice.item())\n",
    "                iou_scores.append(iou.item())\n",
    "                pa_scores.append(pa.item())\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': f\"{loss.item():.4f}\",\n",
    "                'dice': f\"{np.mean(dice_scores):.4f}\"\n",
    "            })\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    val_loss = running_loss / len(dataloader)\n",
    "    val_dice = np.mean(dice_scores)\n",
    "    val_iou = np.mean(iou_scores)\n",
    "    val_pa = np.mean(pa_scores)\n",
    "    \n",
    "    return val_loss, val_dice, val_iou, val_pa\n",
    "\n",
    "\n",
    "print(\"Training functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2c0afb",
   "metadata": {},
   "source": [
    "## 8. Training Loop\n",
    "\n",
    "### Complete Training Pipeline with Monitoring and Checkpointing\n",
    "\n",
    "#### Training Configuration\n",
    "\n",
    "**Primary Hyperparameters:**\n",
    "- **Number of Epochs:** Configurable (typically 50-100)\n",
    "- **Early Stopping Patience:** Stop if no validation improvement for N epochs\n",
    "- **Metric for Best Model:** Dice coefficient (primary segmentation metric)\n",
    "- **Checkpoint Strategy:** Save only the best model (highest validation Dice)\n",
    "\n",
    "---\n",
    "\n",
    "#### Per-Epoch Workflow\n",
    "\n",
    "1. **Training Phase:**\n",
    "   - Execute `train_one_epoch()` on entire training set\n",
    "   - Model weights updated via backpropagation\n",
    "   - Returns average training loss\n",
    "\n",
    "2. **Validation Phase:**\n",
    "   - Execute `validate()` on validation set\n",
    "   - No weight updates (evaluation only)\n",
    "   - Returns: validation loss, Dice, IoU, pixel accuracy\n",
    "\n",
    "3. **Learning Rate Adjustment:**\n",
    "   - `ReduceLROnPlateau` monitors validation Dice\n",
    "   - Reduces LR by factor of 0.5 if no improvement for 10 epochs\n",
    "   - Helps escape local minima and fine-tune convergence\n",
    "\n",
    "4. **History Logging:**\n",
    "   - Track all metrics per epoch for visualization\n",
    "   - Stored in `history` dictionary: losses, metrics, learning rate\n",
    "\n",
    "5. **Model Checkpointing:**\n",
    "   - Save complete checkpoint if validation Dice improves:\n",
    "     - Model weights (`model_state_dict`)\n",
    "     - Optimizer state (`optimizer_state_dict`)\n",
    "     - Scheduler state (`scheduler_state_dict`)\n",
    "     - Best Dice score and full training history\n",
    "     - Original configuration (for reproducibility)\n",
    "   - Only one checkpoint saved (overwrites previous best)\n",
    "\n",
    "6. **Early Stopping Check:**\n",
    "   - Count consecutive epochs without improvement\n",
    "   - Stop training if patience limit reached\n",
    "   - Prevents overfitting and saves compute time\n",
    "\n",
    "---\n",
    "\n",
    "#### Console Output\n",
    "\n",
    "**Epoch Summary Display:**\n",
    "- Training loss (current epoch)\n",
    "- Validation loss, Dice, IoU, pixel accuracy\n",
    "- üèÜ indicator when new best Dice achieved\n",
    "- ‚¨áÔ∏è indicator when learning rate reduced\n",
    "- ‚ö†Ô∏è warning with early stopping countdown\n",
    "\n",
    "**Final Summary:**\n",
    "- Best achieved Dice coefficient\n",
    "- Best IoU and pixel accuracy\n",
    "- Total training time\n",
    "\n",
    "---\n",
    "\n",
    "#### Training Monitoring Tips\n",
    "\n",
    "**Good Training Signs:**\n",
    "- Training loss decreases steadily\n",
    "- Validation Dice increases and plateaus\n",
    "- Small gap between train/val loss (no overfitting)\n",
    "\n",
    "**Warning Signs:**\n",
    "- Validation Dice oscillates wildly ‚Üí reduce learning rate\n",
    "- Large train/val loss gap ‚Üí overfitting (add regularization)\n",
    "- Both losses plateau early ‚Üí increase model capacity or check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c20750f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{'='*60}\")\n",
    "print(f\"Starting Training - ASPP-Enhanced Residual SE U-Net\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Training configuration\n",
    "num_epochs = config['training']['num_epochs']\n",
    "patience = config['training']['early_stopping_patience']\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'val_dice': [],\n",
    "    'val_iou': [],\n",
    "    'val_pa': [],\n",
    "    'lr': []\n",
    "}\n",
    "\n",
    "best_dice = 0.0\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "print(f\"Epochs: {num_epochs}\")\n",
    "print(f\"Early Stopping Patience: {patience}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Main training loop\n",
    "for epoch in range(num_epochs):    \n",
    "    # Train\n",
    "    train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device, epoch)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_dice, val_iou, val_pa = validate(model, val_loader, criterion, device, epoch)\n",
    "    \n",
    "    # Update learning rate\n",
    "    old_lr = optimizer.param_groups[0]['lr']\n",
    "    scheduler.step(val_dice)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Update history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_dice'].append(val_dice)\n",
    "    history['val_iou'].append(val_iou)\n",
    "    history['val_pa'].append(val_pa)\n",
    "    history['lr'].append(current_lr)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"{'='*100}\")\n",
    "    dice_indicator = ' üèÜ' if val_dice > best_dice else ''\n",
    "    lr_change = f' ‚¨áÔ∏è (reduced from {old_lr:.6f})' if current_lr < old_lr else ''\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Dice: {val_dice:.4f}{dice_indicator}\")\n",
    "    print(f\"Val mIoU: {val_iou:.4f} | Val mPA: {val_pa:.4f}\")\n",
    "    if lr_change: print(f\"LR: {current_lr:.6f}{lr_change}\")\n",
    "    print(f\"{'='*100}\")\n",
    "    \n",
    "    # Check for improvement\n",
    "    is_best = val_dice > best_dice\n",
    "    if is_best:\n",
    "        best_dice = val_dice\n",
    "        epochs_without_improvement = 0\n",
    "        \n",
    "        # Save best model\n",
    "        checkpoint_dir = Path(get_path(config['logging']['checkpoint_dir']))\n",
    "        checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "        best_model_path = checkpoint_dir / 'best_model_aspp_residual_se_unet.pth'\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'best_dice': best_dice,\n",
    "            'history': history,\n",
    "            'config': config\n",
    "        }, best_model_path)\n",
    "        \n",
    "        print(f\"  ‚Üí Saved best model (Dice: {best_dice:.4f})\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        print(f\"  ‚ö†Ô∏è  No improvement for {epochs_without_improvement}/{patience} epochs\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"‚õî EARLY STOPPING TRIGGERED\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"  Stopped at epoch: {epoch+1}\")\n",
    "        print(f\"  Best Dice Score:  {best_dice:.4f}\")\n",
    "        print(f\"  Patience limit:   {patience} epochs without improvement\")\n",
    "        print(f\"{'='*70}\")\n",
    "        break\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training Completed!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Best Validation Dice: {best_dice:.4f}\")\n",
    "print(f\"Best Validation IoU:  {max(history['val_iou']):.4f}\")\n",
    "print(f\"Best Validation PA:   {max(history['val_pa']):.4f}\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b29807",
   "metadata": {},
   "source": [
    "## 9. Training Visualization\n",
    "\n",
    "### Comprehensive Training Dynamics Analysis\n",
    "\n",
    "Visualize training progress across multiple dimensions to diagnose model behavior:\n",
    "\n",
    "---\n",
    "\n",
    "#### Plot 1: Loss Curves (Top-Left)\n",
    "**Training vs Validation Loss**\n",
    "\n",
    "**What to Look For:**\n",
    "- ‚úì **Both decreasing:** Model is learning effectively\n",
    "- ‚úì **Converging:** Training reaching optimal point\n",
    "- ‚ö†Ô∏è **Diverging (val > train):** Overfitting ‚Üí add regularization or early stopping\n",
    "- ‚ö†Ô∏è **Plateauing early:** Underfitting ‚Üí increase model capacity or learning rate\n",
    "\n",
    "**Interpretation:**\n",
    "- Small gap: Good generalization\n",
    "- Large gap: Model memorizing training data\n",
    "\n",
    "---\n",
    "\n",
    "#### Plot 2: Dice Coefficient (Top-Right)\n",
    "**Validation Performance Trend**\n",
    "\n",
    "**What to Look For:**\n",
    "- ‚úì **Steady increase:** Model improving segmentation quality\n",
    "- ‚úì **Plateau near 0.95-0.98:** Expected for fetal head segmentation\n",
    "- Red dashed line: Best achieved Dice (target for model selection)\n",
    "\n",
    "**Interpretation:**\n",
    "- Dice > 0.95: Excellent segmentation\n",
    "- Dice < 0.90: Potential issues (check data, architecture, or hyperparameters)\n",
    "\n",
    "---\n",
    "\n",
    "#### Plot 3: IoU Score (Bottom-Left)\n",
    "**Intersection over Union Metric**\n",
    "\n",
    "**What to Look For:**\n",
    "- Should correlate with Dice (similar trend)\n",
    "- IoU typically 2-5% lower than Dice (more strict metric)\n",
    "- Useful for comparing with other segmentation papers\n",
    "\n",
    "**Interpretation:**\n",
    "- IoU > 0.90: Strong performance\n",
    "- IoU closely follows Dice: Consistent predictions\n",
    "\n",
    "---\n",
    "\n",
    "#### Plot 4: Learning Rate Schedule (Bottom-Right)\n",
    "**ReduceLROnPlateau Behavior**\n",
    "\n",
    "**What to Look For:**\n",
    "- Step-downs when validation plateaus\n",
    "- Log scale shows magnitude changes clearly\n",
    "- Should see 2-4 reductions during training\n",
    "\n",
    "**Interpretation:**\n",
    "- Early reductions: Fast convergence, may need higher initial LR\n",
    "- No reductions: Validation improving consistently (good sign)\n",
    "- Many reductions: Model struggling, check architecture or data\n",
    "\n",
    "---\n",
    "\n",
    "#### Output Files\n",
    "\n",
    "**Saved Visualization:**\n",
    "- File: `results/logs/training_curves.png`\n",
    "- Resolution: 150 DPI (publication quality)\n",
    "- Use for: Model comparison, experiment tracking, thesis documentation\n",
    "\n",
    "**Download from Colab:**\n",
    "```python\n",
    "from google.colab import files\n",
    "files.download('content/outputs/results/logs/training_curves.png')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97005d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Loss curves\n",
    "axes[0, 0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0, 0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Loss (Dice + BCE)', fontsize=12)\n",
    "axes[0, 0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=11)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Dice coefficient\n",
    "axes[0, 1].plot(history['val_dice'], label='Val Dice', color='green', linewidth=2)\n",
    "axes[0, 1].axhline(y=best_dice, color='red', linestyle='--', label=f'Best: {best_dice:.4f}')\n",
    "axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Dice Coefficient', fontsize=12)\n",
    "axes[0, 1].set_title('Validation Dice Coefficient', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=11)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# IoU\n",
    "axes[1, 0].plot(history['val_iou'], label='Val IoU', color='orange', linewidth=2)\n",
    "axes[1, 0].axhline(y=max(history['val_iou']), color='red', linestyle='--', \n",
    "                   label=f\"Best: {max(history['val_iou']):.4f}\")\n",
    "axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 0].set_ylabel('IoU Score', fontsize=12)\n",
    "axes[1, 0].set_title('Validation IoU Score', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend(fontsize=11)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "axes[1, 1].plot(history['lr'], label='Learning Rate', color='red', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Learning Rate', fontsize=12)\n",
    "axes[1, 1].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].legend(fontsize=11)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save figure\n",
    "log_dir = Path(get_path(config['logging']['log_dir']))\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(log_dir / 'training_curves.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"Training curves saved to {log_dir / 'training_curves.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc6834e",
   "metadata": {},
   "source": [
    "## 10. Model Evaluation and Inference\n",
    "\n",
    "### Test Set Evaluation with Best Model\n",
    "\n",
    "**Objective:** Assess final model performance on held-out test set\n",
    "\n",
    "---\n",
    "\n",
    "#### Evaluation Workflow\n",
    "\n",
    "1. **Load Best Checkpoint:**\n",
    "   - Restore model weights from epoch with highest validation Dice\n",
    "   - Set model to evaluation mode (`model.eval()`)\n",
    "   - Display checkpoint metadata (epoch number, best Dice score)\n",
    "\n",
    "2. **Generate Predictions:**\n",
    "   - Forward pass on test batch: `logits = model(images)`\n",
    "   - Convert logits to binary masks:\n",
    "     - Apply sigmoid: `probs = torch.sigmoid(logits)` ‚Üí probabilities [0, 1]\n",
    "     - Threshold at 0.5: `preds = (probs > 0.5).float()` ‚Üí binary {0, 1}\n",
    "\n",
    "3. **Calculate Per-Sample Metrics:**\n",
    "   - **Dice Coefficient:** Measures region overlap (2√ó|A‚à©B| / (|A|+|B|))\n",
    "   - **IoU (Jaccard Index):** Measures segmentation accuracy (|A‚à©B| / |A‚à™B|)\n",
    "   - **Pixel Accuracy:** Percentage of correctly classified pixels\n",
    "\n",
    "4. **Visualize Results:**\n",
    "   - Display 4 test samples in 3-column grid:\n",
    "     - Column 1: Input ultrasound image (grayscale)\n",
    "     - Column 2: Ground truth mask (binary)\n",
    "     - Column 3: Model prediction + metrics overlay\n",
    "   - Save visualization to `results/predictions/sample_predictions.png`\n",
    "\n",
    "---\n",
    "\n",
    "#### Interpretation Guide\n",
    "\n",
    "**Visual Assessment:**\n",
    "- ‚úì **Smooth boundaries:** Model captures fetal head contour well\n",
    "- ‚úì **Minimal noise:** Clean segmentation (no scattered false positives)\n",
    "- ‚ö†Ô∏è **Fragmented predictions:** Potential underfitting or poor data quality\n",
    "- ‚ö†Ô∏è **Over-segmentation:** Model including surrounding structures\n",
    "\n",
    "**Metric Ranges:**\n",
    "- **Dice ‚â• 0.97:** Excellent (state-of-the-art)\n",
    "- **Dice 0.90-0.97:** Good (acceptable for clinical use)\n",
    "- **Dice < 0.90:** Poor (needs model/data improvement)\n",
    "\n",
    "**Common Issues:**\n",
    "- Dice high, but visual mismatch ‚Üí Check mask preprocessing\n",
    "- Consistent under-segmentation ‚Üí Adjust threshold (<0.5)\n",
    "- Consistent over-segmentation ‚Üí Adjust threshold (>0.5)\n",
    "\n",
    "---\n",
    "\n",
    "#### Output Files\n",
    "\n",
    "**Generated Artifacts:**\n",
    "1. `sample_predictions.png`: 4-sample visualization grid\n",
    "2. Per-sample metrics (printed to console)\n",
    "3. Visual comparison for qualitative assessment\n",
    "\n",
    "---\n",
    "\n",
    "#### Next Steps After Evaluation\n",
    "\n",
    "1. **If performance is satisfactory (Dice ‚â• 0.95):**\n",
    "   - Download checkpoint for deployment (see Section 11)\n",
    "   - Generate predictions on full test set\n",
    "   - Calculate aggregate statistics\n",
    "\n",
    "2. **If performance is suboptimal (Dice < 0.90):**\n",
    "   - Analyze failure cases (which samples have low Dice?)\n",
    "   - Check data quality (incorrect masks, artifacts)\n",
    "   - Tune hyperparameters (learning rate, loss weights)\n",
    "   - Try architectural modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d67ce74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint_path = Path(get_path(config['logging']['checkpoint_dir'])) / 'best_model_aspp_residual_se_unet.pth'\n",
    "checkpoint = torch.load(checkpoint_path, weights_only=False)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loaded best model from epoch {checkpoint['epoch']+1}\")\n",
    "print(f\"Best Dice Score: {checkpoint['best_dice']:.4f}\")\n",
    "\n",
    "# Get test samples\n",
    "test_images, test_masks = next(iter(test_loader))\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "# Generate predictions\n",
    "with torch.no_grad():\n",
    "    # Model outputs raw logits - apply sigmoid to get probabilities\n",
    "    test_logits = model(test_images)\n",
    "    test_probs = torch.sigmoid(test_logits)  # Convert logits to [0, 1] probabilities\n",
    "    test_preds = (test_probs > 0.5).float()  # Threshold at 0.5\n",
    "\n",
    "# Visualize predictions\n",
    "num_samples = min(4, len(test_images))\n",
    "fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4 * num_samples))\n",
    "\n",
    "if num_samples == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for i in range(num_samples):\n",
    "    # Move to CPU and convert to numpy\n",
    "    img = test_images[i, 0].cpu().numpy()\n",
    "    mask = test_masks[i, 0].numpy()\n",
    "    pred = test_preds[i, 0].cpu().numpy()\n",
    "    \n",
    "    # Calculate metrics for this sample (ensure both tensors on same device)\n",
    "    pred_tensor = test_preds[i].cpu()\n",
    "    mask_tensor = test_masks[i].to(pred_tensor.device)\n",
    "    dice = dice_coefficient(pred_tensor, mask_tensor).item()\n",
    "    iou = iou_score(pred_tensor, mask_tensor).item()\n",
    "    pa = pixel_accuracy(pred_tensor, mask_tensor)\n",
    "    \n",
    "    # Input image\n",
    "    axes[i, 0].imshow(img, cmap='gray')\n",
    "    axes[i, 0].set_title('Input Image', fontsize=12, fontweight='bold')\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    # Ground truth\n",
    "    axes[i, 1].imshow(mask, cmap='gray')\n",
    "    axes[i, 1].set_title('Ground Truth', fontsize=12, fontweight='bold')\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    # Prediction\n",
    "    axes[i, 2].imshow(pred, cmap='gray')\n",
    "    axes[i, 2].set_title(f'Prediction\\nDice: {dice:.4f} | IoU: {iou:.4f} | PA: {pa:.4f}', \n",
    "                         fontsize=10, fontweight='bold')\n",
    "    axes[i, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save predictions\n",
    "pred_dir = Path(get_path(config['logging']['prediction_dir']))\n",
    "pred_dir.mkdir(parents=True, exist_ok=True)\n",
    "save_prediction_grid(test_images[:4].cpu(), test_masks[:4], test_preds[:4].cpu(), \n",
    "                    str(pred_dir / 'sample_predictions.png'), num_samples=4)\n",
    "print(f\"Sample predictions saved to {pred_dir / 'sample_predictions.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6c6fd2",
   "metadata": {},
   "source": [
    "## 11. Download Results from Google Colab\n",
    "\n",
    "### Export Training Artifacts\n",
    "\n",
    "**Important:** Google Colab runtimes are temporary. Download all results before disconnecting.\n",
    "\n",
    "#### Available Outputs\n",
    "\n",
    "1. **Model Checkpoint** (`best_model_aspp_residual_se_unet.pth`)\n",
    "   - Contains: Model weights, optimizer state, training history\n",
    "   - Use for: Inference, transfer learning, deployment\n",
    "\n",
    "2. **Training Curves** (`training_curves.png`)\n",
    "   - Contains: Loss, Dice, IoU, learning rate plots\n",
    "   - Use for: Performance analysis, thesis documentation\n",
    "\n",
    "3. **Sample Predictions** (`sample_predictions.png`)\n",
    "   - Contains: Visual comparison of 4 test samples\n",
    "   - Use for: Qualitative assessment, presentations\n",
    "\n",
    "#### Download Methods\n",
    "\n",
    "**Method 1: Download Individual Files**\n",
    "```python\n",
    "from google.colab import files\n",
    "\n",
    "# Download best model checkpoint\n",
    "files.download('/content/outputs/results/checkpoints/best_model_aspp_residual_se_unet.pth')\n",
    "\n",
    "# Download training curves\n",
    "files.download('/content/outputs/results/logs/training_curves.png')\n",
    "\n",
    "# Download sample predictions\n",
    "files.download('/content/outputs/results/predictions/sample_predictions.png')\n",
    "```\n",
    "\n",
    "**Method 2: Download All Results as ZIP**\n",
    "```python\n",
    "# Create ZIP archive of all results\n",
    "!cd /content/outputs && zip -r results.zip results/\n",
    "\n",
    "# Download ZIP file\n",
    "from google.colab import files\n",
    "files.download('/content/outputs/results.zip')\n",
    "```\n",
    "\n",
    "**Method 3: Mount Google Drive (Automatic Backup)**\n",
    "```python\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Copy results to Google Drive\n",
    "!cp -r /content/outputs/results /content/drive/MyDrive/Fetal_Head_Segmentation_Results/\n",
    "```\n",
    "\n",
    "#### Runtime Warnings\n",
    "\n",
    "‚ö†Ô∏è **Colab Free Tier Limitations:**\n",
    "- 12-hour maximum runtime\n",
    "- Runtime disconnects if idle for 90 minutes\n",
    "- All `/content/` data is lost after disconnect\n",
    "\n",
    "**Best Practice:** Download results immediately after training completes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79547a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Download individual files (run each line separately)\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Downloading model checkpoint...\")\n",
    "files.download('/content/outputs/results/checkpoints/best_model_aspp_residual_se_unet.pth')\n",
    "\n",
    "print(\"Downloading training curves...\")\n",
    "files.download('/content/outputs/results/logs/training_curves.png')\n",
    "\n",
    "print(\"Downloading sample predictions...\")\n",
    "files.download('/content/outputs/results/predictions/sample_predictions.png')\n",
    "\n",
    "print(\"\\n‚úì All files downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e08fdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Download all results as a single ZIP archive\n",
    "import os\n",
    "\n",
    "print(\"Creating ZIP archive of all results...\")\n",
    "os.chdir('/content/outputs')\n",
    "!zip -r results.zip results/\n",
    "\n",
    "print(\"\\nDownloading ZIP archive...\")\n",
    "from google.colab import files\n",
    "files.download('/content/outputs/results.zip')\n",
    "\n",
    "print(\"\\n‚úì Complete results archive downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859aecb0",
   "metadata": {},
   "source": [
    "## 12. Training Summary and Next Steps\n",
    "\n",
    "### Experiment Summary\n",
    "\n",
    "**Model Architecture:** MobileNetV2-Based ASPP Residual SE U-Net\n",
    "\n",
    "**Key Results:**\n",
    "- Best Validation Dice: [Displayed after training]\n",
    "- Best Validation IoU: [Displayed after training]\n",
    "- Best Validation Pixel Accuracy: [Displayed after training]\n",
    "- Total Training Time: [Varies by hardware]\n",
    "- Final Model Size: ~11-13 MB (frozen encoder)\n",
    "\n",
    "**Architecture Highlights:**\n",
    "- Pre-trained MobileNetV2 encoder (ImageNet weights)\n",
    "- ASPP bottleneck for multi-scale context\n",
    "- SE blocks for channel attention\n",
    "- ~70% parameters frozen (efficient training)\n",
    "\n",
    "---\n",
    "\n",
    "### Model Performance Checklist\n",
    "\n",
    "**Excellent Performance (Ready for Deployment):**\n",
    "- ‚úì Validation Dice ‚â• 0.97\n",
    "- ‚úì Validation IoU ‚â• 0.95\n",
    "- ‚úì Visual predictions match ground truth closely\n",
    "- ‚úì Smooth segmentation boundaries\n",
    "- ‚úì Low false positive rate\n",
    "\n",
    "**Good Performance (Acceptable with Minor Tuning):**\n",
    "- ‚óã Validation Dice 0.90-0.97\n",
    "- ‚óã Some boundary roughness\n",
    "- ‚óã Occasional false positives in difficult cases\n",
    "\n",
    "**Needs Improvement:**\n",
    "- ‚úó Validation Dice < 0.90\n",
    "- ‚úó Fragmented or over-segmented predictions\n",
    "- ‚úó Large train-validation gap (overfitting)\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "#### 1. If Performance is Satisfactory (Dice ‚â• 0.95)\n",
    "\n",
    "**Deployment Tasks:**\n",
    "- Download checkpoint for inference pipeline\n",
    "- Test on external datasets (different ultrasound machines)\n",
    "- Optimize for real-time inference (ONNX, TensorRT)\n",
    "- Implement clinical validation protocol\n",
    "\n",
    "**Analysis Tasks:**\n",
    "- Generate predictions on full test set\n",
    "- Calculate aggregate statistics (mean, std, min, max Dice)\n",
    "- Analyze failure cases (which samples have low Dice?)\n",
    "- Create ROC curves and other diagnostic plots\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. If Performance Needs Improvement (Dice < 0.90)\n",
    "\n",
    "**Data Quality:**\n",
    "- Verify mask preprocessing (should be binary {0, 1})\n",
    "- Check image-mask alignment\n",
    "- Inspect samples with lowest Dice scores\n",
    "- Consider data augmentation adjustments\n",
    "\n",
    "**Hyperparameter Tuning:**\n",
    "- Increase learning rate (try 5e-4 or 1e-3)\n",
    "- Adjust loss weights (try 0.7 Dice / 0.3 BCE)\n",
    "- Modify augmentation intensity\n",
    "- Experiment with batch size\n",
    "\n",
    "**Architecture Modifications:**\n",
    "- Unfreeze encoder for fine-tuning (after initial training)\n",
    "- Adjust SE reduction ratio (try 8 or 32)\n",
    "- Modify ASPP atrous rates\n",
    "- Add more decoder channels\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Comparative Analysis\n",
    "\n",
    "**Benchmark Against Other Models:**\n",
    "- Standard U-Net (baseline)\n",
    "- Attention U-Net (attention mechanisms)\n",
    "- Residual SE U-Net (without MobileNetV2)\n",
    "\n",
    "**Metrics to Compare:**\n",
    "- Dice coefficient, IoU, pixel accuracy\n",
    "- Training time and convergence speed\n",
    "- Model size and inference time\n",
    "- Robustness to data variations\n",
    "\n",
    "---\n",
    "\n",
    "### Citation and Acknowledgment\n",
    "\n",
    "**If using this implementation, please cite:**\n",
    "\n",
    "```\n",
    "@misc{fetal-head-segmentation,\n",
    "  author = {Trinh Thai Son},\n",
    "  title = {Fetal Head Segmentation using MobileNetV2-Based ASPP Residual SE U-Net},\n",
    "  year = {2025},\n",
    "  publisher = {GitHub},\n",
    "  url = {https://github.com/TrinhThaiSonDHQT/Fetal-Head-Segmentation}\n",
    "}\n",
    "```\n",
    "\n",
    "**Dataset Citation:**\n",
    "```\n",
    "@article{van2018automatic,\n",
    "  title={Automatic fetal head detection and measurement in ultrasound images by iterative randomized Hough transform},\n",
    "  author={Van Den Heuvel, Thomas LA and De Bruijn, Dagmar and De Korte, Chris L and Van Ginneken, Bram},\n",
    "  journal={Ultrasound in medicine \\& biology},\n",
    "  volume={44},\n",
    "  number={1},\n",
    "  pages={1--11},\n",
    "  year={2018},\n",
    "  publisher={Elsevier}\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Contact and Support\n",
    "\n",
    "**Project Repository:** https://github.com/TrinhThaiSonDHQT/Fetal-Head-Segmentation\n",
    "\n",
    "**For Issues or Questions:**\n",
    "- Open an issue on GitHub\n",
    "- Check documentation in `/docs/` folder\n",
    "- Review other model implementations in `/accuracy_focus/`\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations on completing the training! üéâ**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
