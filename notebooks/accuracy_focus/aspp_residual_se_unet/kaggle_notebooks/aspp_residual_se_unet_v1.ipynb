{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f60927af",
   "metadata": {},
   "source": [
    "# ASPP-Enhanced Residual SE U-Net for Fetal Head Segmentation\n",
    "## Training Notebook (Kaggle Compatible)\n",
    "\n",
    "**Platform:** Google Colab with GPU acceleration\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "This notebook implements an **ASPP-Enhanced Residual U-Net with Squeeze-and-Excitation (SE)** mechanism for medical image segmentation with the following components:\n",
    "\n",
    "**Core Architecture:**\n",
    "- U-Net encoder-decoder structure with skip connections\n",
    "- ResidualBlockSE (two 3√ó3 convs + SE + residual skip connection)\n",
    "- MaxPool2d downsampling (encoder) and ConvTranspose2d upsampling (decoder)\n",
    "\n",
    "**Key Innovations:**\n",
    "\n",
    "1. **Residual Blocks with SE Attention:**\n",
    "   - Two 3√ó3 convolutional layers per block with BatchNorm and ReLU\n",
    "   - Squeeze-and-Excitation blocks for channel-wise attention\n",
    "   - Residual skip connections for improved gradient flow\n",
    "   - Applied throughout encoder and decoder paths\n",
    "\n",
    "2. **ASPP Module at Bottleneck:**\n",
    "   - Multi-scale feature extraction using atrous (dilated) convolutions\n",
    "   - Parallel branches with different dilation rates [6, 12, 18]\n",
    "   - 1√ó1 convolution for point-wise features\n",
    "   - Global average pooling branch for image-level context\n",
    "   - Captures features at multiple scales simultaneously\n",
    "\n",
    "3. **SE Blocks on Skip Connections:**\n",
    "   - Channel-wise attention applied before concatenation\n",
    "   - Learns to emphasize important feature channels\n",
    "   - Suppresses less relevant activations\n",
    "\n",
    "**Output:**\n",
    "- Sigmoid activation for binary segmentation\n",
    "- Direct probability output [0, 1] range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bbfeab",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "### Kaggle Configuration\n",
    "\n",
    "**Dataset:** `fetal-head-segmentation` (must be added to notebook)\n",
    "\n",
    "**Directory Structure:**\n",
    "- **Project Root:** `/kaggle/input/fetal-head-segmentation/` (read-only)\n",
    "- **Outputs:** `/kaggle/working/results/`\n",
    "  - Checkpoints, logs, predictions, and visualizations\n",
    "  - Automatically available for download after training completes\n",
    "\n",
    "**Steps:**\n",
    "1. Verify dataset is attached to notebook\n",
    "2. Install/upgrade dependencies (Albumentations 1.4.0, specific NumPy/SciPy versions)\n",
    "3. Import required modules and verify CUDA availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a7b3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: LOCAL\n",
      "\n",
      "[Local Setup]\n",
      "Project root: e:\\Fetal Head Segmentation\\notebooks\n",
      "\n",
      "‚úì Environment setup complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"[Kaggle Setup]\")\n",
    "\n",
    "# Setup paths for Kaggle\n",
    "project_root = Path('/kaggle/input/fhs-residual-se-unet')\n",
    "output_root = Path('/kaggle/working')\n",
    "cache_root = output_root / 'cache'\n",
    "\n",
    "# Verify dataset exists\n",
    "if not project_root.exists():\n",
    "    raise RuntimeError(\n",
    "        f\"Dataset not found at {project_root}\\n\"\n",
    "        f\"Please add the 'fhs-residual-se-unet' dataset to your Kaggle notebook.\"\n",
    "    )\n",
    "\n",
    "if not (project_root / 'accuracy_focus').exists():\n",
    "    raise RuntimeError(\n",
    "        f\"'accuracy_focus' folder not found in {project_root}\\n\"\n",
    "        f\"Please ensure your dataset structure is correct.\"\n",
    "    )\n",
    "\n",
    "print(f\"Project root: {project_root} (read-only)\")\n",
    "print(f\"Output root: {output_root} (writable)\")\n",
    "\n",
    "# Add project to path\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"\\n‚úì Environment setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67ed7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for Kaggle\n",
    "print(\"Installing required packages...\")\n",
    "\n",
    "# This ensures override Kaggle's pre-installed packages\n",
    "!pip install --force-reinstall --no-cache-dir -q \\\n",
    "    \"numpy==1.26.4\" \\\n",
    "    \"scipy==1.11.4\" \\\n",
    "    \"scikit-learn==1.5.1\" \\\n",
    "    \"albumentations==1.4.0\" \\\n",
    "    \"opencv-python-headless==4.9.0.80\" \\\n",
    "    \"PyYAML>=5.4\" \\\n",
    "    \"tqdm>=4.62\"\n",
    "\n",
    "print(\"\\n‚úì Packages installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697dbbbd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'accuracy_focus.standard_unet.src'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01maccuracy_focus\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstandard_unet\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlosses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DiceBCELoss\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mshared\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HC18Dataset\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mshared\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msegmentation_metrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dice_coefficient, iou_score, pixel_accuracy\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'accuracy_focus.standard_unet.src'"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Model architecture\n",
    "from accuracy_focus.improved_unet.src.models.aspp_residual_se_unet.aspp_residual_se_unet_model import ASPPResidualSEUNet\n",
    "from accuracy_focus.improved_unet.src.models.aspp_residual_se_unet.aspp_residual_se_unet_model import count_parameters\n",
    "\n",
    "# Loss function\n",
    "from accuracy_focus.standard_unet.src.losses import DiceBCELoss\n",
    "\n",
    "# Dataset and data utilities\n",
    "from shared.src.data import HC18Dataset\n",
    "from shared.src.utils.transforms import get_transforms\n",
    "\n",
    "# Evaluation metrics\n",
    "from shared.src.metrics.segmentation_metrics import dice_coefficient, iou_score, pixel_accuracy\n",
    "\n",
    "# Visualization utilities\n",
    "from shared.src.utils.visualization import save_prediction_grid, visualize_sample\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Environment Information\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191b164f",
   "metadata": {},
   "source": [
    "## 2. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cf7c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_path = project_root / 'accuracy_focus' / 'improved_unet' / 'configs' / 'aspp_residual_se_unet_config.yaml'\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Adjust output paths for Google Colab\n",
    "print(\"Adjusting paths for Google Colab environment...\")\n",
    "config['logging']['checkpoint_dir'] = str(output_root / 'checkpoints')\n",
    "config['logging']['log_dir'] = str(output_root / 'logs')\n",
    "config['logging']['prediction_dir'] = str(output_root / 'predictions')\n",
    "config['logging']['visualization_dir'] = str(output_root / 'visualizations')\n",
    "\n",
    "print(f\"  Outputs will be saved to: {output_root}\")\n",
    "\n",
    "print(\"\\nConfiguration loaded:\")\n",
    "print(f\"  Model: {config['model']['name']}\")\n",
    "print(f\"  Base Filters: {config['model']['base_filters']}\")\n",
    "print(f\"  SE Reduction Ratio: {config['model']['reduction_ratio']}\")\n",
    "print(f\"  ASPP Atrous Rates: {config['model']['atrous_rates']}\")\n",
    "print(f\"  ASPP Dropout: {config['model']['aspp_dropout']}\")\n",
    "print(f\"  Learning Rate: {config['training']['optimizer']['lr']}\")\n",
    "print(f\"  Loss Function: {config['loss']['name']}\")\n",
    "print(f\"  Batch Size: {config['training']['batch_size']}\")\n",
    "print(f\"  Epochs: {config['training']['num_epochs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4b8ee2",
   "metadata": {},
   "source": [
    "## 3. Model Initialization\n",
    "\n",
    "### ASPP-Enhanced Residual SE U-Net Architecture Details\n",
    "\n",
    "**Encoder Path:**\n",
    "- 4 downsampling stages: 64 ‚Üí 128 ‚Üí 256 ‚Üí 512 filters\n",
    "- Each stage: ResidualBlockSE (Conv2d + BatchNorm + ReLU) √ó 2 + SE + skip\n",
    "- Downsampling: MaxPool2d (2√ó2, stride=2)\n",
    "- SE blocks apply channel-wise attention after each residual block\n",
    "\n",
    "**Bottleneck - ASPP Module:**\n",
    "- **1024 filters** with multi-scale contextual feature extraction\n",
    "- **Parallel branches:**\n",
    "  - 1√ó1 convolution (point-wise features)\n",
    "  - 3√ó3 atrous convolutions with dilation rates [6, 12, 18]\n",
    "  - Global average pooling + 1√ó1 conv (image-level features)\n",
    "- Concatenates all branches and projects to output channels\n",
    "- Dropout for regularization (configurable)\n",
    "- Captures features at multiple receptive field sizes\n",
    "\n",
    "**Decoder Path:**\n",
    "- 4 upsampling stages: 512 ‚Üí 256 ‚Üí 128 ‚Üí 64 filters\n",
    "- Upsampling: ConvTranspose2d (2√ó2, stride=2)\n",
    "- Each stage: Concatenate with SE-enhanced skip connection + ResidualBlockSE √ó 2\n",
    "- Skip connections pass through SE blocks before concatenation\n",
    "\n",
    "**Output Layer:**\n",
    "- Conv2d (1√ó1) to single channel\n",
    "- Sigmoid activation for binary segmentation\n",
    "- Output range: [0, 1] probabilities\n",
    "\n",
    "**Key Benefits:**\n",
    "- **ASPP:** Multi-scale context improves boundary detection and handles varying object sizes\n",
    "- **Residual connections:** Better gradient flow, enables deeper networks\n",
    "- **SE attention:** Emphasizes important channels, suppresses noise\n",
    "- **Skip connection SE:** Refined feature fusion between encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309b8f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(config['device'] if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize model\n",
    "model = ASPPResidualSEUNet(\n",
    "    in_channels=config['model']['in_channels'],\n",
    "    out_channels=config['model']['out_channels'],\n",
    "    base_channels=config['model']['base_filters'],\n",
    "    reduction_ratio=config['model']['reduction_ratio'],\n",
    "    atrous_rates=config['model']['atrous_rates'],\n",
    "    aspp_dropout=config['model']['aspp_dropout']\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "total_params, trainable_params = count_parameters(model)\n",
    "\n",
    "# Model summary\n",
    "print(f\"\\nASPP-Enhanced Residual SE U-Net Architecture:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Model size: ~{total_params * 4 / (1024**2):.2f} MB (float32)\")\n",
    "print(f\"  Input channels: {config['model']['in_channels']}\")\n",
    "print(f\"  Output channels: {config['model']['out_channels']}\")\n",
    "print(f\"  Base filters: {config['model']['base_filters']}\")\n",
    "print(f\"  SE reduction ratio: {config['model']['reduction_ratio']}\")\n",
    "print(f\"  ASPP atrous rates: {config['model']['atrous_rates']}\")\n",
    "print(f\"  ASPP dropout: {config['model']['aspp_dropout']}\")\n",
    "\n",
    "# Test forward pass\n",
    "test_input = torch.randn(1, 1, 256, 256).to(device)\n",
    "test_output = model(test_input)\n",
    "print(f\"\\nTest forward pass:\")\n",
    "print(f\"  Input shape: {test_input.shape}\")\n",
    "print(f\"  Output shape: {test_output.shape}\")\n",
    "print(f\"  Output range: [{test_output.min().item():.4f}, {test_output.max().item():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7043d5",
   "metadata": {},
   "source": [
    "## 4. Loss Function and Optimization\n",
    "\n",
    "### Loss Function: DiceBCELoss\n",
    "\n",
    "**Hybrid Loss Design:**\n",
    "- **Dice Loss (70%):** Optimizes region overlap (DSC metric)\n",
    "  - Measures intersection over union of predicted and ground truth\n",
    "  - Handles class imbalance naturally\n",
    "  - Directly optimizes evaluation metric\n",
    "- **BCE Loss (30%):** Optimizes pixel-wise classification\n",
    "  - Binary cross-entropy for pixel-level accuracy\n",
    "  - Provides strong gradients for learning\n",
    "\n",
    "**Key Features:**\n",
    "- Expects **probabilities** as input (model applies sigmoid internally)\n",
    "- Smooth parameter (1e-6) prevents division by zero\n",
    "- Weighted combination balances region-level and pixel-level optimization\n",
    "\n",
    "### Optimizer: Adam\n",
    "- Adaptive learning rate per parameter\n",
    "- Efficient for large models with sparse gradients\n",
    "- Configurable: betas=(0.9, 0.999), weight decay, epsilon\n",
    "\n",
    "### Learning Rate Scheduler: ReduceLROnPlateau\n",
    "- Monitors validation Dice coefficient\n",
    "- Reduces LR when validation performance plateaus\n",
    "- Helps escape local minima and fine-tune convergence\n",
    "- Factor: 0.5 (reduces LR by half)\n",
    "- Patience: waits N epochs before reducing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1db387f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function (DiceBCELoss - Combined Dice + BCE)\n",
    "loss_config = config['loss']\n",
    "dice_weight_config = loss_config.get('dice_weight', 0.8)\n",
    "bce_weight_config = loss_config.get('bce_weight', 0.2)\n",
    "smooth_config = loss_config.get('smooth', 1e-6)\n",
    "\n",
    "criterion = DiceBCELoss(\n",
    "    dice_weight=dice_weight_config,\n",
    "    bce_weight=bce_weight_config,\n",
    "    smooth=smooth_config\n",
    ")\n",
    "print(f\"Loss Function: {loss_config['name']}\")\n",
    "print(f\"  Dice weight: {dice_weight_config}\")\n",
    "print(f\"  BCE weight: {bce_weight_config}\")\n",
    "print(f\"  Smooth parameter: {smooth_config}\")\n",
    "\n",
    "# Optimizer Adam\n",
    "optimizer_config = config['training']['optimizer']\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=optimizer_config['lr'],\n",
    "    betas=tuple(optimizer_config['betas']),\n",
    "    eps=optimizer_config['eps'],\n",
    "    weight_decay=optimizer_config['weight_decay']\n",
    ")\n",
    "print(f\"\\nOptimizer: Adam\")\n",
    "print(f\"  Learning rate: {optimizer_config['lr']}\")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler_config = config['training']['scheduler']\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=scheduler_config['mode'],\n",
    "    factor=scheduler_config['factor'],\n",
    "    patience=scheduler_config['patience'],\n",
    "    min_lr=scheduler_config['min_lr']\n",
    ")\n",
    "print(f\"\\nScheduler: ReduceLROnPlateau\")\n",
    "print(f\"  Mode: {scheduler_config['mode']}\")\n",
    "print(f\"  Factor: {scheduler_config['factor']}\")\n",
    "print(f\"  Patience: {scheduler_config['patience']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b602ff",
   "metadata": {},
   "source": [
    "## 5. Data Loading and Augmentation\n",
    "\n",
    "### Preprocessing Pipeline (All Images)\n",
    "\n",
    "Applied consistently to training, validation, and test sets:\n",
    "1. **Normalization:** Divide pixel values by 255.0 ‚Üí [0, 1] range\n",
    "2. **Resizing:** 256√ó256 pixels (maintains aspect ratio consistency)\n",
    "3. **Tensor Conversion:** NumPy array ‚Üí PyTorch tensor (C√óH√óW format)\n",
    "\n",
    "### Data Augmentation (Training Only)\n",
    "\n",
    "**On-the-fly augmentation** using Albumentations library:\n",
    "- **HorizontalFlip:** p=0.5 (mirrors left-right)\n",
    "- **VerticalFlip:** p=0.5 (mirrors top-bottom, implicit in ShiftScaleRotate)\n",
    "- **Rotation:** ¬±20¬∞ with p=0.5 (handles probe orientation variations)\n",
    "- **ShiftScaleRotate:** p=0.5\n",
    "  - Translation: ¬±10% (handles positioning variations)\n",
    "  - Scaling: ¬±10% (handles zoom variations)\n",
    "\n",
    "**Benefits:**\n",
    "- Augmentation applied **per epoch** ‚Üí different samples each time\n",
    "- Improves model generalization and robustness\n",
    "- Prevents overfitting on small datasets (999 training samples)\n",
    "- Image-mask transforms synchronized automatically\n",
    "\n",
    "**Validation/Test:**\n",
    "- **No augmentation** applied\n",
    "- Only preprocessing (normalize, resize, tensorize)\n",
    "- Ensures consistent evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979990e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = config['data']\n",
    "training_config = config['training']\n",
    "\n",
    "# Helper to build paths\n",
    "def get_path(config_path):\n",
    "    \"\"\"Helper to handle both absolute and relative paths\"\"\"\n",
    "    p = Path(config_path)\n",
    "    if p.is_absolute():\n",
    "        return str(p)\n",
    "    else:\n",
    "        return str(project_root / config_path)\n",
    "\n",
    "# Create augmentation transforms\n",
    "print(\"Creating augmentation transforms...\")\n",
    "train_transform = get_transforms(height=256, width=256, is_train=True)\n",
    "val_transform = get_transforms(height=256, width=256, is_train=False)\n",
    "print(\"  Train transform: WITH augmentation (HorizontalFlip, Rotation, ShiftScaleRotate)\")\n",
    "print(\"  Val transform: WITHOUT augmentation (resize + normalize only)\")\n",
    "\n",
    "# Create datasets - using HC18Dataset for on-the-fly augmentation\n",
    "print(\"\\nCreating training dataset...\")\n",
    "train_dataset = HC18Dataset(\n",
    "    image_dir=get_path(data_config['train_images']),\n",
    "    mask_dir=get_path(data_config['train_masks']),\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "print(\"Creating validation dataset...\")\n",
    "val_dataset = HC18Dataset(\n",
    "    image_dir=get_path(data_config['val_images']),\n",
    "    mask_dir=get_path(data_config['val_masks']),\n",
    "    transform=val_transform\n",
    ")\n",
    "\n",
    "print(\"Creating test dataset...\")\n",
    "test_dataset = HC18Dataset(\n",
    "    image_dir=get_path(data_config['test_images']),\n",
    "    mask_dir=get_path(data_config['test_masks']),\n",
    "    transform=val_transform\n",
    ")\n",
    "\n",
    "# Use num_workers=2 for Google Colab (works well with Colab's resources)\n",
    "num_workers = 2\n",
    "print(f\"\\nDataLoader settings:\")\n",
    "print(f\"  num_workers: {num_workers}\")\n",
    "print(f\"  Batch size: {training_config['batch_size']}\")\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=training_config['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=training_config['pin_memory']\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=training_config['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=training_config['pin_memory']\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=training_config['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=training_config['pin_memory']\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Datasets Ready:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Train samples: {len(train_dataset)}\")\n",
    "print(f\"  Validation samples: {len(val_dataset)}\")\n",
    "print(f\"  Test samples: {len(test_dataset)}\")\n",
    "print(f\"  Batch size: {training_config['batch_size']}\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")\n",
    "print(f\"  Train augmentation: ENABLED (on-the-fly)\")\n",
    "print(f\"  Val/Test augmentation: DISABLED\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d27e972",
   "metadata": {},
   "source": [
    "## 6. Data Quality Verification\n",
    "\n",
    "Verify data integrity before training to catch preprocessing errors early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599cadc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of training data\n",
    "sample_images, sample_masks = next(iter(train_loader))\n",
    "\n",
    "print(f\"Sample batch:\")\n",
    "print(f\"  Images shape: {sample_images.shape}\")\n",
    "print(f\"  Masks shape: {sample_masks.shape}\")\n",
    "print(f\"  Image range: [{sample_images.min():.4f}, {sample_images.max():.4f}]\")\n",
    "print(f\"  Mask range: [{sample_masks.min():.4f}, {sample_masks.max():.4f}]\")\n",
    "print(f\"  Mask unique values: {torch.unique(sample_masks)}\")\n",
    "print(f\"  Mask mean (% foreground): {sample_masks.mean():.4f}\")\n",
    "\n",
    "# CRITICAL CHECK: Ensure masks are binary {0, 1}\n",
    "if not torch.all((sample_masks == 0) | (sample_masks == 1)):\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: Masks are not binary! Check preprocessing.\")\n",
    "else:\n",
    "    print(\"\\n‚úì Masks are properly binary {0, 1}\")\n",
    "\n",
    "# Check if masks have reasonable foreground ratio (2-10% typical for fetal head)\n",
    "fg_ratio = sample_masks.mean().item()\n",
    "if fg_ratio < 0.01 or fg_ratio > 0.3:\n",
    "    print(f\"‚ö†Ô∏è  WARNING: Unusual foreground ratio: {fg_ratio:.2%} (expected 2-10%)\")\n",
    "else:\n",
    "    print(f\"‚úì Foreground ratio looks reasonable: {fg_ratio:.2%}\")\n",
    "\n",
    "# Visualize first sample\n",
    "visualize_sample(sample_images[0], sample_masks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db16d74a",
   "metadata": {},
   "source": [
    "## 7. Training and Validation Functions\n",
    "\n",
    "### train_one_epoch()\n",
    "- Sets model to training mode (enables dropout, batchnorm updates)\n",
    "- Iterates through training batches\n",
    "- Forward pass ‚Üí loss calculation ‚Üí backward pass ‚Üí optimizer step\n",
    "- Returns average epoch loss\n",
    "\n",
    "### validate()\n",
    "- Sets model to evaluation mode (disables dropout, batchnorm updates)\n",
    "- Computes loss and metrics on validation set without gradient computation\n",
    "- **Model outputs probabilities** (sigmoid applied internally)\n",
    "- Thresholds at 0.5 to generate binary predictions\n",
    "- Calculates: Dice coefficient, IoU, Pixel Accuracy\n",
    "- Returns average metrics across all validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e51cef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, device, epoch):\n",
    "    \"\"\"\n",
    "    Train for one epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1} [Train]\", leave=False)\n",
    "    for batch_idx, (images, masks) in enumerate(pbar):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, masks)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update statistics\n",
    "        running_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, device, epoch):\n",
    "    \"\"\"\n",
    "    Validate the model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    dice_scores = []\n",
    "    iou_scores = []\n",
    "    pa_scores = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1} [Val]\", leave=False)\n",
    "        for batch_idx, (images, masks) in enumerate(pbar):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, masks)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Calculate metrics\n",
    "            preds = (outputs > 0.5).float()\n",
    "            \n",
    "            for i in range(images.size(0)):\n",
    "                dice = dice_coefficient(preds[i], masks[i])\n",
    "                iou = iou_score(preds[i], masks[i])\n",
    "                pa = pixel_accuracy(preds[i], masks[i])\n",
    "                \n",
    "                dice_scores.append(dice.item())\n",
    "                iou_scores.append(iou.item())\n",
    "                pa_scores.append(pa.item())\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': f\"{loss.item():.4f}\",\n",
    "                'dice': f\"{np.mean(dice_scores):.4f}\"\n",
    "            })\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    val_loss = running_loss / len(dataloader)\n",
    "    val_dice = np.mean(dice_scores)\n",
    "    val_iou = np.mean(iou_scores)\n",
    "    val_pa = np.mean(pa_scores)\n",
    "    \n",
    "    return val_loss, val_dice, val_iou, val_pa\n",
    "\n",
    "\n",
    "print(\"Training functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadfef5f",
   "metadata": {},
   "source": [
    "## 8. Training Loop\n",
    "\n",
    "### Training Configuration\n",
    "- **Epochs:** Configurable (typically 50-100)\n",
    "- **Early Stopping:** Monitors validation Dice coefficient\n",
    "  - Stops if no improvement for N consecutive epochs\n",
    "  - Prevents overfitting and saves compute time\n",
    "- **Model Checkpointing:** Saves best model based on validation Dice\n",
    "\n",
    "### Per-Epoch Workflow\n",
    "1. Train on full training set\n",
    "2. Validate on validation set\n",
    "3. Update learning rate (ReduceLROnPlateau scheduler)\n",
    "4. Log metrics: loss, Dice, IoU, pixel accuracy\n",
    "5. Save model if validation Dice improves\n",
    "6. Check early stopping criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92874ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{'='*60}\")\n",
    "print(f\"Starting Training - ASPP-Enhanced Residual SE U-Net\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Training configuration\n",
    "num_epochs = config['training']['num_epochs']\n",
    "patience = config['training']['early_stopping_patience']\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'val_dice': [],\n",
    "    'val_iou': [],\n",
    "    'val_pa': [],\n",
    "    'lr': []\n",
    "}\n",
    "\n",
    "best_dice = 0.0\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "print(f\"Epochs: {num_epochs}\")\n",
    "print(f\"Early Stopping Patience: {patience}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Main training loop\n",
    "for epoch in range(num_epochs):    \n",
    "    # Train\n",
    "    train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device, epoch)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_dice, val_iou, val_pa = validate(model, val_loader, criterion, device, epoch)\n",
    "    \n",
    "    # Update learning rate\n",
    "    old_lr = optimizer.param_groups[0]['lr']\n",
    "    scheduler.step(val_dice)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Update history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_dice'].append(val_dice)\n",
    "    history['val_iou'].append(val_iou)\n",
    "    history['val_pa'].append(val_pa)\n",
    "    history['lr'].append(current_lr)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"{'='*100}\")\n",
    "    dice_indicator = ' üèÜ' if val_dice > best_dice else ''\n",
    "    lr_change = f' ‚¨áÔ∏è (reduced from {old_lr:.6f})' if current_lr < old_lr else ''\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Dice: {val_dice:.4f}{dice_indicator}\")\n",
    "    print(f\"Val mIoU: {val_iou:.4f} | Val mPA: {val_pa:.4f}\")\n",
    "    if lr_change: print(f\"LR: {current_lr:.6f}{lr_change}\")\n",
    "    print(f\"{'='*100}\")\n",
    "    \n",
    "    # Check for improvement\n",
    "    is_best = val_dice > best_dice\n",
    "    if is_best:\n",
    "        best_dice = val_dice\n",
    "        epochs_without_improvement = 0\n",
    "        \n",
    "        # Save best model\n",
    "        checkpoint_dir = Path(get_path(config['logging']['checkpoint_dir']))\n",
    "        checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "        best_model_path = checkpoint_dir / 'best_model_aspp_residual_se_unet_v1.pth'\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'best_dice': best_dice,\n",
    "            'history': history,\n",
    "            'config': config\n",
    "        }, best_model_path)\n",
    "        \n",
    "        print(f\"  ‚Üí Saved best model (Dice: {best_dice:.4f})\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        print(f\"  ‚ö†Ô∏è  No improvement for {epochs_without_improvement}/{patience} epochs\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"‚õî EARLY STOPPING TRIGGERED\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"  Stopped at epoch: {epoch+1}\")\n",
    "        print(f\"  Best Dice Score:  {best_dice:.4f}\")\n",
    "        print(f\"  Patience limit:   {patience} epochs without improvement\")\n",
    "        print(f\"{'='*70}\")\n",
    "        break\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training Completed!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Best Validation Dice: {best_dice:.4f}\")\n",
    "print(f\"Best Validation IoU:  {max(history['val_iou']):.4f}\")\n",
    "print(f\"Best Validation PA:   {max(history['val_pa']):.4f}\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6244960b",
   "metadata": {},
   "source": [
    "## 9. Training Visualization\n",
    "\n",
    "Generate plots to analyze training dynamics and convergence:\n",
    "- **Loss curves:** Training vs validation loss over epochs\n",
    "- **Dice coefficient:** Validation performance trend\n",
    "- **IoU score:** Intersection over Union metric progression\n",
    "- **Learning rate:** ReduceLROnPlateau schedule adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f848f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Loss curves\n",
    "axes[0, 0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0, 0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Loss (Dice + BCE)', fontsize=12)\n",
    "axes[0, 0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=11)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Dice coefficient\n",
    "axes[0, 1].plot(history['val_dice'], label='Val Dice', color='green', linewidth=2)\n",
    "axes[0, 1].axhline(y=best_dice, color='red', linestyle='--', label=f'Best: {best_dice:.4f}')\n",
    "axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Dice Coefficient', fontsize=12)\n",
    "axes[0, 1].set_title('Validation Dice Coefficient', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=11)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# IoU\n",
    "axes[1, 0].plot(history['val_iou'], label='Val IoU', color='orange', linewidth=2)\n",
    "axes[1, 0].axhline(y=max(history['val_iou']), color='red', linestyle='--', \n",
    "                   label=f\"Best: {max(history['val_iou']):.4f}\")\n",
    "axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 0].set_ylabel('IoU Score', fontsize=12)\n",
    "axes[1, 0].set_title('Validation IoU Score', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend(fontsize=11)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "axes[1, 1].plot(history['lr'], label='Learning Rate', color='red', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Learning Rate', fontsize=12)\n",
    "axes[1, 1].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].legend(fontsize=11)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save figure\n",
    "log_dir = Path(get_path(config['logging']['log_dir']))\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(log_dir / 'training_curves.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"Training curves saved to {log_dir / 'training_curves.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597f51ea",
   "metadata": {},
   "source": [
    "## 10. Model Inference and Results\n",
    "\n",
    "### Evaluation on Test Set\n",
    "\n",
    "Load the best checkpoint (highest validation Dice) and visualize predictions:\n",
    "- Compare input images, ground truth masks, and model predictions\n",
    "- Calculate per-sample metrics (Dice, IoU, Pixel Accuracy)\n",
    "- Assess segmentation quality visually\n",
    "\n",
    "**Note:** Model outputs probabilities via sigmoid, thresholded at 0.5 for binary masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e5f517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint_path = Path(get_path(config['logging']['checkpoint_dir'])) / 'best_model_aspp_residual_se_unet_v1.pth'\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, weights_only=False)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loaded best model from epoch {checkpoint['epoch']+1}\")\n",
    "print(f\"Best Dice Score: {checkpoint['best_dice']:.4f}\")\n",
    "\n",
    "# Get validation samples\n",
    "test_images, test_masks = next(iter(test_loader))\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "# Generate predictions\n",
    "with torch.no_grad():\n",
    "    test_probs = model(test_images)\n",
    "    test_preds = (test_probs > 0.5).float()\n",
    "\n",
    "# Visualize predictions\n",
    "num_samples = min(4, len(test_images))\n",
    "fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4 * num_samples))\n",
    "\n",
    "if num_samples == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for i in range(num_samples):\n",
    "    # Move to CPU and convert to numpy\n",
    "    img = test_images[i, 0].cpu().numpy()\n",
    "    mask = test_masks[i, 0].numpy()\n",
    "    pred = test_preds[i, 0].cpu().numpy()\n",
    "    \n",
    "    # Calculate metrics for this sample (ensure both tensors on same device)\n",
    "    pred_tensor = test_preds[i].cpu()\n",
    "    mask_tensor = test_masks[i].to(pred_tensor.device)\n",
    "    dice = dice_coefficient(pred_tensor, mask_tensor).item()\n",
    "    iou = iou_score(pred_tensor, mask_tensor).item()\n",
    "    pa = pixel_accuracy(pred_tensor, mask_tensor)\n",
    "    \n",
    "    # Input image\n",
    "    axes[i, 0].imshow(img, cmap='gray')\n",
    "    axes[i, 0].set_title('Input Image', fontsize=12, fontweight='bold')\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    # Ground truth\n",
    "    axes[i, 1].imshow(mask, cmap='gray')\n",
    "    axes[i, 1].set_title('Ground Truth', fontsize=12, fontweight='bold')\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    # Prediction\n",
    "    axes[i, 2].imshow(pred, cmap='gray')\n",
    "    axes[i, 2].set_title(f'Prediction\\nDice: {dice:.4f} | IoU: {iou:.4f} | PA: {pa:.4f}', \n",
    "                         fontsize=10, fontweight='bold')\n",
    "    axes[i, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save predictions\n",
    "pred_dir = Path(get_path(config['logging']['prediction_dir']))\n",
    "pred_dir.mkdir(parents=True, exist_ok=True)\n",
    "save_prediction_grid(test_images[:4].cpu(), test_masks[:4], test_preds[:4].cpu(), \n",
    "                    str(pred_dir / 'sample_predictions.png'), num_samples=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8daaca9",
   "metadata": {},
   "source": [
    "## 11. Summary and Results\n",
    "\n",
    "### Key Innovations\n",
    "\n",
    "1. **ASPP Module at Bottleneck:**\n",
    "   - Multi-scale feature extraction using atrous (dilated) convolutions\n",
    "   - Parallel branches: 1√ó1 conv, 3√ó3 atrous convs (dilation [6, 12, 18]), global pooling\n",
    "   - Captures contextual information at multiple receptive field sizes\n",
    "   - Improves boundary detection and handles varying object scales\n",
    "\n",
    "2. **Residual Blocks with SE Attention:**\n",
    "   - Two 3√ó3 convolutions with BatchNorm and ReLU\n",
    "   - Squeeze-and-Excitation for channel-wise recalibration\n",
    "   - Residual skip connections improve gradient flow\n",
    "   - Enables deeper, more stable training\n",
    "\n",
    "3. **SE Blocks on Skip Connections:**\n",
    "   - Channel-wise attention applied to encoder features before concatenation\n",
    "   - Emphasizes relevant features, suppresses noise\n",
    "   - Refined feature fusion between encoder and decoder\n",
    "\n",
    "4. **Improved Gradient Flow:**\n",
    "   - Residual connections throughout network architecture\n",
    "   - Enables training of very deep networks\n",
    "   - Reduces vanishing gradient problem\n",
    "\n",
    "### Architecture Highlights\n",
    "\n",
    "- **Encoder:** 4 stages with ResidualBlockSE (64‚Üí128‚Üí256‚Üí512 filters)\n",
    "- **Bottleneck:** ASPP module (512‚Üí1024 channels, multi-scale context)\n",
    "- **Decoder:** 4 stages with ResidualBlockSE (512‚Üí256‚Üí128‚Üí64 filters)\n",
    "- **Total Parameters:** ~38M parameters (~146 MB float32)\n",
    "\n",
    "### Training Configuration\n",
    "\n",
    "- **Loss:** DiceBCELoss (0.7 Dice + 0.3 BCE)\n",
    "- **Optimizer:** Adam (lr=1e-3, weight decay configurable)\n",
    "- **Scheduler:** ReduceLROnPlateau (factor=0.5, patience configurable)\n",
    "- **Augmentation:** HorizontalFlip, VerticalFlip, Rotation (¬±20¬∞), ShiftScaleRotate\n",
    "\n",
    "### Expected Performance\n",
    "\n",
    "The ASPP-enhanced architecture should provide significant improvements over standard U-Net:\n",
    "- **Better multi-scale understanding:** ASPP captures features at different receptive fields\n",
    "- **Improved boundary detection:** Multi-scale context helps precise segmentation\n",
    "- **Enhanced feature learning:** SE attention focuses on relevant channels\n",
    "- **More stable training:** Residual connections enable deeper networks\n",
    "- **Better generalization:** Robust to varying fetal head sizes and positions\n",
    "\n",
    "### Comparison with Other Models\n",
    "\n",
    "- **vs Standard U-Net:** +ASPP, +Residual blocks, +SE attention\n",
    "- **vs Residual SE U-Net:** +ASPP module for multi-scale context\n",
    "- **vs Attention U-Net:** +ASPP, +SE attention, +Residual connections\n",
    "\n",
    "---\n",
    "\n",
    "### Google Colab Notes\n",
    "\n",
    "**Outputs Location:**\n",
    "- Checkpoints: `/content/Fetal-Head-Segmentation/results/checkpoints/`\n",
    "- Logs: `/content/Fetal-Head-Segmentation/results/logs/`\n",
    "- Predictions: `/content/Fetal-Head-Segmentation/results/predictions/`\n",
    "- Visualizations: `/content/Fetal-Head-Segmentation/results/visualizations/`\n",
    "\n",
    "**Download Best Model:**\n",
    "```python\n",
    "from google.colab import files\n",
    "files.download('/content/Fetal-Head-Segmentation/results/checkpoints/best_model_aspp_residual_se_unet_v1.pth')\n",
    "```\n",
    "\n",
    "**Download Training Curves:**\n",
    "```python\n",
    "files.download('/content/Fetal-Head-Segmentation/results/logs/training_curves.png')\n",
    "```\n",
    "\n",
    "**Download All Results (as ZIP):**\n",
    "```python\n",
    "!cd /content/Fetal-Head-Segmentation && zip -r results.zip results/\n",
    "files.download('/content/Fetal-Head-Segmentation/results.zip')\n",
    "```\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Evaluation:**\n",
    "   - Test on HC18 Grand Challenge test set\n",
    "   - Calculate comprehensive metrics (DSC, HD95, mIoU)\n",
    "   - Compare with baseline models\n",
    "\n",
    "2. **Analysis:**\n",
    "   - Visualize ASPP feature maps at different dilation rates\n",
    "   - Analyze SE attention weights\n",
    "   - Error analysis on failure cases\n",
    "\n",
    "3. **Optimization:**\n",
    "   - Model quantization for faster inference\n",
    "   - TensorRT optimization for deployment\n",
    "   - ONNX export for cross-platform compatibility\n",
    "\n",
    "4. **Ablation Study:**\n",
    "   - Remove ASPP to measure its contribution\n",
    "   - Remove SE blocks to measure attention impact\n",
    "   - Compare with simpler architectures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
