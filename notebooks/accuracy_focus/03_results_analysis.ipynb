{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5383b9cd",
   "metadata": {},
   "source": [
    "# Results Analysis - Model Performance Evaluation\n",
    "\n",
    "This notebook analyzes the trained model's performance and creates publication-quality visualizations for thesis presentation.\n",
    "\n",
    "**Target Performance Metrics:**\n",
    "- DSC (Dice Similarity Coefficient): ≥97.81%\n",
    "- mIoU (Mean Intersection over Union): ≥97.90%\n",
    "- mPA (Mean Pixel Accuracy): ≥99.18%\n",
    "\n",
    "**Contents:**\n",
    "1. Load trained model weights\n",
    "2. Comprehensive metrics evaluation\n",
    "3. Visualization of predictions\n",
    "4. Error analysis\n",
    "5. Comparison with baseline/targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37388580",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184a1b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.data import HC18Dataset\n",
    "from src.models import ImprovedUNet\n",
    "from src.losses import DiceBCELoss\n",
    "from src.utils import get_transforms\n",
    "from train import evaluate_model\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (15, 8)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a72107",
   "metadata": {},
   "source": [
    "## 2. Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332ab068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model weights\n",
    "MODEL_PATH = '../weights/best_model.pth'\n",
    "\n",
    "# Initialize model\n",
    "model = ImprovedUNet(in_channels=1, out_channels=1).to(DEVICE)\n",
    "\n",
    "# Load checkpoint\n",
    "if Path(MODEL_PATH).exists():\n",
    "    checkpoint = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"MODEL LOADED SUCCESSFULLY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Checkpoint Epoch: {checkpoint.get('epoch', 'N/A')}\")\n",
    "    print(f\"Best Dice Score: {checkpoint.get('best_dice', 'N/A'):.4f}\")\n",
    "    if 'val_metrics' in checkpoint:\n",
    "        print(f\"Val mIoU: {checkpoint['val_metrics'].get('miou', 'N/A'):.4f}\")\n",
    "        print(f\"Val mPA: {checkpoint['val_metrics'].get('pixel_accuracy', 'N/A'):.4f}\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(f\"⚠️ Model weights not found at {MODEL_PATH}\")\n",
    "    print(\"Please train the model first using main.py or the training notebook\")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d2ef1c",
   "metadata": {},
   "source": [
    "## 3. Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c056b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset\n",
    "TEST_IMG_DIR = '../dataset/test_set/images'\n",
    "TEST_MASK_DIR = '../dataset/test_set/masks'\n",
    "\n",
    "test_transforms = get_transforms(256, 256, is_train=False)\n",
    "test_dataset = HC18Dataset(TEST_IMG_DIR, TEST_MASK_DIR, transform=test_transforms)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20c39b9",
   "metadata": {},
   "source": [
    "## 4. Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525205c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "loss_fn = DiceBCELoss()\n",
    "test_metrics = evaluate_model(test_loader, model, loss_fn, DEVICE)\n",
    "\n",
    "# Target metrics from paper\n",
    "TARGET_DSC = 0.9781\n",
    "TARGET_MIOU = 0.9790\n",
    "TARGET_MPA = 0.9918\n",
    "\n",
    "# Display results\n",
    "print(\"=\"*60)\n",
    "print(\"TEST SET PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Loss:            {test_metrics['loss']:.4f}\")\n",
    "print(f\"DSC (Dice):      {test_metrics['dice']:.4f}  (Target: {TARGET_DSC:.4f})\")\n",
    "print(f\"mIoU:            {test_metrics['miou']:.4f}  (Target: {TARGET_MIOU:.4f})\")\n",
    "print(f\"Pixel Accuracy:  {test_metrics['pixel_accuracy']:.4f}  (Target: {TARGET_MPA:.4f})\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compare with targets\n",
    "metrics_comparison = {\n",
    "    'Metric': ['DSC', 'mIoU', 'Pixel Accuracy'],\n",
    "    'Our Model': [test_metrics['dice'], test_metrics['miou'], test_metrics['pixel_accuracy']],\n",
    "    'Target (Paper)': [TARGET_DSC, TARGET_MIOU, TARGET_MPA]\n",
    "}\n",
    "df_comparison = pd.DataFrame(metrics_comparison)\n",
    "df_comparison['Difference'] = df_comparison['Our Model'] - df_comparison['Target (Paper)']\n",
    "df_comparison['Status'] = df_comparison['Difference'].apply(lambda x: '✓ Achieved' if x >= 0 else '✗ Below target')\n",
    "\n",
    "print(\"\\nMETRICS COMPARISON:\")\n",
    "print(df_comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a79d495",
   "metadata": {},
   "source": [
    "## 5. Visualize Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecb25ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target metrics\n",
    "TARGET_DSC = 0.9781\n",
    "TARGET_MIOU = 0.9790\n",
    "TARGET_MPA = 0.9918\n",
    "\n",
    "# Create comparison dataframe\n",
    "metrics_comparison = {\n",
    "    'Metric': ['DSC', 'mIoU', 'mPA'],\n",
    "    'Our Model': [DSC_SCORE, MIOU_SCORE, MPA_SCORE],\n",
    "    'Target': [TARGET_DSC, TARGET_MIOU, TARGET_MPA]\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(metrics_comparison)\n",
    "df_comparison['Difference'] = df_comparison['Our Model'] - df_comparison['Target']\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Performance Comparison\")\n",
    "print(\"=\"*70)\n",
    "print(df_comparison.to_string(index=False))\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Bar chart comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(metrics_comparison['Metric']))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, metrics_comparison['Our Model'], width, \n",
    "               label='Our Model', color='steelblue', alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, metrics_comparison['Target'], width, \n",
    "               label='Target', color='coral', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Metrics', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Model Performance vs Target Metrics', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics_comparison['Metric'])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.set_ylim([0.95, 1.0])\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.4f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5868a909",
   "metadata": {},
   "source": [
    "## 6. Prediction Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c770e6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions on test samples\n",
    "n_samples = 10\n",
    "fig, axes = plt.subplots(n_samples, 4, figsize=(16, 4*n_samples))\n",
    "\n",
    "sample_count = 0\n",
    "with torch.no_grad():\n",
    "    for images, masks in test_loader:\n",
    "        if sample_count >= n_samples:\n",
    "            break\n",
    "        \n",
    "        images_gpu = images.to(DEVICE)\n",
    "        predictions = model(images_gpu)\n",
    "        \n",
    "        for i in range(images.size(0)):\n",
    "            if sample_count >= n_samples:\n",
    "                break\n",
    "            \n",
    "            img = images[i].squeeze().cpu().numpy()\n",
    "            mask = masks[i].squeeze().cpu().numpy()\n",
    "            pred = predictions[i].squeeze().cpu().numpy()\n",
    "            pred_binary = (pred > 0.5).astype(np.float32)\n",
    "            \n",
    "            # Error map\n",
    "            error = np.abs(mask - pred_binary)\n",
    "            \n",
    "            # Plot\n",
    "            axes[sample_count, 0].imshow(img, cmap='gray')\n",
    "            axes[sample_count, 0].set_title('Input Image', fontsize=12)\n",
    "            axes[sample_count, 0].axis('off')\n",
    "            \n",
    "            axes[sample_count, 1].imshow(mask, cmap='gray')\n",
    "            axes[sample_count, 1].set_title('Ground Truth', fontsize=12)\n",
    "            axes[sample_count, 1].axis('off')\n",
    "            \n",
    "            axes[sample_count, 2].imshow(pred_binary, cmap='gray')\n",
    "            axes[sample_count, 2].set_title('Prediction', fontsize=12)\n",
    "            axes[sample_count, 2].axis('off')\n",
    "            \n",
    "            axes[sample_count, 3].imshow(error, cmap='hot', vmin=0, vmax=1)\n",
    "            axes[sample_count, 3].set_title('Error Map', fontsize=12)\n",
    "            axes[sample_count, 3].axis('off')\n",
    "            \n",
    "            sample_count += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Segmentation Results on Test Set', y=1.001, fontsize=16, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fe2c63",
   "metadata": {},
   "source": [
    "## 7. Summary & Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80118bd3",
   "metadata": {},
   "source": [
    "### Key Findings:\n",
    "\n",
    "1. **Model Performance:**\n",
    "   - The Improved U-Net successfully segments fetal heads from ultrasound images\n",
    "   - Performance metrics are compared against target benchmarks\n",
    "   \n",
    "2. **Architecture Highlights:**\n",
    "   - Residual blocks for better gradient flow\n",
    "   - ASPP module for multi-scale context\n",
    "   - Feature Pyramid + Scale Attention for enhanced feature fusion\n",
    "   \n",
    "3. **Next Steps:**\n",
    "   - If targets not met: hyperparameter tuning, more epochs, data augmentation\n",
    "   - If targets met: model optimization for inference speed\n",
    "   - Prepare publication-quality figures for thesis defense\n",
    "\n",
    "---\n",
    "\n",
    "**For full training (100 epochs), use:** `python main.py`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
