{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb8756ac",
   "metadata": {},
   "source": [
    "# Attention U-Net for Fetal Head Segmentation\n",
    "## Training Notebook (Google Colab Compatible)\n",
    "\n",
    "**Platform:** Google Colab with GPU acceleration\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "This notebook implements an **Attention U-Net** for medical image segmentation with the following components:\n",
    "\n",
    "**Core Architecture:**\n",
    "- Standard U-Net encoder-decoder structure with skip connections\n",
    "- Convolutional blocks (Conv2d ‚Üí BatchNorm ‚Üí ReLU)\n",
    "- MaxPool2d downsampling (encoder) and ConvTranspose2d upsampling (decoder)\n",
    "\n",
    "**Key Innovation - Attention Gates:**\n",
    "- Applied to skip connections before concatenation with decoder features\n",
    "- Learns spatial attention weights to focus on relevant anatomical regions\n",
    "- Suppresses irrelevant background activations automatically\n",
    "- Improves feature selection without increasing computational cost significantly\n",
    "\n",
    "**Output:**\n",
    "- Raw logits (no sigmoid activation) for numerically stable loss computation\n",
    "- Compatible with DiceBCEWithLogitsLoss for class imbalance handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57803ae",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "### Google Colab Configuration\n",
    "\n",
    "**Repository:** `https://github.com/TrinhThaiSonDHQT/Fetal-Head-Segmentation`\n",
    "\n",
    "**Directory Structure:**\n",
    "- **Project Root:** `/content/Fetal-Head-Segmentation/`\n",
    "- **Outputs:** `/content/Fetal-Head-Segmentation/results/`\n",
    "  - Checkpoints, logs, predictions, and visualizations\n",
    "  - Download results after training completes\n",
    "\n",
    "**Steps:**\n",
    "1. Clone repository from GitHub\n",
    "2. Install dependencies (Albumentations 1.3.1)\n",
    "3. Import required modules and verify CUDA availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e750a9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the GitHub repository\n",
    "import os\n",
    "\n",
    "# Check if already cloned\n",
    "if not os.path.exists('/content/Fetal-Head-Segmentation'):\n",
    "    print(\"Cloning repository from GitHub...\")\n",
    "    !git clone https://github.com/TrinhThaiSonDHQT/Fetal-Head-Segmentation.git\n",
    "    print(\"‚úì Repository cloned successfully\")\n",
    "else:\n",
    "    print(\"‚úì Repository already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f993db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"[Google Colab Setup]\")\n",
    "\n",
    "# Setup paths for Google Colab\n",
    "project_root = Path('/content/Fetal-Head-Segmentation')\n",
    "output_root = project_root / 'results'\n",
    "cache_root = output_root / 'cache'\n",
    "\n",
    "# Verify project exists\n",
    "if not project_root.exists():\n",
    "    raise RuntimeError(\n",
    "        f\"Project not found at {project_root}\\n\"\n",
    "        f\"Please run the previous cell to clone the repository from GitHub.\"\n",
    "    )\n",
    "\n",
    "if not (project_root / 'accuracy_focus').exists():\n",
    "    raise RuntimeError(\n",
    "        f\"'accuracy_focus' folder not found in {project_root}\\n\"\n",
    "        f\"Please ensure the repository was cloned correctly.\"\n",
    "    )\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Output root: {output_root}\")\n",
    "\n",
    "# Add project to path\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"\\n‚úì Environment setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdeeacac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for Google Colab\n",
    "print(\"Installing required packages...\")\n",
    "\n",
    "# Colab has most packages pre-installed (PyTorch, NumPy, Matplotlib, OpenCV)\n",
    "# Pin Albumentations to 1.3.1 for compatibility with both Colab and Kaggle\n",
    "!pip install -q albumentations==1.3.1\n",
    "\n",
    "print(\"\\n‚úì Packages installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126700b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from accuracy_focus.attention_unet.src.losses.bce_logits import DiceBCEWithLogitsLoss\n",
    "from accuracy_focus.attention_unet.src.models.attention_unet.attention_unet import AttentionUNet\n",
    "\n",
    "from shared.src.data import HC18Dataset\n",
    "from shared.src.metrics.segmentation_metrics import dice_coefficient, iou_score, pixel_accuracy\n",
    "from shared.src.utils.visualization import save_prediction_grid, visualize_sample\n",
    "from shared.src.utils.transforms import get_transforms\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372c34d7",
   "metadata": {},
   "source": [
    "## 2. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afec2163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_path = project_root / 'accuracy_focus' / 'attention_unet' / 'configs' / 'attention_unet_config_v3.yaml'\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Adjust output paths for Google Colab environment\n",
    "print(f\"Adjusting paths for Google Colab environment...\")\n",
    "config['logging']['checkpoint_dir'] = str(output_root / 'results' / 'checkpoints')\n",
    "config['logging']['log_dir'] = str(output_root / 'results' / 'logs')\n",
    "config['logging']['prediction_dir'] = str(output_root / 'results' / 'predictions')\n",
    "config['logging']['visualization_dir'] = str(output_root / 'results' / 'visualizations')\n",
    "\n",
    "print(f\"  Outputs will be saved to: {output_root / 'results'}\")\n",
    "\n",
    "print(\"\\nConfiguration loaded:\")\n",
    "print(f\"  Model: {config['model']['name']}\")\n",
    "print(f\"  Base Filters: {config['model']['base_filters']}\")\n",
    "print(f\"  Learning Rate: {config['training']['optimizer']['lr']}\")\n",
    "print(f\"  Loss Function: {config['loss']['name']}\")\n",
    "print(f\"  Batch Size: {config['training']['batch_size']}\")\n",
    "print(f\"  Epochs: {config['training']['num_epochs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441fe73f",
   "metadata": {},
   "source": [
    "## 3. Model Initialization\n",
    "\n",
    "### Attention U-Net Architecture Details\n",
    "\n",
    "**Encoder Path:**\n",
    "- 4 downsampling blocks: 64 ‚Üí 128 ‚Üí 256 ‚Üí 512 filters\n",
    "- Each block: ConvBlock (Conv2d + BatchNorm + ReLU) √ó 2\n",
    "- Downsampling: MaxPool2d (2√ó2, stride=2)\n",
    "\n",
    "**Bottleneck:**\n",
    "- ConvBlock with 1024 filters\n",
    "- Captures highest-level semantic features\n",
    "\n",
    "**Decoder Path:**\n",
    "- 4 upsampling blocks: 512 ‚Üí 256 ‚Üí 128 ‚Üí 64 filters\n",
    "- Upsampling: ConvTranspose2d (2√ó2, stride=2)\n",
    "- Each block: ConvBlock √ó 2 after skip connection concatenation\n",
    "\n",
    "**Attention Mechanism:**\n",
    "- **Attention Gates** applied to encoder features before skip connections\n",
    "- Gating signal from decoder path guides attention weights\n",
    "- Focuses on salient regions, suppresses noise\n",
    "\n",
    "**Output Layer:**\n",
    "- Conv2d (1√ó1) to single channel\n",
    "- **No sigmoid activation** ‚Üí outputs raw logits\n",
    "- Required for DiceBCEWithLogitsLoss numerical stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573de887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(config['device'] if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = AttentionUNet(\n",
    "    in_channels=config['model']['in_channels'],\n",
    "    out_channels=config['model']['out_channels'],\n",
    "    base_filters=config['model']['base_filters'],\n",
    "    use_sigmoid=False \n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params\n",
    "\n",
    "total_params, trainable_params = count_parameters(model)\n",
    "\n",
    "# Model summary\n",
    "print(f\"\\nAttention U-Net Architecture:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Model size: ~{total_params * 4 / (1024**2):.2f} MB (float32)\")\n",
    "print(f\"  Input channels: {config['model']['in_channels']}\")\n",
    "print(f\"  Output channels: {config['model']['out_channels']}\")\n",
    "print(f\"  Base filters: {config['model']['base_filters']}\")\n",
    "print(f\"  Output activation: None (logits for DiceBCELoss)\")\n",
    "\n",
    "# Test forward pass\n",
    "test_input = torch.randn(1, 1, 256, 256).to(device)\n",
    "test_output = model(test_input)\n",
    "print(f\"\\nTest forward pass:\")\n",
    "print(f\"  Input shape: {test_input.shape}\")\n",
    "print(f\"  Output shape: {test_output.shape}\")\n",
    "print(f\"  Output range: [{test_output.min().item():.4f}, {test_output.max().item():.4f}]\")\n",
    "print(f\"  Output type: Logits (unbounded values)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5b1984",
   "metadata": {},
   "source": [
    "## 4. Loss Function and Optimization\n",
    "\n",
    "### Loss Function: DiceBCEWithLogitsLoss\n",
    "\n",
    "**Hybrid Loss Design:**\n",
    "- **Dice Loss (70%):** Optimizes region overlap (DSC metric)\n",
    "- **BCE Loss (30%):** Optimizes pixel-wise classification\n",
    "\n",
    "**Key Features:**\n",
    "- Expects **logits** as input (model outputs raw values)\n",
    "- Applies sigmoid internally for numerical stability\n",
    "- **Automatic pos_weight calculation** from first training batch\n",
    "  - Adapts to actual foreground/background pixel ratio\n",
    "  - Handles extreme class imbalance (0.5-1% foreground typical)\n",
    "  - No manual hyperparameter tuning required\n",
    "\n",
    "**Expected pos_weight:**\n",
    "- Formula: `num_background_pixels / num_foreground_pixels`\n",
    "- Typical range for fetal head: 100-200\n",
    "\n",
    "### Optimizer: Adam\n",
    "- Adaptive learning rate per parameter\n",
    "- Default betas: (0.9, 0.999), configurable weight decay\n",
    "\n",
    "### Learning Rate Scheduler: ReduceLROnPlateau\n",
    "- Monitors validation Dice coefficient\n",
    "- Reduces LR when validation performance plateaus\n",
    "- Helps escape local minima and fine-tune convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4691d290",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_config = config['loss']\n",
    "dice_weight_config = loss_config.get('dice_weight', 0.7)\n",
    "bce_weight_config = loss_config.get('bce_weight', 0.3)\n",
    "smooth_config = loss_config.get('smooth', 1e-6)\n",
    "\n",
    "# Use DiceBCEWithLogitsLoss with AUTO pos_weight calculation\n",
    "criterion = DiceBCEWithLogitsLoss(\n",
    "    dice_weight=dice_weight_config,\n",
    "    bce_weight=bce_weight_config,\n",
    "    pos_weight=None,  # Will be auto-computed from first batch\n",
    "    auto_weight=True,  # Enable automatic pos_weight calculation\n",
    "    smooth=smooth_config\n",
    ")\n",
    "\n",
    "print(f\"Loss Function: DiceBCEWithLogitsLoss (with auto pos_weight)\")\n",
    "print(f\"  Dice weight: {dice_weight_config}\")\n",
    "print(f\"  BCE weight: {bce_weight_config}\")\n",
    "print(f\"  Smooth parameter: {smooth_config}\")\n",
    "print(f\"  pos_weight: AUTO (will be computed from first training batch)\")\n",
    "print(f\"  Note: pos_weight adapts to actual foreground/background ratio\")\n",
    "\n",
    "# Optimizer (Adam)\n",
    "optimizer_config = config['training']['optimizer']\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=optimizer_config['lr'],\n",
    "    betas=tuple(optimizer_config['betas']),\n",
    "    eps=optimizer_config['eps'],\n",
    "    weight_decay=optimizer_config['weight_decay']\n",
    ")\n",
    "print(f\"\\nOptimizer: Adam\")\n",
    "print(f\"  Learning rate: {optimizer_config['lr']}\")\n",
    "print(f\"  Weight decay: {optimizer_config['weight_decay']}\")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler_config = config['training']['scheduler']\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=scheduler_config['mode'],\n",
    "    factor=scheduler_config['factor'],\n",
    "    patience=scheduler_config['patience'],\n",
    "    min_lr=scheduler_config['min_lr']\n",
    ")\n",
    "print(f\"\\nScheduler: ReduceLROnPlateau\")\n",
    "print(f\"  Mode: {scheduler_config['mode']}\")\n",
    "print(f\"  Factor: {scheduler_config['factor']}\")\n",
    "print(f\"  Patience: {scheduler_config['patience']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f2d7cb",
   "metadata": {},
   "source": [
    "**Understanding Automatic pos_weight:**\n",
    "\n",
    "The `pos_weight` parameter compensates for class imbalance in binary segmentation:\n",
    "\n",
    "- **Calculated once** during first training batch forward pass\n",
    "- **Formula:** `pos_weight = N_negative / N_positive`\n",
    "- **Purpose:** Upweights loss from rare positive (foreground) pixels\n",
    "- **Example:** \n",
    "  - Image: 256√ó256 = 65,536 pixels\n",
    "  - Foreground: ~500 pixels (0.76%)\n",
    "  - Background: ~65,036 pixels (99.24%)\n",
    "  - `pos_weight = 65,036 / 500 ‚âà 130`\n",
    "\n",
    "This ensures the model pays equal attention to both classes despite the imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644156e8",
   "metadata": {},
   "source": [
    "## 5. Data Loading and Augmentation\n",
    "\n",
    "### Preprocessing Pipeline (All Images)\n",
    "\n",
    "Applied consistently to training, validation, and test sets:\n",
    "1. **Normalization:** Divide pixel values by 255.0 ‚Üí [0, 1] range\n",
    "2. **Resizing:** 256√ó256 pixels (maintains aspect ratio consistency)\n",
    "3. **Tensor Conversion:** NumPy array ‚Üí PyTorch tensor (C√óH√óW format)\n",
    "\n",
    "### Data Augmentation (Training Only)\n",
    "\n",
    "**On-the-fly augmentation** using Albumentations library:\n",
    "- **HorizontalFlip:** p=0.5 (mirrors left-right)\n",
    "- **VerticalFlip:** p=0.5 (mirrors top-bottom)\n",
    "- **Rotation:** ¬±20¬∞ with p=0.5 (handles probe orientation variations)\n",
    "- **ShiftScaleRotate:** p=0.5\n",
    "  - Translation: ¬±10% (handles positioning variations)\n",
    "  - Scaling: ¬±10% (handles zoom variations)\n",
    "\n",
    "**Benefits:**\n",
    "- Augmentation applied **per epoch** ‚Üí different samples each time\n",
    "- Improves model generalization and robustness\n",
    "- Prevents overfitting on small datasets\n",
    "- Image-mask transforms synchronized automatically\n",
    "\n",
    "**Validation/Test:**\n",
    "- **No augmentation** applied\n",
    "- Only preprocessing (normalize, resize, tensorize)\n",
    "- Ensures consistent evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad39041",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = config['data']\n",
    "training_config = config['training']\n",
    "\n",
    "# Helper to build paths\n",
    "def get_path(config_path):\n",
    "    \"\"\"Helper to handle both absolute and relative paths\"\"\"\n",
    "    p = Path(config_path)\n",
    "    if p.is_absolute():\n",
    "        return str(p)\n",
    "    else:\n",
    "        return str(project_root / config_path)\n",
    "\n",
    "# Create augmentation transforms\n",
    "print(\"Creating augmentation transforms...\")\n",
    "train_transform = get_transforms(height=256, width=256, is_train=True)\n",
    "val_transform = get_transforms(height=256, width=256, is_train=False)\n",
    "print(\"  Train transform: WITH augmentation (HorizontalFlip, Rotation, ShiftScaleRotate)\")\n",
    "print(\"  Val transform: WITHOUT augmentation (resize + normalize only)\")\n",
    "\n",
    "# Create datasets - using HC18Dataset for on-the-fly augmentation\n",
    "print(\"\\nCreating training dataset...\")\n",
    "train_dataset = HC18Dataset(\n",
    "    image_dir=get_path(data_config['train_images']),\n",
    "    mask_dir=get_path(data_config['train_masks']),\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "print(\"Creating validation dataset...\")\n",
    "val_dataset = HC18Dataset(\n",
    "    image_dir=get_path(data_config['val_images']),\n",
    "    mask_dir=get_path(data_config['val_masks']),\n",
    "    transform=val_transform\n",
    ")\n",
    "\n",
    "print(\"Creating test dataset...\")\n",
    "test_dataset = HC18Dataset(\n",
    "    image_dir=get_path(data_config['test_images']),\n",
    "    mask_dir=get_path(data_config['test_masks']),\n",
    "    transform=val_transform\n",
    ")\n",
    "\n",
    "# Use num_workers=0 for Google Colab to avoid multiprocessing issues\n",
    "num_workers = 0\n",
    "print(f\"\\nDataLoader settings:\")\n",
    "print(f\"  num_workers: {num_workers} (disabled for Google Colab)\")\n",
    "print(f\"  Batch size: {training_config['batch_size']}\")\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=training_config['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=training_config['pin_memory']\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=training_config['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=training_config['pin_memory']\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=training_config['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=training_config['pin_memory']\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Datasets Ready:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Train samples: {len(train_dataset)}\")\n",
    "print(f\"  Validation samples: {len(val_dataset)}\")\n",
    "print(f\"  Test samples: {len(test_dataset)}\")\n",
    "print(f\"  Batch size: {training_config['batch_size']}\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")\n",
    "print(f\"  Train augmentation: ENABLED (on-the-fly)\")\n",
    "print(f\"  Val/Test augmentation: DISABLED\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8cd7a9",
   "metadata": {},
   "source": [
    "## 6. Data Quality Verification\n",
    "\n",
    "Verify data integrity before training to catch preprocessing errors early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1864fe0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of training data\n",
    "sample_images, sample_masks = next(iter(train_loader))\n",
    "\n",
    "print(f\"Sample batch:\")\n",
    "print(f\"  Images shape: {sample_images.shape}\")\n",
    "print(f\"  Masks shape: {sample_masks.shape}\")\n",
    "print(f\"  Image range: [{sample_images.min():.4f}, {sample_images.max():.4f}]\")\n",
    "print(f\"  Mask range: [{sample_masks.min():.4f}, {sample_masks.max():.4f}]\")\n",
    "print(f\"  Mask unique values: {torch.unique(sample_masks)}\")\n",
    "print(f\"  Mask mean (% foreground): {sample_masks.mean():.4f}\")\n",
    "\n",
    "# CRITICAL CHECK: Ensure masks are binary {0, 1}\n",
    "if not torch.all((sample_masks == 0) | (sample_masks == 1)):\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: Masks are not binary! Check preprocessing.\")\n",
    "else:\n",
    "    print(\"\\n‚úì Masks are properly binary {0, 1}\")\n",
    "\n",
    "# Check if masks have reasonable foreground ratio (2-10% typical for fetal head)\n",
    "fg_ratio = sample_masks.mean().item()\n",
    "if fg_ratio < 0.01 or fg_ratio > 0.3:\n",
    "    print(f\"‚ö†Ô∏è  WARNING: Unusual foreground ratio: {fg_ratio:.2%} (expected 2-10%)\")\n",
    "else:\n",
    "    print(f\"‚úì Foreground ratio looks reasonable: {fg_ratio:.2%}\")\n",
    "\n",
    "# Visualize first sample\n",
    "visualize_sample(sample_images[0], sample_masks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebef2a42",
   "metadata": {},
   "source": [
    "## 7. Training and Validation Functions\n",
    "\n",
    "### train_one_epoch()\n",
    "- Sets model to training mode\n",
    "- Iterates through training batches\n",
    "- Forward pass ‚Üí loss calculation ‚Üí backward pass ‚Üí optimizer step\n",
    "- Returns average epoch loss\n",
    "\n",
    "### validate()\n",
    "- Sets model to evaluation mode (disables dropout, batchnorm updates)\n",
    "- Computes loss and metrics on validation set\n",
    "- **Applies sigmoid to logits** before thresholding (model outputs logits)\n",
    "- Calculates: Dice coefficient, IoU, Pixel Accuracy\n",
    "- Returns average metrics across all validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c608eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, device, epoch):\n",
    "    \"\"\"\n",
    "    Train for one epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1} [Train]\", leave=False)\n",
    "    for batch_idx, (images, masks) in enumerate(pbar):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, masks)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update statistics\n",
    "        running_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, device, epoch):\n",
    "    \"\"\"\n",
    "    Validate the model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    dice_scores = []\n",
    "    iou_scores = []\n",
    "    pa_scores = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1} [Val]\", leave=False)\n",
    "        for batch_idx, (images, masks) in enumerate(pbar):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, masks)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Calculate metrics\n",
    "            # IMPORTANT: Model outputs LOGITS, need sigmoid before thresholding\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            \n",
    "            for i in range(images.size(0)):\n",
    "                dice = dice_coefficient(preds[i], masks[i])\n",
    "                iou = iou_score(preds[i], masks[i])\n",
    "                pa = pixel_accuracy(preds[i], masks[i])\n",
    "                \n",
    "                dice_scores.append(dice.item())\n",
    "                iou_scores.append(iou.item())\n",
    "                pa_scores.append(pa.item())\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': f\"{loss.item():.4f}\",\n",
    "                'dice': f\"{np.mean(dice_scores):.4f}\"\n",
    "            })\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    val_loss = running_loss / len(dataloader)\n",
    "    val_dice = np.mean(dice_scores)\n",
    "    val_iou = np.mean(iou_scores)\n",
    "    val_pa = np.mean(pa_scores)\n",
    "    \n",
    "    return val_loss, val_dice, val_iou, val_pa\n",
    "\n",
    "\n",
    "print(\"Training functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4f480d",
   "metadata": {},
   "source": [
    "## 8. Training Loop\n",
    "\n",
    "### Training Configuration\n",
    "- **Epochs:** Configurable (typically 50-100)\n",
    "- **Early Stopping:** Monitors validation Dice coefficient\n",
    "  - Stops if no improvement for N consecutive epochs\n",
    "  - Prevents overfitting and saves compute time\n",
    "- **Model Checkpointing:** Saves best model based on validation Dice\n",
    "\n",
    "### Per-Epoch Workflow\n",
    "1. Train on full training set\n",
    "2. Validate on validation set\n",
    "3. Update learning rate (ReduceLROnPlateau scheduler)\n",
    "4. Log metrics: loss, Dice, IoU, pixel accuracy\n",
    "5. Save model if validation Dice improves\n",
    "6. Check early stopping criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cfde12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{'='*60}\")\n",
    "print(f\"Starting Training - Attention U-Net\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Training configuration\n",
    "num_epochs = config['training']['num_epochs']\n",
    "patience = config['training']['early_stopping_patience']\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'val_dice': [],\n",
    "    'val_iou': [],\n",
    "    'val_pa': [],\n",
    "    'lr': []\n",
    "}\n",
    "\n",
    "best_dice = 0.0\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "print(f\"Epochs: {num_epochs}\")\n",
    "print(f\"Early Stopping Patience: {patience}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Main training loop\n",
    "for epoch in range(num_epochs):    \n",
    "    # Train\n",
    "    train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device, epoch)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_dice, val_iou, val_pa = validate(model, val_loader, criterion, device, epoch)\n",
    "    \n",
    "    # Update learning rate\n",
    "    old_lr = optimizer.param_groups[0]['lr']\n",
    "    scheduler.step(val_dice)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Update history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_dice'].append(val_dice)\n",
    "    history['val_iou'].append(val_iou)\n",
    "    history['val_pa'].append(val_pa)\n",
    "    history['lr'].append(current_lr)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"{'='*100}\")\n",
    "    dice_indicator = ' üèÜ' if val_dice > best_dice else ''\n",
    "    lr_change = f' ‚¨áÔ∏è (reduced from {old_lr:.6f})' if current_lr < old_lr else ''\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Dice: {val_dice:.4f}{dice_indicator}\")\n",
    "    print(f\"Val mIoU: {val_iou:.4f} | Val mPA: {val_pa:.4f}\")\n",
    "    if lr_change: print(f\"LR: {current_lr:.6f}{lr_change}\")\n",
    "    print(f\"{'='*100}\")\n",
    "    \n",
    "    # Check for improvement\n",
    "    is_best = val_dice > best_dice\n",
    "    if is_best:\n",
    "        best_dice = val_dice\n",
    "        epochs_without_improvement = 0\n",
    "        \n",
    "        # Save best model\n",
    "        checkpoint_dir = Path(get_path(config['logging']['checkpoint_dir']))\n",
    "        checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "        best_model_path = checkpoint_dir / 'best_model_attention_unet_v4.pth'\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'best_dice': best_dice,\n",
    "            'history': history,\n",
    "            'config': config\n",
    "        }, best_model_path)\n",
    "        \n",
    "        print(f\"  ‚Üí Saved best model (Dice: {best_dice:.4f})\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        print(f\"  ‚ö†Ô∏è  No improvement for {epochs_without_improvement}/{patience} epochs\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"‚õî EARLY STOPPING TRIGGERED\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"  Stopped at epoch: {epoch+1}\")\n",
    "        print(f\"  Best Dice Score:  {best_dice:.4f}\")\n",
    "        print(f\"  Patience limit:   {patience} epochs without improvement\")\n",
    "        print(f\"{'='*70}\")\n",
    "        break\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training Completed!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Best Validation Dice: {best_dice:.4f}\")\n",
    "print(f\"Best Validation IoU:  {max(history['val_iou']):.4f}\")\n",
    "print(f\"Best Validation PA:   {max(history['val_pa']):.4f}\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8493591c",
   "metadata": {},
   "source": [
    "## 9. Training Visualization\n",
    "\n",
    "Generate plots to analyze training dynamics and convergence:\n",
    "- **Loss curves:** Training vs validation loss over epochs\n",
    "- **Dice coefficient:** Validation performance trend\n",
    "- **IoU score:** Intersection over Union metric progression\n",
    "- **Learning rate:** ReduceLROnPlateau schedule adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0675355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Loss curves\n",
    "axes[0, 0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0, 0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Loss (Dice + BCE)', fontsize=12)\n",
    "axes[0, 0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=11)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Dice coefficient\n",
    "axes[0, 1].plot(history['val_dice'], label='Val Dice', color='green', linewidth=2)\n",
    "axes[0, 1].axhline(y=best_dice, color='red', linestyle='--', label=f'Best: {best_dice:.4f}')\n",
    "axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Dice Coefficient', fontsize=12)\n",
    "axes[0, 1].set_title('Validation Dice Coefficient', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=11)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# IoU\n",
    "axes[1, 0].plot(history['val_iou'], label='Val IoU', color='orange', linewidth=2)\n",
    "axes[1, 0].axhline(y=max(history['val_iou']), color='red', linestyle='--', \n",
    "                   label=f\"Best: {max(history['val_iou']):.4f}\")\n",
    "axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 0].set_ylabel('IoU Score', fontsize=12)\n",
    "axes[1, 0].set_title('Validation IoU Score', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend(fontsize=11)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "axes[1, 1].plot(history['lr'], label='Learning Rate', color='red', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Learning Rate', fontsize=12)\n",
    "axes[1, 1].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].legend(fontsize=11)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save figure\n",
    "log_dir = Path(get_path(config['logging']['log_dir']))\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(log_dir / 'training_curves.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"Training curves saved to {log_dir / 'training_curves.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a9c474",
   "metadata": {},
   "source": [
    "## 10. Model Inference and Results\n",
    "\n",
    "### Evaluation on Test Set\n",
    "\n",
    "Load the best checkpoint (highest validation Dice) and visualize predictions:\n",
    "- Compare input images, ground truth masks, and model predictions\n",
    "- Calculate per-sample metrics (Dice, IoU)\n",
    "- Assess segmentation quality visually\n",
    "\n",
    "**Note:** Model outputs logits, so sigmoid is applied before thresholding at 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be5abe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint_path = Path(get_path(config['logging']['checkpoint_dir'])) / 'best_model_attention_unet_v4.pth'\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loaded best model from epoch {checkpoint['epoch']+1}\")\n",
    "print(f\"Best Dice Score: {checkpoint['best_dice']:.4f}\")\n",
    "\n",
    "# Get validation samples\n",
    "test_images, test_masks = next(iter(test_loader))\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "# Generate predictions\n",
    "with torch.no_grad():\n",
    "    # IMPORTANT: Model outputs LOGITS, apply sigmoid to get probabilities\n",
    "    val_logits = model(test_images)\n",
    "    val_preds = (torch.sigmoid(val_logits) > 0.5).float()\n",
    "\n",
    "# Visualize predictions\n",
    "num_samples = min(4, len(test_images))\n",
    "fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4 * num_samples))\n",
    "\n",
    "if num_samples == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for i in range(num_samples):\n",
    "    # Move to CPU and convert to numpy\n",
    "    img = test_images[i, 0].cpu().numpy()\n",
    "    mask = test_masks[i, 0].numpy()\n",
    "    pred = val_preds[i, 0].cpu().numpy()\n",
    "    \n",
    "    # Calculate metrics for this sample\n",
    "    dice = dice_coefficient(val_preds[i].cpu(), test_masks[i].to(device)).item()\n",
    "    iou = iou_score(val_preds[i].cpu(), test_masks[i].to(device)).item()\n",
    "    \n",
    "    # Input image\n",
    "    axes[i, 0].imshow(img, cmap='gray')\n",
    "    axes[i, 0].set_title('Input Image', fontsize=12, fontweight='bold')\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    # Ground truth\n",
    "    axes[i, 1].imshow(mask, cmap='gray')\n",
    "    axes[i, 1].set_title('Ground Truth', fontsize=12, fontweight='bold')\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    # Prediction\n",
    "    axes[i, 2].imshow(pred, cmap='gray')\n",
    "    axes[i, 2].set_title(f'Prediction\\nDice: {dice:.4f} | IoU: {iou:.4f}', \n",
    "                         fontsize=12, fontweight='bold')\n",
    "    axes[i, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save predictions\n",
    "pred_dir = Path(get_path(config['logging']['prediction_dir']))\n",
    "pred_dir.mkdir(parents=True, exist_ok=True)\n",
    "save_prediction_grid(test_images[:4].cpu(), test_masks[:4], val_preds[:4].cpu(), \n",
    "                    str(pred_dir / 'sample_predictions.png'), num_samples=4)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
