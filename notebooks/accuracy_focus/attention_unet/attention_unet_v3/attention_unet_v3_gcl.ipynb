{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07947a49",
   "metadata": {},
   "source": [
    "# Attention U-Net Training Notebook (Google Colab)\n",
    "## Fetal Head Segmentation in Ultrasound Images\n",
    "\n",
    "**Compatible with:** Google Colab\n",
    "\n",
    "This notebook implements an **Attention U-Net** architecture with:\n",
    "1. Standard U-Net encoder/decoder with convolutional blocks\n",
    "2. **Attention Gates** applied to skip connections before concatenation\n",
    "3. Spatial attention mechanism to focus on relevant features\n",
    "4. Improved feature selection through gating mechanism\n",
    "5. Apply aggressive transforms for better generalization\n",
    "6. Add Gradient Clipping and Learning Rate Warm-up\n",
    "\n",
    "**Key Innovation:** Attention Gates weight the skip connections from the encoder, allowing the network to focus on relevant spatial regions and suppress irrelevant activations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ed6cd5",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "**Google Colab Environment:**\n",
    "- Clone project from GitHub: `https://github.com/TrinhThaiSonDHQT/Fetal-Head-Segmentation`\n",
    "- Project cloned to `/content/Fetal-Head-Segmentation/`\n",
    "- Outputs saved to `/content/Fetal-Head-Segmentation/results/` (can be downloaded after training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250fee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the GitHub repository\n",
    "import os\n",
    "\n",
    "# Check if already cloned\n",
    "if not os.path.exists('/content/Fetal-Head-Segmentation'):\n",
    "    print(\"Cloning repository from GitHub...\")\n",
    "    !git clone https://github.com/TrinhThaiSonDHQT/Fetal-Head-Segmentation.git\n",
    "    print(\"‚úì Repository cloned successfully\")\n",
    "else:\n",
    "    print(\"‚úì Repository already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d997f4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"[Google Colab Setup]\")\n",
    "\n",
    "# Setup paths for Google Colab\n",
    "project_root = Path('/content/Fetal-Head-Segmentation')\n",
    "output_root = project_root / 'results'\n",
    "cache_root = output_root / 'cache'\n",
    "\n",
    "# Verify project exists\n",
    "if not project_root.exists():\n",
    "    raise RuntimeError(\n",
    "        f\"Project not found at {project_root}\\n\"\n",
    "        f\"Please run the previous cell to clone the repository from GitHub.\"\n",
    "    )\n",
    "\n",
    "if not (project_root / 'accuracy_focus').exists():\n",
    "    raise RuntimeError(\n",
    "        f\"'accuracy_focus' folder not found in {project_root}\\n\"\n",
    "        f\"Please ensure the repository was cloned correctly.\"\n",
    "    )\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Output root: {output_root}\")\n",
    "\n",
    "# Add project to path\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"\\n‚úì Environment setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb5feba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for Google Colab\n",
    "print(\"Installing required packages...\")\n",
    "\n",
    "# Colab has most packages pre-installed (PyTorch, NumPy, Matplotlib, OpenCV)\n",
    "# Pin Albumentations to 1.3.1 for compatibility with both Colab and Kaggle\n",
    "!pip install -q albumentations==1.3.1\n",
    "\n",
    "print(\"\\n‚úì Packages installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde9f083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Import project modules\n",
    "from accuracy_focus.attention_unet.src.losses.dice_loss import DiceLoss\n",
    "from accuracy_focus.attention_unet.src.models.attention_unet import AttentionUNet\n",
    "\n",
    "from shared.src.data import HC18Dataset\n",
    "from shared.src.metrics.segmentation_metrics import dice_coefficient, iou_score, pixel_accuracy\n",
    "from shared.src.utils.visualization import save_prediction_grid, visualize_sample\n",
    "from shared.src.utils.transforms.aggressive_transforms import get_aggressive_transforms\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b28631",
   "metadata": {},
   "source": [
    "## 2. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2828a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_path = project_root / 'accuracy_focus' / 'attention_unet' / 'configs' / 'attention_unet_v2_config.yaml'\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Adjust output paths for Google Colab environment\n",
    "print(f\"Adjusting paths for Google Colab environment...\")\n",
    "config['logging']['checkpoint_dir'] = str(output_root / 'checkpoints')\n",
    "config['logging']['log_dir'] = str(output_root / 'logs')\n",
    "config['logging']['prediction_dir'] = str(output_root / 'predictions')\n",
    "config['logging']['visualization_dir'] = str(output_root / 'visualizations')\n",
    "\n",
    "print(f\"  Outputs will be saved to: {output_root}\")\n",
    "print(f\"  (download from Colab files panel after training)\")\n",
    "\n",
    "print(\"\\nConfiguration loaded:\")\n",
    "print(f\"  Model: {config['model']['name']}\")\n",
    "print(f\"  Base Filters: {config['model']['base_filters']}\")\n",
    "print(f\"  Learning Rate: {config['training']['optimizer']['lr']}\")\n",
    "print(f\"  Loss Function: {config['loss']['name']}\")\n",
    "print(f\"  Batch Size: {config['training']['batch_size']}\")\n",
    "print(f\"  Epochs: {config['training']['num_epochs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d75206",
   "metadata": {},
   "source": [
    "## 3. Initialize Model\n",
    "\n",
    "The Attention U-Net has:\n",
    "- **Encoder**: 4 downsampling blocks with ConvBlock (64 ‚Üí 128 ‚Üí 256 ‚Üí 512 channels)\n",
    "- **Bottleneck**: ConvBlock (1024 channels)\n",
    "- **Decoder**: 4 upsampling blocks with ConvBlock (512 ‚Üí 256 ‚Üí 128 ‚Üí 64 channels)\n",
    "- **Attention Gates**: Applied to skip connections before concatenation with decoder features\n",
    "- **Skip Connections**: Attention-weighted features from encoder concatenated with decoder\n",
    "- **Activation**: ReLU in convolutional blocks, Sigmoid output\n",
    "- **Spatial Attention**: Attention gates focus on relevant spatial regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00e15db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(config['device'] if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize model (outputs probabilities with sigmoid)\n",
    "model = AttentionUNet(\n",
    "    in_channels=config['model']['in_channels'],\n",
    "    out_channels=config['model']['out_channels'],\n",
    "    base_filters=config['model']['base_filters'],\n",
    "    use_sigmoid=True \n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params\n",
    "\n",
    "total_params, trainable_params = count_parameters(model)\n",
    "\n",
    "# Model summary\n",
    "print(f\"\\nAttention U-Net Architecture:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Model size: ~{total_params * 4 / (1024**2):.2f} MB (float32)\")\n",
    "print(f\"  Input channels: {config['model']['in_channels']}\")\n",
    "print(f\"  Output channels: {config['model']['out_channels']}\")\n",
    "print(f\"  Base filters: {config['model']['base_filters']}\")\n",
    "\n",
    "# Test forward pass\n",
    "test_input = torch.randn(1, 1, 256, 256).to(device)\n",
    "test_output = model(test_input)\n",
    "print(f\"\\nTest forward pass:\")\n",
    "print(f\"  Input shape: {test_input.shape}\")\n",
    "print(f\"  Output shape: {test_output.shape}\")\n",
    "print(f\"  Output range: [{test_output.min().item():.4f}, {test_output.max().item():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a84c06",
   "metadata": {},
   "source": [
    "## 4. Setup Loss and Optimizer\n",
    "\n",
    "- **Loss**: DiceLoss (Sorensen-Dice Loss)\n",
    "- **Optimizer**: Adam with learning rate from config\n",
    "- **Scheduler**: ReduceLROnPlateau (monitors validation Dice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2998e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function with automatic class imbalance handling\n",
    "loss_config = config['loss']\n",
    "# Loss Function (Dice Loss)\n",
    "smooth_param = loss_config['smooth']\n",
    "criterion = DiceLoss(smooth=smooth_param)\n",
    "print(f\"Loss Function: {loss_config['name']}\")\n",
    "print(f\"  Smooth parameter: {smooth_param}\")\n",
    "\n",
    "# Optimizer (Adam)\n",
    "optimizer_config = config['training']['optimizer']\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=optimizer_config['lr'],\n",
    "    betas=tuple(optimizer_config['betas']),\n",
    "    eps=optimizer_config['eps'],\n",
    "    weight_decay=optimizer_config['weight_decay']\n",
    ")\n",
    "print(f\"\\nOptimizer: Adam\")\n",
    "print(f\"  Learning rate: {optimizer_config['lr']}\")\n",
    "print(f\"  Weight decay: {optimizer_config['weight_decay']}\")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler_config = config['training']['scheduler']\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=scheduler_config['mode'],\n",
    "    factor=scheduler_config['factor'],\n",
    "    patience=scheduler_config['patience'],\n",
    "    min_lr=scheduler_config['min_lr']\n",
    ")\n",
    "print(f\"\\nScheduler: ReduceLROnPlateau\")\n",
    "print(f\"  Mode: {scheduler_config['mode']}\")\n",
    "print(f\"  Factor: {scheduler_config['factor']}\")\n",
    "print(f\"  Patience: {scheduler_config['patience']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9e96fa",
   "metadata": {},
   "source": [
    "## 5. Prepare Data Loaders\n",
    "\n",
    "**Preprocessing** (applied to all images):\n",
    "- Images normalized by dividing by 255.0\n",
    "- Resized to 256√ó256 pixels\n",
    "- Converted to PyTorch tensors (C, H, W)\n",
    "\n",
    "**Augmentation** (training only - applied on-the-fly):\n",
    "- Horizontal & Vertical flip (p=0.5)\n",
    "- Rotation (¬±20¬∞, p=0.5)\n",
    "- ShiftScaleRotate: Translation (¬±10%), Scale (¬±10%), p=0.5\n",
    "- Elastic deformation simulates tissue movement in ultrasound\n",
    "- Grid distortion creates localized warping\n",
    "- Gaussian noise simulates sensor noise\n",
    "- CLAHE enhances local contrast\n",
    "\n",
    "**Note:** Augmentations are applied dynamically during training. Fresh augmented samples are generated every epoch for better model generalization. Validation uses only preprocessing without augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccda8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = config['data']\n",
    "training_config = config['training']\n",
    "\n",
    "# Helper to build paths\n",
    "def get_path(config_path):\n",
    "    \"\"\"Helper to handle both absolute and relative paths\"\"\"\n",
    "    p = Path(config_path)\n",
    "    if p.is_absolute():\n",
    "        return str(p)\n",
    "    else:\n",
    "        return str(project_root / config_path)\n",
    "\n",
    "# Create augmentation transforms\n",
    "print(\"Creating augmentation transforms...\")\n",
    "train_transform = get_aggressive_transforms(height=256, width=256, is_train=True)\n",
    "val_transform = get_aggressive_transforms(height=256, width=256, is_train=False)\n",
    "print(\"  Train transform: WITH aggressive augmentation\")\n",
    "print(\"  Val transform: WITHOUT augmentation (resize + normalize only)\")\n",
    "\n",
    "# Create datasets - using HC18Dataset for on-the-fly augmentation\n",
    "print(\"\\nCreating training dataset...\")\n",
    "train_dataset = HC18Dataset(\n",
    "    image_dir=get_path(data_config['train_images']),\n",
    "    mask_dir=get_path(data_config['train_masks']),\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "print(\"Creating validation dataset...\")\n",
    "val_dataset = HC18Dataset(\n",
    "    image_dir=get_path(data_config['val_images']),\n",
    "    mask_dir=get_path(data_config['val_masks']),\n",
    "    transform=val_transform\n",
    ")\n",
    "\n",
    "# Use num_workers=2 for Colab (Colab has good CPU support)\n",
    "num_workers = 2\n",
    "print(f\"\\nDataLoader settings:\")\n",
    "print(f\"  num_workers: {num_workers}\")\n",
    "print(f\"  Batch size: {training_config['batch_size']}\")\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=training_config['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=training_config['pin_memory']\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=training_config['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=training_config['pin_memory']\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Datasets Ready:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Train samples: {len(train_dataset)}\")\n",
    "print(f\"  Validation samples: {len(val_dataset)}\")\n",
    "print(f\"  Batch size: {training_config['batch_size']}\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "print(f\"  Train augmentation: ENABLED (on-the-fly)\")\n",
    "print(f\"  Val augmentation: DISABLED\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71b735b",
   "metadata": {},
   "source": [
    "## 6. Visualize Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df35c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of training data\n",
    "sample_images, sample_masks = next(iter(train_loader))\n",
    "\n",
    "print(f\"Sample batch:\")\n",
    "print(f\"  Images shape: {sample_images.shape}\")\n",
    "print(f\"  Masks shape: {sample_masks.shape}\")\n",
    "print(f\"  Image range: [{sample_images.min():.4f}, {sample_images.max():.4f}]\")\n",
    "print(f\"  Mask range: [{sample_masks.min():.4f}, {sample_masks.max():.4f}]\")\n",
    "print(f\"  Mask unique values: {torch.unique(sample_masks)}\")\n",
    "print(f\"  Mask mean (% foreground): {sample_masks.mean():.4f}\")\n",
    "\n",
    "# CRITICAL CHECK: Ensure masks are binary {0, 1}\n",
    "if not torch.all((sample_masks == 0) | (sample_masks == 1)):\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: Masks are not binary! Check preprocessing.\")\n",
    "else:\n",
    "    print(\"\\n‚úì Masks are properly binary {0, 1}\")\n",
    "\n",
    "# Visualize first sample\n",
    "visualize_sample(sample_images[0], sample_masks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d3cf59",
   "metadata": {},
   "source": [
    "## 7. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0687ef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, device, epoch):\n",
    "    \"\"\"\n",
    "    Train for one epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1} [Train]\", leave=False)\n",
    "    for batch_idx, (images, masks) in enumerate(pbar):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, masks)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient Clipping (prevents gradient explosion)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update statistics\n",
    "        running_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, device, epoch):\n",
    "    \"\"\"\n",
    "    Validate the model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    dice_scores = []\n",
    "    iou_scores = []\n",
    "    pa_scores = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1} [Val]\", leave=False)\n",
    "        for batch_idx, (images, masks) in enumerate(pbar):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, masks)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Calculate metrics\n",
    "            preds = (outputs > 0.5).float()\n",
    "            \n",
    "            for i in range(images.size(0)):\n",
    "                dice = dice_coefficient(preds[i], masks[i])\n",
    "                iou = iou_score(preds[i], masks[i])\n",
    "                pa = pixel_accuracy(preds[i], masks[i])\n",
    "                \n",
    "                dice_scores.append(dice.item())\n",
    "                iou_scores.append(iou.item())\n",
    "                pa_scores.append(pa.item())\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': f\"{loss.item():.4f}\",\n",
    "                'dice': f\"{np.mean(dice_scores):.4f}\"\n",
    "            })\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    val_loss = running_loss / len(dataloader)\n",
    "    val_dice = np.mean(dice_scores)\n",
    "    val_iou = np.mean(iou_scores)\n",
    "    val_pa = np.mean(pa_scores)\n",
    "    \n",
    "    return val_loss, val_dice, val_iou, val_pa\n",
    "\n",
    "\n",
    "print(\"Training functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325a98ac",
   "metadata": {},
   "source": [
    "## 8. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f016f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{'='*60}\")\n",
    "print(f\"Starting Training - Attention U-Net\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Learning Rate Warm-up\n",
    "base_lr_config = config['training']['optimizer']['lr']\n",
    "def get_lr(epoch, base_lr=base_lr_config, warmup_epochs=10):\n",
    "    if epoch < warmup_epochs:\n",
    "        return base_lr * (epoch + 1) / warmup_epochs\n",
    "    else:\n",
    "        return base_lr\n",
    "\n",
    "# Training configuration\n",
    "num_epochs = config['training']['num_epochs']\n",
    "patience = config['training']['early_stopping_patience']\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'val_dice': [],\n",
    "    'val_iou': [],\n",
    "    'val_pa': [],\n",
    "    'lr': []\n",
    "}\n",
    "\n",
    "best_dice = 0.0\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "print(f\"Epochs: {num_epochs}\")\n",
    "print(f\"Early Stopping Patience: {patience}\")\n",
    "print(f\"Learning Rate Warm-up: 10 epochs\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Main training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Apply Learning Rate Warm-up\n",
    "    current_lr = get_lr(epoch)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = current_lr\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device, epoch)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_dice, val_iou, val_pa = validate(model, val_loader, criterion, device, epoch)\n",
    "    \n",
    "    # Update learning rate with scheduler (after warm-up)\n",
    "    old_lr = current_lr\n",
    "    if epoch >= 10:  # Only apply scheduler after warm-up\n",
    "        scheduler.step(val_dice)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Update history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_dice'].append(val_dice)\n",
    "    history['val_iou'].append(val_iou)\n",
    "    history['val_pa'].append(val_pa)\n",
    "    history['lr'].append(current_lr)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"{'='*100}\")\n",
    "    dice_indicator = ' üèÜ' if val_dice > best_dice else ''\n",
    "    lr_change = f' ‚¨áÔ∏è (reduced from {old_lr:.6f})' if current_lr < old_lr else ''\n",
    "    warmup_indicator = ' üî• (warm-up)' if epoch < 10 else ''\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Dice: {val_dice:.4f}{dice_indicator}\")\n",
    "    print(f\"Val mIoU: {val_iou:.4f} | Val mPA: {val_pa:.4f}\")\n",
    "    print(f\"LR: {current_lr:.6f}{lr_change}{warmup_indicator}\")\n",
    "    print(f\"{'='*100}\")\n",
    "    \n",
    "    # Check for improvement\n",
    "    is_best = val_dice > best_dice\n",
    "    if is_best:\n",
    "        best_dice = val_dice\n",
    "        epochs_without_improvement = 0\n",
    "        \n",
    "        # Save best model\n",
    "        checkpoint_dir = Path(get_path(config['logging']['checkpoint_dir']))\n",
    "        checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "        best_model_path = checkpoint_dir / 'best_model_attention_unet_v3.pth'\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'best_dice': best_dice,\n",
    "            'history': history,\n",
    "            'config': config\n",
    "        }, best_model_path)\n",
    "        \n",
    "        print(f\"  ‚Üí Saved best model (Dice: {best_dice:.4f})\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        print(f\"  ‚ö†Ô∏è  No improvement for {epochs_without_improvement}/{patience} epochs\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"‚õî EARLY STOPPING TRIGGERED\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"  Stopped at epoch: {epoch+1}\")\n",
    "        print(f\"  Best Dice Score:  {best_dice:.4f}\")\n",
    "        print(f\"  Patience limit:   {patience} epochs without improvement\")\n",
    "        print(f\"{'='*70}\")\n",
    "        break\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training Completed!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Best Validation Dice: {best_dice:.4f}\")\n",
    "print(f\"Best Validation IoU:  {max(history['val_iou']):.4f}\")\n",
    "print(f\"Best Validation PA:   {max(history['val_pa']):.4f}\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf0be17",
   "metadata": {},
   "source": [
    "## 9. Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d11f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Loss curves\n",
    "axes[0, 0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0, 0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Loss (Dice)', fontsize=12)\n",
    "axes[0, 0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=11)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Dice coefficient\n",
    "axes[0, 1].plot(history['val_dice'], label='Val Dice', color='green', linewidth=2)\n",
    "axes[0, 1].axhline(y=best_dice, color='red', linestyle='--', label=f'Best: {best_dice:.4f}')\n",
    "axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Dice Coefficient', fontsize=12)\n",
    "axes[0, 1].set_title('Validation Dice Coefficient', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=11)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# IoU\n",
    "axes[1, 0].plot(history['val_iou'], label='Val IoU', color='orange', linewidth=2)\n",
    "axes[1, 0].axhline(y=max(history['val_iou']), color='red', linestyle='--', \n",
    "                   label=f\"Best: {max(history['val_iou']):.4f}\")\n",
    "axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 0].set_ylabel('IoU Score', fontsize=12)\n",
    "axes[1, 0].set_title('Validation IoU Score', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend(fontsize=11)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "axes[1, 1].plot(history['lr'], label='Learning Rate', color='red', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Learning Rate', fontsize=12)\n",
    "axes[1, 1].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].legend(fontsize=11)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save figure\n",
    "log_dir = Path(get_path(config['logging']['log_dir']))\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(log_dir / 'training_curves.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"Training curves saved to {log_dir / 'training_curves.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f8b624",
   "metadata": {},
   "source": [
    "## 10. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bfb726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint_path = Path(get_path(config['logging']['checkpoint_dir'])) / 'best_model_attention_unet_v3.pth'\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loaded best model from epoch {checkpoint['epoch']+1}\")\n",
    "print(f\"Best Dice Score: {checkpoint['best_dice']:.4f}\")\n",
    "\n",
    "# Get validation samples\n",
    "val_images, val_masks = next(iter(val_loader))\n",
    "val_images = val_images.to(device)\n",
    "\n",
    "# Generate predictions\n",
    "with torch.no_grad():\n",
    "    # Model outputs probabilities (sigmoid already applied)\n",
    "    val_probs = model(val_images)\n",
    "    val_preds = (val_probs > 0.5).float()\n",
    "\n",
    "# Visualize predictions\n",
    "num_samples = min(4, len(val_images))\n",
    "fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4 * num_samples))\n",
    "\n",
    "if num_samples == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for i in range(num_samples):\n",
    "    # Move to CPU and convert to numpy\n",
    "    img = val_images[i, 0].cpu().numpy()\n",
    "    mask = val_masks[i, 0].numpy()\n",
    "    pred = val_preds[i, 0].cpu().numpy()\n",
    "    \n",
    "    # Calculate metrics for this sample\n",
    "    dice = dice_coefficient(val_preds[i].cpu(), val_masks[i].to(device)).item()\n",
    "    iou = iou_score(val_preds[i].cpu(), val_masks[i].to(device)).item()\n",
    "    \n",
    "    # Input image\n",
    "    axes[i, 0].imshow(img, cmap='gray')\n",
    "    axes[i, 0].set_title('Input Image', fontsize=12, fontweight='bold')\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    # Ground truth\n",
    "    axes[i, 1].imshow(mask, cmap='gray')\n",
    "    axes[i, 1].set_title('Ground Truth', fontsize=12, fontweight='bold')\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    # Prediction\n",
    "    axes[i, 2].imshow(pred, cmap='gray')\n",
    "    axes[i, 2].set_title(f'Prediction\\nDice: {dice:.4f} | IoU: {iou:.4f}', \n",
    "                         fontsize=12, fontweight='bold')\n",
    "    axes[i, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save predictions\n",
    "pred_dir = Path(get_path(config['logging']['prediction_dir']))\n",
    "pred_dir.mkdir(parents=True, exist_ok=True)\n",
    "save_prediction_grid(val_images[:4].cpu(), val_masks[:4], val_preds[:4].cpu(), \n",
    "                    str(pred_dir / 'sample_predictions.png'), num_samples=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d8bcfe",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "### Key Innovations:\n",
    "\n",
    "1. **Attention Gates on Skip Connections**: Spatial attention mechanism weights encoder features before concatenation\n",
    "   - Focuses on relevant spatial regions\n",
    "   - Suppresses irrelevant activations\n",
    "   - Improves feature selection\n",
    "\n",
    "2. **Standard U-Net Backbone**: Proven encoder-decoder architecture with skip connections\n",
    "\n",
    "3. **Improved Feature Selection**: Attention mechanism helps the network focus on the fetal head region\n",
    "\n",
    "4. **Aggressive Data Augmentation**: Elastic deformation, grid distortion, Gaussian noise, CLAHE\n",
    "\n",
    "5. **Training Enhancements**: Gradient clipping (max_norm=1.0) and learning rate warm-up (10 epochs)\n",
    "\n",
    "### Architecture Highlights:\n",
    "\n",
    "- **Encoder**: 4 stages with ConvBlock (64‚Üí128‚Üí256‚Üí512)\n",
    "- **Bottleneck**: ConvBlock (1024 channels)\n",
    "- **Decoder**: 4 stages with ConvBlock + Attention Gates (512‚Üí256‚Üí128‚Üí64)\n",
    "- **Total Parameters**: ~34M parameters (~131 MB)\n",
    "\n",
    "### Training Configuration:\n",
    "\n",
    "- **Loss**: DiceLoss (Sorensen-Dice)\n",
    "- **Optimizer**: Adam (lr from config, weight decay)\n",
    "- **Scheduler**: ReduceLROnPlateau (monitors validation Dice)\n",
    "- **Augmentation**: Aggressive transforms (on-the-fly)\n",
    "- **Gradient Clipping**: max_norm=1.0\n",
    "- **LR Warm-up**: 10 epochs\n",
    "\n",
    "### Expected Performance:\n",
    "\n",
    "The Attention Gates should improve segmentation accuracy by:\n",
    "- Better boundary detection through spatial attention\n",
    "- Improved focus on relevant regions (fetal head)\n",
    "- Reduced false positives by suppressing background features\n",
    "\n",
    "### Results:\n",
    "\n",
    "View the training curves and predictions above. Compare with:\n",
    "- **Standard U-Net**: Baseline performance\n",
    "- **Residual SE U-Net**: Channel attention approach\n",
    "- **ASPP-Enhanced models**: Multi-scale feature extraction\n",
    "\n",
    "---\n",
    "\n",
    "### Google Colab Specific Notes:\n",
    "\n",
    "**Setup:**\n",
    "- Repository cloned from: `https://github.com/TrinhThaiSonDHQT/Fetal-Head-Segmentation`\n",
    "- All project files loaded automatically from GitHub\n",
    "\n",
    "**Outputs:**\n",
    "- Results saved to: `/content/Fetal-Head-Segmentation/results/`\n",
    "- Download results: Files panel (left sidebar) ‚Üí right-click folder ‚Üí Download\n",
    "- Best model: `best_model_attention_unet_v2.pth`\n",
    "\n",
    "**Tips:**\n",
    "- Use GPU runtime: Runtime ‚Üí Change runtime type ‚Üí T4 GPU\n",
    "- Keep this tab open during training (or enable background execution)\n",
    "- Download results before closing notebook (files are deleted when runtime stops)\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Compare with Standard U-Net and other variants\n",
    "2. Analyze attention maps to understand what the network focuses on\n",
    "3. Test on HC18 test set\n",
    "4. Experiment with different attention configurations"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
