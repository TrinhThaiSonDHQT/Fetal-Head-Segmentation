{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7709760",
   "metadata": {},
   "source": [
    "# Fetal Head Segmentation Training (Universal - Colab/Kaggle Compatible)\n",
    "\n",
    "This notebook implements the complete training pipeline for the Improved U-Net model.\n",
    "\n",
    "**Target Performance Metrics:**\n",
    "- DSC (Dice Similarity Coefficient): ≥97.81%\n",
    "- mIoU (Mean Intersection over Union): ≥97.90%\n",
    "- mPA (Mean Pixel Accuracy): ≥99.18%\n",
    "\n",
    "**Platforms Supported:**\n",
    "- ✅ Google Colab\n",
    "- ✅ Kaggle Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0243ca49",
   "metadata": {},
   "source": [
    "## 1. Setup Environment & Platform Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76184b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PLATFORM DETECTION\n",
      "======================================================================\n",
      "✓ Running locally\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Detect platform\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Platform detection\n",
    "IS_COLAB = 'COLAB_GPU' in os.environ or 'google.colab' in sys.modules\n",
    "IS_KAGGLE = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PLATFORM DETECTION\")\n",
    "print(\"=\"*70)\n",
    "if IS_COLAB:\n",
    "    print(\"✓ Running on Google Colab\")\n",
    "    PLATFORM = 'colab'\n",
    "elif IS_KAGGLE:\n",
    "    print(\"✓ Running on Kaggle\")\n",
    "    PLATFORM = 'kaggle'\n",
    "else:\n",
    "    print(\"✓ Running locally\")\n",
    "    PLATFORM = 'local'\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "447e9102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PyTorch version: 2.9.0+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9983519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (if needed)\n",
    "if PLATFORM == 'colab':\n",
    "    !pip install albumentations==1.3.1 -q\n",
    "    !pip install pyyaml -q\n",
    "    print(\"✓ Packages installed for Colab\")\n",
    "elif PLATFORM == 'kaggle':\n",
    "    # Kaggle has most packages pre-installed, install only if needed\n",
    "    try:\n",
    "        import albumentations\n",
    "        print(f\"✓ Albumentations version: {albumentations.__version__}\")\n",
    "    except ImportError:\n",
    "        !pip install albumentations==1.3.1 -q\n",
    "        print(\"✓ Installed albumentations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1cabdc",
   "metadata": {},
   "source": [
    "## 2. Setup Paths (Platform-Specific)\n",
    "\n",
    "### For Google Colab:\n",
    "1. Upload your project folder to Google Drive\n",
    "2. Update `COLAB_PROJECT_PATH` below\n",
    "\n",
    "### For Kaggle:\n",
    "1. Upload your project as a Kaggle Dataset\n",
    "2. Add it as input to your notebook\n",
    "3. Update `KAGGLE_DATASET_NAME` below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940b18ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Project path: e:\\Fetal Head Segmentation\\notebooks\n",
      "\n",
      "Project files: ['01_data_exploration.ipynb', '02_training_experiments.ipynb', '03_results_analysis.ipynb', 'FHS_Accuracy_Focus.ipynb', 'FHS_Accuracy_Focus_Universal.ipynb', 'rebuild_cache.ipynb', 'results.txt']\n"
     ]
    }
   ],
   "source": [
    "# Platform-specific path configuration\n",
    "if PLATFORM == 'colab':\n",
    "    from google.colab import drive\n",
    "    \n",
    "    # Mount Google Drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # UPDATE THIS PATH for your Google Drive location\n",
    "    COLAB_PROJECT_PATH = '/content/drive/MyDrive/Course projects /Đồ án tốt nghiệp/Source code/Fetal Head Segmentation'\n",
    "    \n",
    "    PROJECT_PATH = COLAB_PROJECT_PATH\n",
    "    os.chdir(PROJECT_PATH)\n",
    "    sys.path.insert(0, PROJECT_PATH)\n",
    "    \n",
    "    print(f\"✓ Mounted Google Drive\")\n",
    "    print(f\"✓ Project path: {PROJECT_PATH}\")\n",
    "\n",
    "elif PLATFORM == 'kaggle':\n",
    "    # UPDATE THIS with your Kaggle dataset name\n",
    "    # Format: 'username/dataset-name'\n",
    "    KAGGLE_DATASET_NAME = 'yourusername/fetal-head-segmentation'\n",
    "    \n",
    "    # Kaggle paths\n",
    "    KAGGLE_INPUT_PATH = f'/kaggle/input/{KAGGLE_DATASET_NAME.split(\"/\")[-1]}'\n",
    "    KAGGLE_WORKING_PATH = '/kaggle/working'\n",
    "    \n",
    "    # Check if dataset is available\n",
    "    if os.path.exists(KAGGLE_INPUT_PATH):\n",
    "        PROJECT_PATH = KAGGLE_INPUT_PATH\n",
    "        print(f\"✓ Using Kaggle dataset: {KAGGLE_DATASET_NAME}\")\n",
    "    else:\n",
    "        # Fallback: look for any mounted dataset\n",
    "        input_datasets = os.listdir('/kaggle/input')\n",
    "        if input_datasets:\n",
    "            PROJECT_PATH = f'/kaggle/input/{input_datasets[0]}'\n",
    "            print(f\"⚠️ Dataset '{KAGGLE_DATASET_NAME}' not found\")\n",
    "            print(f\"✓ Using available dataset: {input_datasets[0]}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(\"No datasets found in /kaggle/input/\")\n",
    "    \n",
    "    # Add project to Python path\n",
    "    sys.path.insert(0, PROJECT_PATH)\n",
    "    \n",
    "    # Use working directory for outputs\n",
    "    OUTPUT_PATH = KAGGLE_WORKING_PATH\n",
    "    print(f\"✓ Project path (read-only): {PROJECT_PATH}\")\n",
    "    print(f\"✓ Output path: {OUTPUT_PATH}\")\n",
    "\n",
    "else:\n",
    "    # Local execution - navigate up 3 levels to project root\n",
    "    PROJECT_PATH = str(Path.cwd().parent.parent.parent)\n",
    "    OUTPUT_PATH = PROJECT_PATH\n",
    "    sys.path.insert(0, PROJECT_PATH)\n",
    "    print(f\"✓ Project path: {PROJECT_PATH}\")\n",
    "\n",
    "print(f\"\\nProject files: {os.listdir(PROJECT_PATH)[:10]}\")  # Show first 10 files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf369c4e",
   "metadata": {},
   "source": [
    "## 3. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a19032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "# Import from new structure\n",
    "from shared.configs.config_loader import load_config\n",
    "from shared.src.data import HC18Dataset, CachedHC18Dataset\n",
    "from accuracy_focus.improved_u_net.src.models import ImprovedUNet\n",
    "from shared.src.losses import DiceBCELoss\n",
    "from shared.src.utils import get_transforms, get_optimizer\n",
    "from shared.src.utils.train import train_one_epoch, evaluate_model\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab301612",
   "metadata": {},
   "source": [
    "## 4. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ced5230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration from new location\n",
    "config_path = os.path.join(PROJECT_PATH, 'shared/configs/train_config.yaml')\n",
    "print(f\"Loading configuration from: {config_path}\")\n",
    "config = load_config(config_path)\n",
    "\n",
    "# Extract config values\n",
    "data_cfg = config['data']\n",
    "model_cfg = config['model']\n",
    "train_cfg = config['training']\n",
    "aug_cfg = config['augmentation']\n",
    "checkpoint_cfg = config['checkpoint']\n",
    "logging_cfg = config['logging']\n",
    "\n",
    "# Setup device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create output directories (use OUTPUT_PATH for Kaggle)\n",
    "if PLATFORM == 'kaggle':\n",
    "    # Kaggle: save to /kaggle/working (writable)\n",
    "    checkpoint_dir = os.path.join(OUTPUT_PATH, 'checkpoints')\n",
    "    log_dir = os.path.join(OUTPUT_PATH, 'logs')\n",
    "    prediction_dir = os.path.join(OUTPUT_PATH, 'predictions')\n",
    "    visualization_dir = os.path.join(OUTPUT_PATH, 'visualizations')\n",
    "else:\n",
    "    # Colab/Local: use config paths (now pointing to improved_u_net folder)\n",
    "    checkpoint_dir = os.path.join(PROJECT_PATH, 'accuracy_focus/improved_u_net', checkpoint_cfg['save_dir'])\n",
    "    log_dir = os.path.join(PROJECT_PATH, 'accuracy_focus/improved_u_net', logging_cfg['log_dir'])\n",
    "    prediction_dir = os.path.join(PROJECT_PATH, 'accuracy_focus/improved_u_net', logging_cfg['prediction_dir'])\n",
    "    visualization_dir = os.path.join(PROJECT_PATH, 'accuracy_focus/improved_u_net', logging_cfg['visualization_dir'])\n",
    "\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "if logging_cfg.get('save_predictions', False):\n",
    "    os.makedirs(prediction_dir, exist_ok=True)\n",
    "    os.makedirs(visualization_dir, exist_ok=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FETAL HEAD SEGMENTATION - IMPROVED U-NET TRAINING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Platform: {PLATFORM.upper()}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Batch Size: {train_cfg['batch_size']}\")\n",
    "print(f\"Learning Rate: {train_cfg['optimizer']['lr']}\")\n",
    "print(f\"Number of Epochs: {train_cfg['num_epochs']}\")\n",
    "print(f\"Checkpoints: {checkpoint_dir}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efc6936",
   "metadata": {},
   "source": [
    "## 5. Prepare Datasets and Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a974f1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[1/4] Loading datasets...\\n\")\n",
    "\n",
    "# Get image size from config\n",
    "img_size = aug_cfg['preprocessing']['image_size'][0]  # Assuming square images\n",
    "\n",
    "# Get transforms\n",
    "train_transforms = get_transforms(img_size, img_size, is_train=True)\n",
    "val_transforms = get_transforms(img_size, img_size, is_train=False)\n",
    "\n",
    "# Adjust paths for platform\n",
    "def get_data_path(relative_path):\n",
    "    \"\"\"Convert relative path to absolute path based on platform\"\"\"\n",
    "    return os.path.join(PROJECT_PATH, relative_path)\n",
    "\n",
    "# Determine whether to use cached dataset\n",
    "use_cache = data_cfg.get('use_cache', False)\n",
    "cache_dir = get_data_path(data_cfg.get('cache_dir', 'preprocessed_data'))\n",
    "train_cache_path = Path(cache_dir) / 'train_cache'\n",
    "val_cache_path = Path(cache_dir) / 'val_cache'\n",
    "\n",
    "# Check if cache directories exist and have files\n",
    "train_cache_exists = train_cache_path.exists() and len(list(train_cache_path.glob('*.npz'))) > 0\n",
    "val_cache_exists = val_cache_path.exists() and len(list(val_cache_path.glob('*.npz'))) > 0\n",
    "\n",
    "print(f\"Cache directory: {cache_dir}\")\n",
    "print(f\"Train cache exists: {train_cache_exists}\")\n",
    "print(f\"Val cache exists: {val_cache_exists}\\n\")\n",
    "\n",
    "# Create datasets based on cache availability\n",
    "if use_cache and train_cache_exists:\n",
    "    print(\"Using cached training dataset...\")\n",
    "    train_dataset = CachedHC18Dataset(\n",
    "        image_dir=get_data_path(data_cfg['train_images']),\n",
    "        mask_dir=get_data_path(data_cfg['train_masks']),\n",
    "        cache_dir=str(train_cache_path),\n",
    "        img_height=img_size,\n",
    "        img_width=img_size,\n",
    "        transform=train_transforms,\n",
    "    )\n",
    "else:\n",
    "    if use_cache and not train_cache_exists:\n",
    "        print(\"Cache not found. Using standard dataset for training...\")\n",
    "    train_dataset = HC18Dataset(\n",
    "        get_data_path(data_cfg['train_images']), \n",
    "        get_data_path(data_cfg['train_masks']), \n",
    "        transform=train_transforms\n",
    "    )\n",
    "\n",
    "if use_cache and val_cache_exists:\n",
    "    print(\"Using cached validation dataset...\")\n",
    "    val_dataset = CachedHC18Dataset(\n",
    "        image_dir=get_data_path(data_cfg['val_images']),\n",
    "        mask_dir=get_data_path(data_cfg['val_masks']),\n",
    "        cache_dir=str(val_cache_path),\n",
    "        img_height=img_size,\n",
    "        img_width=img_size,\n",
    "        transform=val_transforms,\n",
    "    )\n",
    "else:\n",
    "    if use_cache and not val_cache_exists:\n",
    "        print(\"Cache not found. Using standard dataset for validation...\")\n",
    "    val_dataset = HC18Dataset(\n",
    "        get_data_path(data_cfg['val_images']), \n",
    "        get_data_path(data_cfg['val_masks']), \n",
    "        transform=val_transforms\n",
    "    )\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples available at: {get_data_path(data_cfg['test_images'])}\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=train_cfg['batch_size'], \n",
    "    shuffle=True, \n",
    "    num_workers=train_cfg['num_workers'], \n",
    "    pin_memory=train_cfg['pin_memory']\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=train_cfg['batch_size'], \n",
    "    shuffle=False, \n",
    "    num_workers=train_cfg['num_workers'], \n",
    "    pin_memory=train_cfg['pin_memory']\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Data loaders created\")\n",
    "print(f\"  Training batches: {len(train_loader)}\")\n",
    "print(f\"  Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a2e664",
   "metadata": {},
   "source": [
    "## 6. Initialize Model, Loss, and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1df26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[2/4] Initializing model...\\n\")\n",
    "\n",
    "# Initialize Improved U-Net model\n",
    "model = ImprovedUNet(\n",
    "    in_channels=model_cfg['in_channels'], \n",
    "    out_channels=model_cfg['out_channels']\n",
    ").to(device)\n",
    "\n",
    "# Print model summary\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# CRITICAL FIX: Loss function with weighted BCE for class imbalance\n",
    "loss_cfg = train_cfg['loss']\n",
    "\n",
    "# Calculate pos_weight based on foreground ratio (default 200 for ~0.4% foreground)\n",
    "# pos_weight = background_pixels / foreground_pixels\n",
    "foreground_ratio = 0.004  # ~0.4% from HC18 dataset\n",
    "pos_weight = (1 - foreground_ratio) / foreground_ratio\n",
    "print(f\"\\n⚠️  CRITICAL: Class imbalance detected!\")\n",
    "print(f\"   Foreground: {foreground_ratio*100:.2f}%, Background: {(1-foreground_ratio)*100:.2f}%\")\n",
    "print(f\"   Using BCEWithLogitsLoss with pos_weight={pos_weight:.1f}\")\n",
    "\n",
    "# CRITICAL: Create pos_weight tensor on the SAME device as model\n",
    "pos_weight_tensor = torch.tensor([pos_weight], device=device)\n",
    "\n",
    "loss_fn = DiceBCELoss(\n",
    "    dice_weight=loss_cfg.get('dice_weight', 0.8),  # Default 0.8 (prioritize Dice)\n",
    "    bce_weight=loss_cfg.get('bce_weight', 0.2),    # Default 0.2\n",
    "    pos_weight=pos_weight_tensor  # CRITICAL: Must be on same device as model\n",
    ")\n",
    "print(f\"Loss: DiceBCELoss (dice_weight={loss_cfg.get('dice_weight', 0.8)}, bce_weight={loss_cfg.get('bce_weight', 0.2)})\")\n",
    "print(f\"      BCE uses pos_weight={pos_weight:.1f} on device={device}\")\n",
    "\n",
    "# Optimizer - supports both SGD and Adam\n",
    "optimizer_cfg = train_cfg['optimizer']\n",
    "optimizer = get_optimizer(model, optimizer_cfg)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler_cfg = train_cfg['scheduler']\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode=scheduler_cfg['mode'],\n",
    "    factor=scheduler_cfg['factor'],\n",
    "    patience=scheduler_cfg['patience'],\n",
    "    min_lr=scheduler_cfg['min_lr']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c9d2cf",
   "metadata": {},
   "source": [
    "## 7. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c7cebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[3/4] Starting training...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "best_dice = 0.0\n",
    "early_stopping_patience = train_cfg.get('early_stopping_patience', 15)  # Default: 15 epochs\n",
    "early_stopping_counter = 0\n",
    "early_stopped = False\n",
    "\n",
    "print(f\"Early stopping enabled with patience: {early_stopping_patience} epochs\\n\")\n",
    "\n",
    "for epoch in range(1, train_cfg['num_epochs'] + 1):\n",
    "    print(f\"\\nEpoch {epoch}/{train_cfg['num_epochs']}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Train for one epoch\n",
    "    train_loss, train_dice = train_one_epoch(\n",
    "        train_loader, model, optimizer, loss_fn, device, epoch\n",
    "    )\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    val_metrics = evaluate_model(val_loader, model, loss_fn, device)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Dice: {train_dice:.4f}\")\n",
    "    print(f\"Val Loss: {val_metrics['loss']:.4f} | Val Dice: {val_metrics['dice']:.4f}\")\n",
    "    print(f\"Val mIoU: {val_metrics['miou']:.4f} | Val mPA: {val_metrics['pixel_accuracy']:.4f}\")\n",
    "    \n",
    "    # Update learning rate based on validation Dice\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    scheduler.step(val_metrics['dice'])\n",
    "    new_lr = optimizer.param_groups[0]['lr']\n",
    "    if new_lr != current_lr:\n",
    "        print(f\"Learning rate reduced: {current_lr:.6f} → {new_lr:.6f}\")\n",
    "    \n",
    "    # Check for improvement\n",
    "    if val_metrics['dice'] > best_dice:\n",
    "        best_dice = val_metrics['dice']\n",
    "        early_stopping_counter = 0\n",
    "        \n",
    "        # Save best model\n",
    "        if checkpoint_cfg['save_best']:\n",
    "            save_path = os.path.join(checkpoint_dir, 'best_model.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_dice': best_dice,\n",
    "                'val_metrics': val_metrics,\n",
    "                'config': config\n",
    "            }, save_path)\n",
    "            print(f\"✓ Saved best model with Dice: {best_dice:.4f}\")\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        print(f\"Early stopping counter: {early_stopping_counter}/{early_stopping_patience}\")\n",
    "        \n",
    "        # Check if early stopping should trigger\n",
    "        if early_stopping_counter >= early_stopping_patience:\n",
    "            print(f\"\\n⚠️ Early stopping triggered! No improvement for {early_stopping_patience} epochs.\")\n",
    "            early_stopped = True\n",
    "    \n",
    "    # Save last checkpoint\n",
    "    if checkpoint_cfg['save_last']:\n",
    "        save_path = os.path.join(checkpoint_dir, 'last_model.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_metrics': val_metrics,\n",
    "            'config': config\n",
    "        }, save_path)\n",
    "    \n",
    "    # Save checkpoint every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        save_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch}.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'config': config\n",
    "        }, save_path)\n",
    "        print(f\"✓ Saved checkpoint at epoch {epoch}\")\n",
    "    \n",
    "    # Break if early stopping triggered\n",
    "    if early_stopped:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5d1a69",
   "metadata": {},
   "source": [
    "## 8. Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64449bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING COMPLETED!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Best Dice Score: {best_dice:.4f} ({best_dice*100:.2f}%)\")\n",
    "print(f\"Best model saved at: {os.path.join(checkpoint_dir, 'best_model.pth')}\")\n",
    "\n",
    "# Print early stopping info\n",
    "if early_stopped:\n",
    "    print(f\"\\n⚠️ Training stopped early at epoch {epoch} (no improvement for {early_stopping_patience} epochs)\")\n",
    "else:\n",
    "    print(f\"\\n✓ Completed all {train_cfg['num_epochs']} epochs\")\n",
    "\n",
    "# Print target metrics comparison\n",
    "target_metrics = config.get('target_metrics', {})\n",
    "if target_metrics:\n",
    "    print(\"\\nTarget Performance Metrics:\")\n",
    "    print(f\"  Target Dice: {target_metrics.get('dice', 0)*100:.2f}% | Achieved: {best_dice*100:.2f}%\")\n",
    "    \n",
    "    if best_dice >= target_metrics.get('dice', 0):\n",
    "        print(\"\\n🎉 Target Dice score achieved!\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️ Target not reached. Gap: {(target_metrics.get('dice', 0) - best_dice)*100:.2f}%\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a34e5c9",
   "metadata": {},
   "source": [
    "## 9. Download Trained Model\n",
    "\n",
    "### Colab: Download to browser\n",
    "### Kaggle: Files saved to /kaggle/working (auto-downloaded when notebook finishes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e608c625",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = os.path.join(checkpoint_dir, 'best_model.pth')\n",
    "\n",
    "if os.path.exists(best_model_path):\n",
    "    if PLATFORM == 'colab':\n",
    "        from google.colab import files\n",
    "        files.download(best_model_path)\n",
    "        print(f\"✓ Downloaded: {best_model_path}\")\n",
    "    elif PLATFORM == 'kaggle':\n",
    "        print(f\"✓ Model saved at: {best_model_path}\")\n",
    "        print(f\"✓ Files in /kaggle/working will be auto-downloaded when notebook completes\")\n",
    "        print(f\"\\nSaved files:\")\n",
    "        for f in os.listdir(checkpoint_dir):\n",
    "            print(f\"  - {f}\")\n",
    "    else:\n",
    "        print(f\"✓ Model saved at: {best_model_path}\")\n",
    "else:\n",
    "    print(\"⚠️ Best model not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfae128",
   "metadata": {},
   "source": [
    "## 10. Visualize Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e6f4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Get a batch from validation set\n",
    "with torch.no_grad():\n",
    "    images, masks = next(iter(val_loader))\n",
    "    images, masks = images.to(device), masks.to(device)\n",
    "    \n",
    "    # CRITICAL FIX: Model outputs logits, apply sigmoid for visualization\n",
    "    logits = model(images)\n",
    "    probs = torch.sigmoid(logits)  # Convert logits to probabilities [0, 1]\n",
    "    preds = (probs > 0.5).float()  # Threshold at 0.5\n",
    "\n",
    "# Visualize first 4 samples\n",
    "num_samples = min(4, images.shape[0])\n",
    "fig, axes = plt.subplots(num_samples, 4, figsize=(16, 4*num_samples))\n",
    "\n",
    "for i in range(num_samples):\n",
    "    # Original image\n",
    "    img = images[i].cpu().squeeze().numpy()\n",
    "    axes[i, 0].imshow(img, cmap='gray')\n",
    "    axes[i, 0].set_title('Input Image')\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    # Ground truth mask\n",
    "    mask = masks[i].cpu().squeeze().numpy()\n",
    "    axes[i, 1].imshow(mask, cmap='gray')\n",
    "    axes[i, 1].set_title('Ground Truth')\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    # Probability heatmap (NEW: show model confidence)\n",
    "    prob = probs[i].cpu().squeeze().numpy()\n",
    "    axes[i, 2].imshow(prob, cmap='jet', vmin=0, vmax=1)\n",
    "    axes[i, 2].set_title('Prediction Probability')\n",
    "    axes[i, 2].axis('off')\n",
    "    \n",
    "    # Binary prediction\n",
    "    pred = preds[i].cpu().squeeze().numpy()\n",
    "    axes[i, 3].imshow(pred, cmap='gray')\n",
    "    \n",
    "    # Calculate Dice for this sample\n",
    "    dice = (2 * (pred * mask).sum()) / (pred.sum() + mask.sum() + 1e-6)\n",
    "    axes[i, 3].set_title(f'Binary Prediction (Dice: {dice:.3f})')\n",
    "    axes[i, 3].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Visualization complete\")\n",
    "print(\"  Note: Model outputs logits → sigmoid applied for visualization\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
