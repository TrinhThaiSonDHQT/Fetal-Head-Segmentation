Our baseline network is U-Net with input features [64, 128, 256, 512], and we replace its encoder with a pre-trained low computationally demanding model, MobileNet v2. MobileNet is specifically optimized for efficient execution on lowpower devices, making it ideal for mobile and edge applications. The pre-trained MobileNet has weights from ImageNet. As Fig. 1 shows, following Howard et al, we utilize 2D convolutional and bottleneck layers in constructing the MobileNet v2 model. The bottleneck layer, consists of three operations. First, a 1×1 convolutional layer increases the number of input channels. This is followed by a 3 × 3 depthwise separable convolutional layer, which applies a single filter to each input channel. Finally, a 1×1 convolutional layer is used to decrease the number of channels back to the original dimension. Given an input image X ∈ C × H ×W in HRS, we apply data augmentation on it to get Xa. Then we feed Xa into the U-Net network. The feature maps 16 × H 2 × W 2 , 24 × W 4 × W 4 , 32 × W 8 × W 8 , 96 × W 16 × W 16 generated from bottleneck layers with channels C in [16, 24, 32, 96] are concatenated to corresponding upsampling layers in the decoder stack through skip connections. The output from the decoder stack is a prediction matrix Zˆ with shape 1×H ×W. After obtaining Zˆ, we apply sigmoid to generate the probability matrices.
