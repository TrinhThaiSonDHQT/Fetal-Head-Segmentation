# Attention U-Net Configuration File

# Data Paths
data:
  train_images: 'shared/dataset_v3/training_set/images'
  train_masks: 'shared/dataset_v3/training_set/masks'
  val_images: 'shared/dataset_v3/validation_set/images'
  val_masks: 'shared/dataset_v3/validation_set/masks'
  test_images: 'shared/dataset_v3/test_set/images'
  test_masks: 'shared/dataset_v3/test_set/masks'

# Model Architecture
model:
  name: 'AttentionUNet'
  in_channels: 1
  out_channels: 1
  base_filters: 64

# Training Hyperparameters
training:
  batch_size: 16
  num_epochs: 55
  num_workers: 2
  pin_memory: true
  early_stopping_patience: 15

  # Optimizer
  optimizer:
    name: 'Adam'
    lr: 0.0001
    betas: [0.9, 0.999]
    eps: 1.0e-8
    weight_decay: 1.0e-5

  # Learning Rate Scheduler
  scheduler:
    name: 'ReduceLROnPlateau'
    mode: 'max' # 'max' because we monitor val_dice (higher is better)
    factor: 0.5
    patience: 10
    min_lr: 1.0e-7
    verbose: true

# Loss Function
loss:
  name: 'DiceBCELoss'
  dice_weight: 0.8
  bce_weight: 0.2
  smooth: 1.0e-6

# Logging and Checkpointing
logging:
  checkpoint_dir: 'accuracy_focus/attention_unet/results/checkpoints'
  log_dir: 'accuracy_focus/attention_unet/results/logs'
  prediction_dir: 'accuracy_focus/attention_unet/results/predictions'
  visualization_dir: 'accuracy_focus/attention_unet/results/visualizations'
  save_every_n_epochs: 5
  save_best_only: true
  visualize_every_n_epochs: 5

# Device
device: 'cuda' # 'cuda' or 'cpu'

# Random Seed (for reproducibility)
seed: 42
